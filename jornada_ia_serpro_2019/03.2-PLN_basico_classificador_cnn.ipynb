{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN) - Básico\n",
    "\n",
    "\n",
    "SERPRO - SUPSD - DIVISÃO DE DESENVOLVIMENTO E SUSTENTAÇÃO DE PRODUTOS COGNITIVOS\n",
    "\n",
    "__JORNADA IA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Exemplo - Classificação de Documentos com CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora as CNNs tenham sido usadas principalmente para tarefas de Visão Computacional, nada as impede de serem usadas em aplicativos de PLN. Uma dessas aplicações para as quais as CNNs têm sido usadas efetivamente é a classificação de sentenças e documentos. \n",
    "\n",
    "A classificação de documentos é uma das tarefas mais populares em PLN. A classificação de documentos é extremamente útil para qualquer pessoa que esteja lidando com coleções massivas de dados, como as de sites de notícias, editores, advogados e universidades. Portanto, é interessante ver como a aprendizagem dos vetores de palavras pode ser adaptada a uma tarefa do mundo real, como a classificação de documentos por meio da vetorização de documentos inteiros em vez de palavras.\n",
    "\n",
    "O Objetivo deste exemplo é classificar documentos utilizando CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Para essa tarefa, usaremos um conjunto de arquivos de texto já categorizados. Estes são artigos de notícias da BBC. Todos os documentos desta coleção pertencem a uma das seguintes categorias: \n",
    "\n",
    "- Negócios\n",
    "- Entretenimento\n",
    "- Política\n",
    "- Esportes\n",
    "- Tecnologia\n",
    "\n",
    "Um exemplo de documento da categoria Tecnologia:\n",
    "\n",
    "'UK net users leading TV downloads British TV viewers lead the trend of illegally downloading US shows from the net, according to research. New episodes of 24, Desperate Housewives and Six Feet Under, appear on the web hours after they are shown in the US, said a report. Web tracking company Envisional said 18% of downloaders were from within the UK and that downloads of TV programmers had increased by 150% in the last year....'\n",
    "\n",
    "Página para o [dataset](http://mlg.ucd.ie/datasets/bbc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos a seguinte estratégia:  \n",
    " - Extrair os dados\n",
    " - Pré-processamento dos dados\n",
    " - Construir a Rede Neural CNN\n",
    " - Treinamento do modelo\n",
    " - Avaliar o modelo\n",
    " - Realizando Predições com dados de Teste\n",
    " - Visualizar as métricas de classificação e matriz e monfusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install keras\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import nltk\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos extrair os dados e carregá-los em um DataFrame, nas colunas \"Category\" e \"Text\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>sport</td>\n",
       "      <td>Henin-Hardenne beaten on comeback\\n\\nJustine H...</td>\n",
       "      <td>465.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>business</td>\n",
       "      <td>US trade gap ballooned in October\\n\\nThe US tr...</td>\n",
       "      <td>429.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>politics</td>\n",
       "      <td>Plan to give elderly care control\\n\\nElderly a...</td>\n",
       "      <td>388.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>tech</td>\n",
       "      <td>Lifestyle 'governs mobile choice'\\n\\nFaster, b...</td>\n",
       "      <td>225.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>politics</td>\n",
       "      <td>Kilroy launches 'Veritas' party\\n\\nEx-BBC chat...</td>\n",
       "      <td>111.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                               Text Filename\n",
       "1361     sport  Henin-Hardenne beaten on comeback\\n\\nJustine H...  465.txt\n",
       "150   business  US trade gap ballooned in October\\n\\nThe US tr...  429.txt\n",
       "510   politics  Plan to give elderly care control\\n\\nElderly a...  388.txt\n",
       "1101      tech  Lifestyle 'governs mobile choice'\\n\\nFaster, b...  225.txt\n",
       "583   politics  Kilroy launches 'Veritas' party\\n\\nEx-BBC chat...  111.txt"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para leitura dos dados\n",
    "def read_data_into_pandas(path):\n",
    "    col_names =  ['Category', 'Text', 'Filename']\n",
    "    data  = pd.DataFrame(columns = col_names)    \n",
    "    for root, _, files in os.walk(path):        \n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(root,filename),'r',encoding='latin-1') as f: \n",
    "                    text = f.read()\n",
    "                    data.loc[len(data)] = [root.split('/')[-1],text,filename]\n",
    "    return data\n",
    "\n",
    "path = 'data/bbc-fulltext'\n",
    "data = read_data_into_pandas(path)\n",
    "data = data.sample(frac=1) #Shuffle\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de pré-processamento dos dados vai realizar as seguintes tarefas:\n",
    "- Remoção de Pontuação\n",
    "- Tokenização\n",
    "- Remoção de Stop words\n",
    "- Aplicação de Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venezuel ident idl farm venezuel auth ident 500 farm includ 56 larg est idl continu controvers land reform policy 2001 land law govern tax seiz unus farm sit 40000 farm yet inspect stat nat land institut told assocy press vic presid jos vic rangel said farm ranch titl ord land produc noth fear crit land reform policy claim presid hugo chavez try enforc communiststyl econom program ign property right dam country land own claim nat land institut mad mistak class land publ priv govern venezuela largest land own say process cauty prev conflict stat mr rangel said land reform constitut permit priv property stressing effort vind soc econom year ineq country on property conflict govern el charcot cattl ranch run agroflor subsidy uk food group vestey agricult min arnoldo marquez told reut new ag sit docu guar priv land admin ranch howev complain prochavez squat tak 80 property last four year uk govern ask venezuel auth resolv conflict ask company going put pap ord hand land said mr marquez\n",
      "Tempo decorrido 8.239\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>sport</td>\n",
       "      <td>heninharden beat comeback justin heninharden l...</td>\n",
       "      <td>465.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>business</td>\n",
       "      <td>us trad gap balloon octob us trad deficit wid ...</td>\n",
       "      <td>429.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>politics</td>\n",
       "      <td>plan giv eld car control eld dis peopl would c...</td>\n",
       "      <td>388.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>tech</td>\n",
       "      <td>lifestyl govern mobl cho fast bet funky hardw ...</td>\n",
       "      <td>225.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy launch verita party exbbc chat show hos...</td>\n",
       "      <td>111.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                               Text Filename\n",
       "1361     sport  heninharden beat comeback justin heninharden l...  465.txt\n",
       "150   business  us trad gap balloon octob us trad deficit wid ...  429.txt\n",
       "510   politics  plan giv eld car control eld dis peopl would c...  388.txt\n",
       "1101      tech  lifestyl govern mobl cho fast bet funky hardw ...  225.txt\n",
       "583   politics  kilroy launch verita party exbbc chat show hos...  111.txt"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estrutura para armazenar pontuações\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# Função para remover pontuações das sentenças\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def preprocessing(text):\n",
    "        \n",
    "    # Remover pontuação\n",
    "    text= remove_punctuation(text)\n",
    "    \n",
    "    # Tokenização\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    # Coloca em minúscula\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    # Existe um pacote de stop words padrão para português -> stopwords.words('portuguese')\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    # Realizar o Stem\n",
    "    stemmer = LancasterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatizer: opção em relação ao stem \n",
    "    # wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Retonar o texto preprocessado agrupando as tokens\n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "start = time.time()\n",
    "data['Text'] = data['Text'].apply(preprocessing)\n",
    "print(data['Text'][0])\n",
    "print('Tempo decorrido %.3f' % (time.time()-start))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>venezuel ident idl farm venezuel auth ident 50...</td>\n",
       "      <td>388.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text Filename\n",
       "0  business  venezuel ident idl farm venezuel auth ident 50...  388.txt"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando os dados após o processamento\n",
    "data.loc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorização dos Dados para o Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos a classe Tokenizer do __[Keras](https://keras.io/)__ para montar nosso vocabulário. Ela permite vetorizar o texto do corpus, transformando o texto na sequência de inteiros (onde cada inteiro representa o índice do token em um dicionário) ou em uma matriz de documentos com as contagem das palavras ou seus respectivos scores TF-IDF\n",
    "\n",
    "https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetorização das variáveis preditoras (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Vocabulário:  21734\n",
      "Top primeiras palavras do vocabulário: \n",
      "[('said', 1), ('us', 2), ('mr', 3), ('year', 4), ('would', 5), ('new', 6), ('also', 7), ('peopl', 8), ('play', 9), ('on', 10), ('gam', 11), ('say', 12), ('tim', 13), ('could', 14), ('mak', 15), ('off', 16), ('last', 17), ('tak', 18), ('first', 19), ('govern', 20)]\n",
      "Exemplo de documento vetorizado: [319, 328, 258, 430, 93, 356, 28, 90, 675, 24, 970, 144, 744, 703, 35, 305, 356, 72, 288, 340, 276, 357, 367, 199, 1, 85, 341, 292, 17, 13, 30, 200, 555, 781, 24, 50, 149, 430, 344, 90, 508, 200, 390, 248, 324, 99, 68, 1, 736, 27]\n",
      "Exemplo de documento vetorizado preenchido com zeros):  [319 328 258 430  93 356  28  90 675  24 970 144 744 703  35 305 356  72\n",
      " 288 340 276 357 367 199   1  85 341 292  17  13  30 200 555 781  24  50\n",
      " 149 430 344  90 508 200 390 248 324  99  68   1 736  27]\n",
      "Tamanho do maior documento: 1455\n",
      "Shape do vetor de treinamento: (2225, 1455)\n"
     ]
    }
   ],
   "source": [
    "# Criação do Vocabulário com base nos textos dos documentos\n",
    "# Número de features (palavras) utilizadas para o treinamento\n",
    "NUM_FEATURES=1000\n",
    "tokenizer = Tokenizer(num_words=NUM_FEATURES)\n",
    "tokenizer.fit_on_texts(data['Text'].values)\n",
    "print(\"Tamanho do Vocabulário: \", len(tokenizer.word_index))\n",
    "print(\"Top primeiras palavras do vocabulário: \")\n",
    "iterator = iter(tokenizer.word_index.items())\n",
    "top_words = [next(iterator) for i in range(20)]\n",
    "print(top_words)\n",
    "\n",
    "# Vetorização dos dados de treinamento\n",
    "X = tokenizer.texts_to_sequences(data['Text'].values)\n",
    "print(\"Exemplo de documento vetorizado:\", X[0][:50])\n",
    "#print(\"Exemplo de documento vetorizado:\", X[0])\n",
    "\n",
    "\n",
    "# Padding dos dados de treinamento: preencher com zeros para que todas as frases fiquem do mesmo tamanho\n",
    "# Necessário para alimentar a rede neural  \n",
    "X = pad_sequences(X,value=0.)\n",
    "\n",
    "print(\"Exemplo de documento vetorizado preenchido com zeros): \", X[0][:50])\n",
    "print(\"Tamanho do maior documento:\",X.shape[1])\n",
    "print(\"Shape do vetor de treinamento:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([319, 328, 258, ...,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimindo o primeiro exemplo\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Matrix TF-IDF (2225, 1000)\n",
      "Exemplo TF-IDF [0.         1.6334511  1.00288827 0.         0.98648218 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.56357913 0.         1.2649335  1.31642468 1.22689338\n",
      " 0.         0.         0.         0.         1.47504034 0.\n",
      " 3.07365032 0.         0.         2.43971249 1.31642468 0.\n",
      " 1.42888369 0.         0.         0.         0.         3.14297232\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.47504034 0.         0.         0.         0.\n",
      " 0.         0.         1.62140976 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.57749977 0.         0.         0.\n",
      " 1.62729228 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         3.06259006 0.         0.         0.         0.\n",
      " 2.95101388 0.         0.         4.26675145 0.         0.\n",
      " 0.         1.67947599 0.         2.91029012 0.         0.\n",
      " 0.         0.         3.244205   0.         0.         0.\n",
      " 0.         0.         1.70548393 0.         1.77876986 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         3.10219109 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.9250167  0.         0.         0.         0.         1.99075004\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.9250167  0.         0.         0.         0.\n",
      " 0.         2.00311431 0.         0.         0.         2.00062533\n",
      " 1.95482321 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.04408044 0.         0.         0.         0.\n",
      " 0.         2.07904838 4.03598268 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         2.49986952 2.2436872  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.39585487 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.22693175 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.17554885 0.         0.         0.         2.22032591 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.14503882 0.         2.36780273 0.         0.         2.5620502\n",
      " 0.         0.         0.         2.27475029 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.49526944 0.         0.         0.         0.\n",
      " 2.24030816 0.         0.         2.34449125 2.29613866 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         2.37970928 2.30704667\n",
      " 0.         0.         2.29975852 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         4.24835211 2.44193678 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.37175262 2.28181757 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.36387158 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         2.48614351\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         4.17899507 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.47711494 0.         2.62375196 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         6.70600135 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.69019629 0.         0.         2.72541984 0.         0.\n",
      " 0.         0.         0.         0.         2.6398924  2.88234298\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 3.02128032 0.         0.         2.69019629 0.         2.73748291\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.85418565 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         2.74971395 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 3.11803582 0.         0.         0.         0.         0.\n",
      " 2.84043359 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         3.17547318 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         2.94913483 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.88234298 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         3.37295417 3.02128032 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         3.06399093 0.         0.         0.         0.\n",
      " 3.12736235 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         3.0552868  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         3.10880337 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 3.06399093 0.         0.         0.         0.         5.248186\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         4.01219937 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Caso queira utilizar uma matriz podemos criar uma Bag of Word com TF-IFD das palavras para cada documento\n",
    "X_matrix_tfidf = tokenizer.texts_to_matrix(data['Text'].values,mode='tfidf')\n",
    "print(\"Shape Matrix TF-IDF\",X_matrix_tfidf.shape)\n",
    "print(\"Exemplo TF-IDF\",X_matrix_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Matrix Count (2225, 1000)\n",
      "Exemplo Frequencia [0. 3. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 3. 0. 0. 2. 1. 0. 1. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 2. 0. 0. 4. 0. 0.\n",
      " 0. 1. 0. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Ou então uma Bag of Word com a contagem das palavras para cada documento\n",
    "X_matrix_count = tokenizer.texts_to_matrix(data['Text'].values,mode='count')\n",
    "print(\"Shape Matrix Count\",X_matrix_count.shape)\n",
    "print(\"Exemplo Frequencia\",X_matrix_count[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetorização da variável alvo (Y), que são as categorias dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias:  Index(['business', 'entertainment', 'politics', 'sport', 'tech'], dtype='object')\n",
      "Exemplo vetor de label: [0 0 0 1 0]\n",
      "Shape do vetor Y (2225, 5)\n"
     ]
    }
   ],
   "source": [
    "#Vetorização dos labels (Target)\n",
    "Y_labels = pd.get_dummies(data['Category'])\n",
    "print(\"Categorias: \",Y_labels.columns)\n",
    "Y = pd.get_dummies(data['Category']).values\n",
    "print(\"Exemplo vetor de label:\",Y[0])\n",
    "print('Shape do vetor Y',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos separar os dados de treinamento (90%) e teste (10%). Para isso vamos usar a função __train_test_split()__ do scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de treino: (2002, 1455) (2002, 5)\n",
      "Dados de teste:  (223, 1455) (223, 5)\n",
      "Exemplo de dado de treino [ 14 369 709 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Separação dos dados de treino e teste\n",
    "\n",
    "# Para uso de X descomente a linha abaixo\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# Para uso de X_matrix_tfidf descomente a linha abaixo\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_matrix_tfidf,Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Para uso de X_matrix_count descomente a linha abaixo\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_matrix_count,Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print('Dados de treino:',X_train.shape,Y_train.shape)\n",
    "print('Dados de teste: ',X_test.shape,Y_test.shape)\n",
    "print('Exemplo de dado de treino', X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção da Rede Neural Convolucional\n",
    "\n",
    "http://tflearn.org/layers/conv/#convolution-1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma rede neural convolucional com a seguinte arquitetura:\n",
    "- O input será uma matriz com dimensão X (maior número de palavras de um documento, caso esteja usando a vetorização)\n",
    "- Camada de entrada terá o número de neurônio igual ao número de features (palavras) do dicionário\n",
    "- Três camadas convolucionais (1d por se tratar de um vetor) e função de ativação ReLU. As camadas terão 128 filtros e com tamanhos 3,5 e 7 respectivamente.\n",
    "- Camada de Max Polling para realizar o agrupamento e redução da dimensionalidade\n",
    "- Camada Dropout para evitar o overfitting\n",
    "- Uma camada totalmente conectada com a função de ativação Softmax com as probabilidades das classes\n",
    "- Por fim uma camada de regressão com a função de perda e otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/layers/core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# Reset do grafo tenforflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Building convolutional network\n",
    "\n",
    "# Para uso de X descomente a linha abaixo\n",
    "network = input_data(shape=[None, X.shape[1]], name='input')\n",
    "\n",
    "# Para uso de X_matrix_tfidf descomente a linha abaixo\n",
    "#network = input_data(shape=[None, X_matrix_tfidf.shape[1]], name='input')\n",
    "\n",
    "# Para uso de X_matrix_count descomente a linha abaixo\n",
    "#network = input_data(shape=[None, X_matrix_count.shape[1]], name='input')\n",
    "\n",
    "network = tflearn.embedding(network, input_dim=NUM_FEATURES, output_dim=128)\n",
    "branch1 = conv_1d(network, 128, 3, padding='same', activation='relu', regularizer=\"L2\")\n",
    "branch2 = conv_1d(network, 128, 5, padding='same', activation='relu', regularizer=\"L2\")\n",
    "branch3 = conv_1d(network, 128, 7, padding='same', activation='relu', regularizer=\"L2\")\n",
    "network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "network = tf.expand_dims(network, 2)\n",
    "network = global_max_pool(network)\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 5, activation='softmax')\n",
    "network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                     loss='categorical_crossentropy', name='target')\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.15640\u001b[0m\u001b[0m | time: 27.245s\n",
      "| Adam | epoch: 005 | loss: 0.15640 - acc: 0.9549 -- iter: 1792/1801\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.14793\u001b[0m\u001b[0m | time: 28.781s\n",
      "| Adam | epoch: 005 | loss: 0.14793 - acc: 0.9594 | val_loss: 0.19385 - val_acc: 0.9403 -- iter: 1801/1801\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, n_epoch = 5,validation_set=0.1,batch_size=32,shuffle=True, show_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9551569530782144]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos salvar o modelo para utilizar no futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/04449579445/workspaces/workspace_cognitiva/cursos_ia/pln_basico/models/model_cnn_doc.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save('models/model_cnn_doc.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando Predições com dados de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos carregá-lo. Note que esse passo não é necessário se já tivermos o modelo carregado em memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/04449579445/workspaces/workspace_cognitiva/cursos_ia/pln_basico/models/model_cnn_doc.tflearn\n"
     ]
    }
   ],
   "source": [
    "model.load('models/model_cnn_doc.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, para finalizar, vamos visualizar algumas predições com os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto -> scotland gam ireland captain rul saturday six nat scotland origin nam start fail injury pick win ita replac nam train friday cent gordon also injury fit test friday see play would obvy replac cent could also mov could also ask travel squad scotland meas chang ireland sid see replac win third cap mak debut vict sou afric last novemb mil \n",
      "    Label -> sport\n",
      "    Label predict-> sport\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> french hono direct park brit film direct sir al park mad off ord art let on frant cult hono sir al receiv par wednesday french cult min poss film tal mr said pres award park french film say hollywood cre us told min hono frant carry world sir al film includ commit found memb direct gre britain form chairm uk film council board brit film work campaign shown us art plac socy mr said show us link quest world work also direct 2003 film lif david play man row art commit sent \n",
      "    Label -> entertainment\n",
      "    Label predict-> entertainment\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> profit euro 2004 uk firm post ris an profit sal england footbal boost euro 2004 profit 2004 rec lost deal chelse said thursday sign new agr scot club rang hop 2005 sal benefit launch new england ahead 2006 world cup janu annount agr chelse gav right mak would end 2006 fiv year ear expect firm receiv pay chelse said numb invest opportun result chief execut pet said firm plan grow sal uk intern firm report first an result sint list london stock exchang jun said uk market seen sal grow last year said launch rang boost sal supply team across world includ nat sid ireland shar trad \n",
      "    Label -> business\n",
      "    Label predict-> business\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> lord lawy lord ag enjoy celebr car serv law commit chairm first sev year also set investig civil ireland pay lord chancel lord said lord on gre gen leg decid lord court ap work commit second gre judg gre lawy gre man lord said pass away wednesday would miss lord chief lord seny judg england wal said lord work way hum right act lawy judg hum nat nee socy said person tot gre affect cam dam presid famy divid said lord good hum judg on fig lat lord publ led rac beg rac ros pol street follow three day near peopl ind build set settl appl the argu pol off rac say new law led cre pol auth commit rac lord abl list said lord tour pres commun system everyon gre qual abl list young peopl many whos could understand hear say talk hum nev lost spec combin hum mark report \n",
      "    Label -> politics\n",
      "    Label predict-> politics\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> fac chin ita ireland friend chin ita tak plac march sid fac march three day world cup qual tel av ita visit gam ahead world cup qual last meet ir beat ita world cup fin howev vict eight attempt sev gam march gam second tim play chin prevy back jun ir win 10 jap said chin mad gre last year provid difficult opposit perform as team last world cup chin play simil typ footbal ita mak return mass on gre pow world gam id prep three import world cup qual ireland round world cup campaign gam frant septemb octob 12 octob \n",
      "    Label -> sport\n",
      "    Label predict-> sport\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> get hollywood star movy receiv star hollywood hono launch film act giant cre monday receiv star hollywood hist star said fin war produc speak ad hollywood may said day hollywood hes hes im get ad premy fin war the follow ceremony hollywood join cost includ pop star act direct said may fin bil that produc say produc said work last 50 year think produc ad long peopl believ new gen fut first appear test \n",
      "    Label -> entertainment\n",
      "    Label predict-> entertainment\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> microsoft tak search microsoft ent search releas test vert find docu email fil pc hard driv bet program work pcs run window window 2000 search market becom increas firm program help peopl find fil search giant googl launch search octob plan releas simil softw janu search provid inform find anyth look said corp presid microsoft internet divid microsoft program us window internet within email program softw giant com lat search compet larg numb riv googl already releas plan get gam janu expect off search ear next year smal firm technolog off hug amount inform peopl increas stor hom comput appl releas simil search system comput cal due releas op system \n",
      "    Label -> tech\n",
      "    Label predict-> tech\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> issu warn scotland min warn fan want hit ban match said ord on sery meas consid scot execut campaign sect rang work tackl problem howev min said stop sect assocy old firm match key ms speak ahead third round scot cup club sunday sect long assocy sect support club becom sign target execut last week ms first min met support repres club discuss issu plan hold next mon off club lead seny pol off loc auth chief among speak bbc radio scotland sunday liv program ms describ friday meet produc said put would key aim ms sect footbal act club tak act past ban fan support group ens gam long problem ms said execut rol tackl said cant get away fact peopl try assocy footbal thing want stop that thing support group clear dont want part eith work us try deal ms pol act said pol want wheth particul individ going top effect cours already ind consid introduc ban ord giv pow peopl going top mad footbal match abl stop attend gam that thing hit peopl allow attend gam said rang effort said dont think doubt seen posit mov club rang footbal club involv work execut produc exampl educ pack young peopl \n",
      "    Label -> politics\n",
      "    Label predict-> sport\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> as bank doll doll lost ground maj cur wednesday sou jap deny plan doll suff biggest fal four month tuesday fear as cent bank low doll jap biggest hold doll world sou four largest doll buy day also high euro on euro wor on buy concern ris oil pric doll push us stock market tuesday jon av clos 16 lost 13 doll latest beg sou parlia report suggest country foreign plan boost hold cur aust doll wednesday howev sou mov fin market issu stat bank chang cur due short term market fact jap seny fin min off told plan chang cur hold foreign think expand euro hold jap foreign exchang start year us cur lost euro fin three month 2004 fal record low stag someth analyst howev point doll rec extend despit posit econom corp dat fact many econom problem foc country mass trad budget deficit analyst predict doll weak com \n",
      "    Label -> business\n",
      "    Label predict-> business\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Texto -> show rock star said famy mak real tv show end didnt lik around hous tim black sing told report europ award also appear popul show bas famy lif agree real show hes don hes hes got someth said said enough work involv mak sery watch iv film day said cur appear judg tal show ear year top pol import peopl rock part car famy driv forc behind three sery earn famy report popul seen sal hit mark record met art sal access act fig sint hit audy eight mil tv describ sery loss explain popul am get kick watch brit famy lik us mak complet every week \n",
      "    Label -> entertainment\n",
      "    Label predict-> entertainment\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Predição dos dados de testes\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Criação do dicionário reverso (key -> Índice ; value -> Token)\n",
    "reverse_dictionary = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "\n",
    "# Varre os dados de teste e imprime a predição dos 5 primeiros textos\n",
    "for i in range(len(X_test[:10])):\n",
    "    documento='';\n",
    "    for j in X_test[i]:\n",
    "        if j != 0:\n",
    "            documento+=reverse_dictionary[j]+' '\n",
    "        else:\n",
    "            break\n",
    "    print('Texto ->',documento)\n",
    "    print('    Label ->', Y_labels.columns[np.argmax(Y_test[i])])\n",
    "    print('    Label predict->', Y_labels.columns[np.argmax(predictions[i])])    \n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>tech</td>\n",
       "      <td>mak green comput hitech industry start get env...</td>\n",
       "      <td>307.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category                                               Text Filename\n",
       "985     tech  mak green comput hitech industry start get env...  307.txt"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Localizando no dataset as linhas que possuem um determinado texto\n",
    "mask = np.column_stack([data[col].str.contains(r\"mak green\", na=False) for col in data])\n",
    "data.loc[mask.any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de Classificação e Matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias:  Index(['business', 'entertainment', 'politics', 'sport', 'tech'], dtype='object')\n",
      "Métricas de Classificação\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97        55\n",
      "           1       0.93      0.93      0.93        29\n",
      "           2       0.98      0.89      0.93        45\n",
      "           3       0.95      1.00      0.97        52\n",
      "           4       0.93      0.98      0.95        42\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       223\n",
      "   macro avg       0.95      0.95      0.95       223\n",
      "weighted avg       0.96      0.96      0.95       223\n",
      "\n",
      "Matriz de Confusão\n",
      "[[53  0  0  0  2]\n",
      " [ 0 27  1  1  0]\n",
      " [ 1  1 40  2  1]\n",
      " [ 0  0  0 52  0]\n",
      " [ 0  1  0  0 41]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#Obtendo a categoria para as saídas dejedas e as previsões\n",
    "ymax = np.argmax(Y_test,axis=1)\n",
    "predictions_max= np.argmax(predictions,axis=1)\n",
    "\n",
    "##Imprime as categorias\n",
    "print(\"Categorias: \",Y_labels.columns)\n",
    "\n",
    "##Imprime as métricas de classificação\n",
    "print('Métricas de Classificação')\n",
    "print(metrics.classification_report(ymax,predictions_max))\n",
    "\n",
    "print('Matriz de Confusão')\n",
    "print(metrics.confusion_matrix(ymax, predictions_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercício \n",
    "\n",
    "Modifique este jupyter para que o modelo obtenha um melhor desempenho.\n",
    "\n",
    "Faça tentativas modificando as seguintes opções e compare os resultados:\n",
    "\n",
    "- Uso dos datasets X_matrix_tfidf e X_matrix_count ao inves de X (lembre de mudar os passos de 'Separação dos dados' e 'Construção da Rede Neural Convolucional' comentando e descomentando as linhas indicadas)\n",
    "- Uso de Lemmatization ao invés de Stemming\n",
    "- Variação no número de features através da variável NUM_FEATURES\n",
    "- Modificações dos hiperparâmetros da rede neural, tais como 'n_epoch' e 'batch_size'\n",
    "- Modifique a arquitetura da rede neural, tal como a quantidade e tamanho dos filtros, quantidade de camadas etc\n",
    "\n",
    "Qual configuração obteve melhores resultados?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
