{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLN  Básico - Exercícios (Respostas) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "portuguese_stemmer = RSLPStemmer()\n",
    "portuguese_stops = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Pré-processamento\n",
    " \n",
    "\"estou\" é uma das stopwords, mas não foi removida pois estava com a primeira letra em maiúsculo na lista de palavras (\"Estou\"). Uma boa prática para realizar o pré-processamento do texto deixar todos as palavras em minúsculo.\n",
    "\n",
    "Modifique a célula acima para transformar cada palavra para minúsculo utilizando list comprehension. Faça essa modificação antes de remover as stop words.\n",
    "\n",
    "Execute novamente a célula e certifique que \"estou\" foi removida\n",
    "\n",
    "Dica: utilizar o método 'lower()' de string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras em minúsculo ['estou', 'estudando', 'um', 'tema', 'interesante', 'em', 'pln']\n",
      "['estudando', 'tema', 'interesante', 'pln']\n"
     ]
    }
   ],
   "source": [
    "#Stop words em português\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Lista de palavras\n",
    "palavras = [\"Estou\", 'estudando', 'um', 'tema', 'interesante', 'em', 'PLN']\n",
    "\n",
    "# List comprehension para transofmrar em minúsculo\n",
    "palavras_minusculo = [word.lower() for word in palavras]\n",
    "print('Palavras em minúsculo',palavras_minusculo)\n",
    "\n",
    "# List comprehension para aplicar as portuguese_stop words a lista de palavras\n",
    "palavras_sem_stopwords = [palavra for palavra in palavras_minusculo if palavra not in portuguese_stops]\n",
    "print(palavras_sem_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "\n",
    "Modifique a célula acima para testar o SnowballStemmer com outras palavras em português. Para isso adicione novas linhas de teste. \n",
    "\n",
    "Execute novamente a célula e verifique os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowballStemmer Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "\n",
      "SnowballStemmer:\n",
      "Conzinhando ->  conzinh\n",
      "Culinária ->  culinár\n",
      "Computação ->  comput\n",
      "Computando ->  comput\n",
      "Computar ->  comput\n",
      "Pedreiro ->  pedreir\n",
      "Pedra ->  pedr\n",
      "Pedregulho-> pedregulh\n"
     ]
    }
   ],
   "source": [
    "#Linguagens suportadas pelo SnowballStemmer\n",
    "print(\"SnowballStemmer Languages:\",SnowballStemmer.languages)\n",
    "\n",
    "#Cria o Stemmer para a linguagem em português\n",
    "portuguese_stemmer = SnowballStemmer('portuguese')\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nSnowballStemmer:\")\n",
    "print(\"Conzinhando -> \",portuguese_stemmer.stem('Conzinhando'))\n",
    "print(\"Culinária -> \",portuguese_stemmer.stem('Culinária'))\n",
    "print(\"Computação -> \",portuguese_stemmer.stem('Computação'))\n",
    "print(\"Computando -> \",portuguese_stemmer.stem('Computando'))\n",
    "print(\"Computar -> \",portuguese_stemmer.stem('Computar'))\n",
    "print(\"Pedreiro -> \",portuguese_stemmer.stem('Pedreiro'))\n",
    "print(\"Pedra -> \", portuguese_stemmer.stem('Pedra'))\n",
    "print(\"Pedregulho->\",portuguese_stemmer.stem('Pedregulho'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3\n",
    "\n",
    "Copie a célula acima e cole abaixo.\n",
    "\n",
    "Em seguida, modifique a nova célula para adicionar o RSLPStemmer específico para português\n",
    "\n",
    "Utilize o comando abaixo para importar antes de usar:\n",
    "    \n",
    "    from nltk.stem import RSLPStemmer\n",
    "\n",
    "Execute novamente a célula e verifique os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RSLnPStemmer:\n",
      "Conzinhando ->  conzinh\n",
      "Culinária ->  culinár\n",
      "Computação ->  comput\n",
      "Computando ->  comput\n",
      "Computar ->  comput\n",
      "Pedreiro ->  pedr\n",
      "Pedra ->  pedr\n",
      "Pedregulho-> pedregulh\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "#Cria o Stemmer para a linguagem em português\n",
    "portuguese_stemmer = RSLPStemmer()\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nRSLnPStemmer:\")\n",
    "print(\"Conzinhando -> \",portuguese_stemmer.stem('Conzinhando'))\n",
    "print(\"Culinária -> \",portuguese_stemmer.stem('Culinária'))\n",
    "print(\"Computação -> \",portuguese_stemmer.stem('Computação'))\n",
    "print(\"Computando -> \",portuguese_stemmer.stem('Computando'))\n",
    "print(\"Computar -> \",portuguese_stemmer.stem('Computar'))\n",
    "print(\"Pedreiro -> \",portuguese_stemmer.stem('Pedreiro'))\n",
    "print(\"Pedra -> \", portuguese_stemmer.stem('Pedra'))\n",
    "print(\"Pedregulho->\",portuguese_stemmer.stem('Pedregulho'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4\n",
    "\n",
    "Copie a célula acima e cole abaixo.\n",
    "\n",
    "Em seguida, modifique a nova célula para testar o Lemmatization Spacy com um outro texto.\n",
    "\n",
    "Execute novamente a célula e verifique os resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto:         O\tpedreiro\tfoi\tna\tpedreira\tpegar\tpedras\t.\t\n",
      "POS Tagging:   DET\tNOUN\tVERB\tADP\tPROPN\tVERB\tNOUN\tPUNCT\t\n",
      "Lemmatization: O\tpedreiro\tser\to\tpedreiro\tpegar\tpedrar\t.\t\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "text = ''\n",
    "pos = ''\n",
    "lemma = ''\n",
    "for token in nlp(\"O pedreiro foi na pedreira pegar pedras.\"):\n",
    "    text += token.text + \"\\t\"\n",
    "    pos += token.pos_ + \"\\t\"\n",
    "    lemma += token.lemma_ + \"\\t\"\n",
    "\n",
    "print(\"Texto:        \", text)\n",
    "print(\"POS Tagging:  \", pos)\n",
    "print(\"Lemmatization:\", lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 -  Modelagem Estatística dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie e cole a célula acima.\n",
    "\n",
    "Modifique a nova célula para que antes da criação dos N-gramas sejam removidas as stopwords e feito o stemming com RSLPStemmer\n",
    "\n",
    "Dica:\n",
    "\n",
    "    portuguese_stemmer = RSLPStemmer()\n",
    "    portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "Execute novamente a célula e verifique os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precis', 'escrev', 'program', 'nltk', 'quebr', 'corpu', '(', 'grand', 'coleç', 'arqu', 'txt', ')', 'unigram', ',', 'bigram', ',', 'trigram', 'quatr', 'gram', '.', 'precis', 'escrev', 'program', 'nltk', 'quebr', 'corpu']\n",
      "\n",
      "Texto: \n",
      "eu preciso escrever um programa em nltk que quebra um corpus (uma grande coleção de arquivos txt) em unigramas, bigramas, trigramas e quatro gramas. eu preciso escrever um programa em nltk que quebra um corpus\n",
      "\n",
      "Bigramas: \n",
      "Counter({('precis', 'escrev'): 2, ('escrev', 'program'): 2, ('program', 'nltk'): 2, ('nltk', 'quebr'): 2, ('quebr', 'corpu'): 2, ('corpu', '('): 1, ('(', 'grand'): 1, ('grand', 'coleç'): 1, ('coleç', 'arqu'): 1, ('arqu', 'txt'): 1, ('txt', ')'): 1, (')', 'unigram'): 1, ('unigram', ','): 1, (',', 'bigram'): 1, ('bigram', ','): 1, (',', 'trigram'): 1, ('trigram', 'quatr'): 1, ('quatr', 'gram'): 1, ('gram', '.'): 1, ('.', 'precis'): 1})\n",
      "\n",
      "\n",
      "\n",
      "Trigramas: \n",
      "Counter({('precis', 'escrev', 'program'): 2, ('escrev', 'program', 'nltk'): 2, ('program', 'nltk', 'quebr'): 2, ('nltk', 'quebr', 'corpu'): 2, ('quebr', 'corpu', '('): 1, ('corpu', '(', 'grand'): 1, ('(', 'grand', 'coleç'): 1, ('grand', 'coleç', 'arqu'): 1, ('coleç', 'arqu', 'txt'): 1, ('arqu', 'txt', ')'): 1, ('txt', ')', 'unigram'): 1, (')', 'unigram', ','): 1, ('unigram', ',', 'bigram'): 1, (',', 'bigram', ','): 1, ('bigram', ',', 'trigram'): 1, (',', 'trigram', 'quatr'): 1, ('trigram', 'quatr', 'gram'): 1, ('quatr', 'gram', '.'): 1, ('gram', '.', 'precis'): 1, ('.', 'precis', 'escrev'): 1})\n",
      "\n",
      "\n",
      "\n",
      "Tetragramas: \n",
      "Counter({('precis', 'escrev', 'program', 'nltk'): 2, ('escrev', 'program', 'nltk', 'quebr'): 2, ('program', 'nltk', 'quebr', 'corpu'): 2, ('nltk', 'quebr', 'corpu', '('): 1, ('quebr', 'corpu', '(', 'grand'): 1, ('corpu', '(', 'grand', 'coleç'): 1, ('(', 'grand', 'coleç', 'arqu'): 1, ('grand', 'coleç', 'arqu', 'txt'): 1, ('coleç', 'arqu', 'txt', ')'): 1, ('arqu', 'txt', ')', 'unigram'): 1, ('txt', ')', 'unigram', ','): 1, (')', 'unigram', ',', 'bigram'): 1, ('unigram', ',', 'bigram', ','): 1, (',', 'bigram', ',', 'trigram'): 1, ('bigram', ',', 'trigram', 'quatr'): 1, (',', 'trigram', 'quatr', 'gram'): 1, ('trigram', 'quatr', 'gram', '.'): 1, ('quatr', 'gram', '.', 'precis'): 1, ('gram', '.', 'precis', 'escrev'): 1, ('.', 'precis', 'escrev', 'program'): 1})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "portuguese_stemmer = RSLPStemmer()\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Texto\n",
    "text = \"Eu preciso escrever um programa em NLTK que quebra um corpus (uma grande coleção de arquivos txt) em unigramas, bigramas, trigramas e quatro gramas. Eu preciso escrever um programa em NLTK que quebra um corpus\"\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenização\n",
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "#Resposta Exercício 1\n",
    "# Remover Stopwords e aplicando stemming\n",
    "token = [portuguese_stemmer.stem(palavra) for palavra in token if palavra not in portuguese_stops]\n",
    "print(token)\n",
    "\n",
    "# N-gramas\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "\n",
    "# Imprime na tela os n-gramas e suas respectivas ocorrências\n",
    "print(\"\\nTexto: \")\n",
    "print(text)\n",
    "print(\"\\nBigramas: \")\n",
    "print (Counter(bigrams))\n",
    "print(\"\\n\")\n",
    "print(\"\\nTrigramas: \")\n",
    "print (Counter(trigrams))\n",
    "print(\"\\n\")\n",
    "print(\"\\nTetragramas: \")\n",
    "print (Counter(fourgrams))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercício 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie e cole a célula acima.\n",
    "\n",
    "Modifique a nova célula para que sejam removidas as stopwords e feito o stemming com RSLPStemmer para cada documento.\n",
    "\n",
    "Inclua um novo documento com uma frase à sua escolha e faça o BoW com o vocabulário criado anteriormente com os outros 2 documentos.\n",
    "\n",
    "Execute novamente a célula e verifique os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras tokenizadas doc1: ['joa', 'gost', 'assist', 'futebol', '.', 'jos', 'gost', 'futebol']\n",
      "Palavras tokenizadas doc2: ['joa', 'gost', 'assist', 'film']\n",
      "Palavras tokenizadas doc3: ['estud', 'pln', 'gost']\n",
      "Combinação ['joa', 'gost', 'assist', 'futebol', '.', 'jos', 'gost', 'futebol', 'joa', 'gost', 'assist', 'film', 'estud', 'pln', 'gost']\n",
      "Lista de palavras: ['.', 'assist', 'estud', 'film', 'futebol', 'gost', 'joa', 'jos', 'pln']\n",
      "Tamanho do vocabulário 9\n",
      "Bow documento 1 [1, 1, 0, 0, 2, 2, 1, 1, 0]\n",
      "Bow documento 2 [0, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "Bow documento 3 [0, 0, 1, 0, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "doc1 = 'Joao gosta de assistir ao futebol. Jose gosta de futebol também' \n",
    "doc2 = 'Joao também gosta de assistir a filmes'\n",
    "doc3 = 'Estamos estudando PLN e gostando muito'\n",
    "\n",
    "#Tokenizacao dos documentos\n",
    "words_doc1 = nltk.word_tokenize(doc1.lower())\n",
    "words_doc1 = [portuguese_stemmer.stem(palavra) for palavra in words_doc1 if palavra not in portuguese_stops]\n",
    "print(\"Palavras tokenizadas doc1:\", words_doc1)\n",
    "\n",
    "words_doc2 = nltk.word_tokenize(doc2.lower())\n",
    "words_doc2 = [portuguese_stemmer.stem(palavra) for palavra in words_doc2 if palavra not in portuguese_stops]\n",
    "print(\"Palavras tokenizadas doc2:\", words_doc2)\n",
    "\n",
    "words_doc3 = nltk.word_tokenize(doc3.lower())\n",
    "words_doc3 = [portuguese_stemmer.stem(palavra) for palavra in words_doc3 if palavra not in portuguese_stops]\n",
    "print(\"Palavras tokenizadas doc3:\", words_doc3)\n",
    "\n",
    "#Todas as Palavras dos documentos\n",
    "words = words_doc1+words_doc2+words_doc3\n",
    "print('Combinação',words)\n",
    "\n",
    "#Obtem a lista  as palavras dos documentos (remove as repetidas e ordena)\n",
    "words = sorted(list(set(words)))\n",
    "print (\"Lista de palavras:\",words)\n",
    "print('Tamanho do vocabulário',len(words))\n",
    "\n",
    "bow = []\n",
    "\n",
    "for w in words:\n",
    "        #bow.append(1) if w in words_doc1 else bow.append(0) # Binário\n",
    "        bow.append(words_doc1.count(w)) #Contagem da palavra\n",
    "\n",
    "print('Bow documento 1',bow)\n",
    "\n",
    "bow = []\n",
    "for w in words:\n",
    "        #bow.append(1) if w in words_doc2 else bow.append(0)\n",
    "        bow.append(words_doc2.count(w))\n",
    "\n",
    "print('Bow documento 2',bow)     \n",
    "\n",
    "bow = []\n",
    "for w in words:\n",
    "        bow.append(words_doc3.count(w)) #Contagem da palavra\n",
    "print('Bow documento 3',bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercício 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie e cole a célula acima.\n",
    "\n",
    "Inclua mais um documento com uma frase à sua escolha.\n",
    "\n",
    "Consulte a documentação da classe CountVectorizer para adicionar dois novos parâmetros:\n",
    "\n",
    "    stop_words para poder remover stopwords em português\n",
    "    max_features para limitar em 5 o número de palavras do vocabulário\n",
    "\n",
    "Execute novamente a célula e verifique os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (from scikit-learn) (1.2.1)\n",
      "Features (bigramas) - Vocabulário:\n",
      "['assistir filmes', 'assistir futebol', 'estudar pln', 'gosta assistir', 'joao gosta']\n",
      "Bow - BagOfN-Gram:\n",
      "[[0 1 0 1 1]\n",
      " [1 0 0 1 1]\n",
      " [0 0 1 0 1]]\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "?CountVectorizer\n",
    "\n",
    "doc1 = 'Joao gosta de assistir ao futebol. Jose gosta de futebol também' \n",
    "doc2 = 'Joao também gosta de assistir a filmes'\n",
    "doc3 = 'Joao também gosta de estudar PLN'\n",
    "\n",
    "#Stop words em português\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Cria um ngram vectorizer\n",
    "# Vamos criar o BOW dos 2 documentos\n",
    "# Usamos n_gram = 2\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2,2),stop_words=portuguese_stops,max_features=5)\n",
    "\n",
    "# Contabilizamos as combinações de caracteres nas palavras\n",
    "bows = ngram_vectorizer.fit_transform([doc1, doc2, doc3])\n",
    "\n",
    "# Imprimimir as features\n",
    "print ('Features (bigramas) - Vocabulário:')\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "\n",
    "# Imprimir o BOW\n",
    "print ('Bow - BagOfN-Gram:')\n",
    "print (bows.toarray().astype(int))\n",
    "print(bows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício (Exemplo Sumarizador)\n",
    "\n",
    "Crie uma função (summarize_tfidf). Esta função utilize TD-IDF para calcular as importâncias das palavras, ao invés da frequência.\n",
    "\n",
    "Dica: utilize TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teste teste teste.', 'teste teste']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "# Função para sumarizar o texto utilizando TF-IDF\n",
    "def summarize_TFIDF(text, n):\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    assert n <= len(sents)\n",
    "    word_sent = word_tokenize(text.lower())\n",
    "    _stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "    \n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sents)\n",
    "    bow = X.toarray()    \n",
    "    \n",
    "    ranking = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(bow)):\n",
    "        for tfidf in bow[i]:\n",
    "            ranking[i] += tfidf             \n",
    "        \n",
    "    sents_idx = nlargest(n, ranking, key=ranking.get)\n",
    "    return [sents[j] for j in sorted(sents_idx)]\n",
    "\n",
    "summarize_TFIDF('teste teste teste. teste teste',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
