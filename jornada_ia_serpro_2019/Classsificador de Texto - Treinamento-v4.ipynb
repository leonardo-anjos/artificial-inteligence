{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação e treinamento do modelo de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports e definição de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importações realizadas com sucesso\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sklearn\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "# Função para salvar uma lista em um binário\n",
    "def save_list_to_file(list_to_save,file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(list_to_save, f)\n",
    "# Função para carregar uma lista \n",
    "def load_list_from_file(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)     \n",
    "print ('Importações realizadas com sucesso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self, val_epoch_thresh, val_acc_thresh):\n",
    "        self.val_epoch_thresh = val_epoch_thresh\n",
    "        self.val_acc_thresh = val_acc_thresh\n",
    "    def on_epoch_end(self, training_state):\n",
    "        print(\"Epoch \", training_state.epoch, \" with Accuracy \", training_state.acc_value)\n",
    "        if training_state.epoch >= self.val_epoch_thresh and training_state.acc_value >= self.val_acc_thresh:\n",
    "            raise StopIteration\n",
    "    def on_train_end(self, training_state):\n",
    "        print(\"Successfully left training! Final model accuracy:\", training_state.acc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros de funcionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros setados\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'ds4'\n",
    "LIMITE = 20000\n",
    "STEPS = 1000\n",
    "REDE = 'net1'\n",
    "BATCH = 10\n",
    "MIN_EPOCHS = 25\n",
    "MIN_ACCURACY = 0.60\n",
    "print ('Parâmetros setados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga dos arquivos frutos do pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregou  11  categorias:  ['atx_precricao', 'atx_decadencia', 'atx_jurisprudencia', 'atx_duplavisita', 'atx_criteriojuridico', 'atx_atenuacao', 'atx_principios', 'atx_retroatividade', 'atx_nulidade', 'atx_intimprevia', 'atx_denuncia_espontanea']\n",
      "\n",
      "Carregou  1577  Docs (primeiro):  [([('empres', 'acim'), ('acim', 'entregou'), ('entregou', 'espontanea'), ('espontanea', 'atraso'), ('atraso', 'gfip'), ('gfip', 'competência'), ('competência', '2010'), ('2010', '2010'), ('2010', 'receit'), ('receit', 'brasil'), ('brasil', 'multou'), ('multou', 'mesm'), ('mesm', 'const'), ('const', 'auto'), ('auto', 'infração'), ('infração', 'pedindo'), ('pedindo', 'impugnação'), ('impugnação', 'pagamento'), ('pagamento', 'desconto'), ('desconto', 'mesm'), ('mesm', 'trint'), ('trint', 'direito'), ('direito', 'prelimin'), ('prelimin', 'viemo'), ('viemo', 'atravé'), ('atravé', 'dest'), ('dest', 'solicit'), ('solicit', 'impugnação'), ('impugnação', 'auto'), ('auto', 'infração'), ('infração', 'declaração'), ('declaração', 'entregu'), ('entregu', 'notificação'), ('notificação', 'pedindo'), ('pedindo', 'mesm'), ('mesm', 'enviad'), ('enviad', 'guia'), ('guia', 'constam'), ('constam', 'mesm'), ('mesm', 'recolhida'), ('recolhida', 'conformidad'), ('conformidad', 'dado'), ('dado', 'declarado'), ('declarado', 'gfip'), ('gfip', 'anexo'), ('anexo', 'cópia'), ('cópia', 'declaraçõ'), ('declaraçõ', 'guia'), ('guia', 'mérito'), ('mérito', 'inciso'), ('inciso', 'outubro'), ('outubro', '1966'), ('1966', 'responsabilidad'), ('responsabilidad', 'excluíd'), ('excluíd', 'denúnc'), ('denúnc', 'espontâne'), ('espontâne', 'infração'), ('infração', 'acompanhad'), ('acompanhad', 'caso'), ('caso', 'pagamento'), ('pagamento', 'tributo'), ('tributo', 'devido'), ('devido', 'juro'), ('juro', 'depósito'), ('depósito', 'importânc'), ('importânc', 'arbitrad'), ('arbitrad', 'autoridad'), ('autoridad', 'administrativ'), ('administrativ', 'mont'), ('mont', 'tributo'), ('tributo', 'depend'), ('depend', 'apuração'), ('apuração', 'parágrafo'), ('parágrafo', 'único'), ('único', 'consider'), ('consider', 'espontâne'), ('espontâne', 'denúnc'), ('denúnc', 'apresentad'), ('apresentad', 'início'), ('início', 'qualqu'), ('qualqu', 'procedimento'), ('procedimento', 'administrativo'), ('administrativo', 'medid'), ('medid', 'fiscalização'), ('fiscalização', 'relacionado'), ('relacionado', 'infração'), ('infração', 'instrução'), ('instrução', 'normativ'), ('normativ', 'novembro'), ('novembro', '2009'), ('2009', 'caso'), ('caso', 'denúnc'), ('denúnc', 'espontâne'), ('espontâne', 'infração'), ('infração', 'lavratur'), ('lavratur', 'auto'), ('auto', 'infração'), ('infração', 'aplicação'), ('aplicação', 'penalidad'), ('penalidad', 'descumprimento'), ('descumprimento', 'obrigação'), ('obrigação', 'acessór'), ('acessór', 'parágrafo'), ('parágrafo', 'único'), ('único', 'consider'), ('consider', 'denúnc'), ('denúnc', 'espontâne'), ('espontâne', 'procedimento'), ('procedimento', 'adotado'), ('adotado', 'infr'), ('infr', 'regul'), ('regul', 'situação'), ('situação', 'configurado'), ('configurado', 'infração'), ('infração', 'início'), ('início', 'qualqu'), ('qualqu', 'ação'), ('ação', 'fisc'), ('fisc', 'relacionad'), ('relacionad', 'infração'), ('infração', 'dispensad'), ('dispensad', 'comunicação'), ('comunicação', 'correção'), ('correção', 'falt'), ('falt', 'conclusão'), ('conclusão', 'vist'), ('vist', 'todo'), ('todo', 'exposto'), ('exposto', 'demonstrad'), ('demonstrad', 'insubsistênc'), ('insubsistênc', 'improcedênc'), ('improcedênc', 'ação'), ('ação', 'fisc'), ('fisc', 'esper'), ('esper', 'requ'), ('requ', 'impugn'), ('impugn', 'acolhid'), ('acolhid', 'pres'), ('pres', 'impugnação'), ('impugnação', 'decidido'), ('decidido', 'cancelando'), ('cancelando', 'débito'), ('débito', 'fisc'), ('fisc', 'reclamado'), ('reclamado', 'termo')], ['atx_intimprevia', 'atx_denuncia_espontanea'], '147087145_1_OCR.txt')]\n",
      "\n",
      "Limitando para apenas  20000  n-grams mais frequentes.\n",
      "\n",
      "Carregou  20000  N-Grams (primeiros):  [('0001', '2015'), ('0001', 'conformando'), ('0001', 'regularidad'), ('0001', 'repres'), ('00036', 'todo'), ('00039', 'referido'), ('00058', '02098'), ('00059', '02257'), ('001129', '2006'), ('00156', '00200'), ('00200', '00647'), ('0022', '2355'), ('002606', 'césar'), ('003902', 'desembargad'), ('0045', 'contato'), ('00456', 'tributo'), ('0063', '3488'), ('00647', 'rddt'), ('0110100', '2015'), ('01129', '00456'), ('0120105', '2015'), ('0120114', '2015'), ('0120200', '2015'), ('0130105', '2015'), ('0140200', '2015'), ('02098', '00039'), ('02257', '00156'), ('0265859', 'ministro'), ('0310304', '2015'), ('03382', '01129'), ('036354', 'juiz'), ('037804', 'álvaro'), ('0651', 'joão'), ('0686', 'ferreiraesanto'), ('0710800', '2015'), ('0747', '1500006'), ('0810300', '2015'), ('0810806', '2015'), ('0811400', '2015'), ('0811402', '2015'), ('09090', '2669'), ('0910500', '2015'), ('0920100', '2015'), ('0920105', '2015'), ('09607', '4362'), ('1000', 'cálculo'), ('1010100', '2015'), ('1010110', '2015'), ('1010200', '2015'), ('1010300', '2015'), ('1010700', '2015'), ('1010704', '2015'), ('1010707', '2015'), ('1020', 'martin'), ('1022', 'termo'), ('1043', '98111'), ('1075', 'celso'), ('1075', 'mult'), ('1075', 'text'), ('1075', 'transgressão'), ('1107', '2010'), ('1116', 'primeir'), ('112585', 'revist'), ('1169446', 'chef'), ('12009', 'referid'), ('1227', 'umuaram'), ('1291553', 'direito'), ('1291553', 'reparem'), ('13097', '2015'), ('1372009', 'referid'), ('17060', 'nulidad'), ('1710', 'aind'), ('1966', '2009'), ('1966', 'artigo'), ('1966', 'código'), ('1966', 'dispõ'), ('1966', 'instituiu'), ('1966', 'instrução'), ('1966', 'novembro'), ('1966', 'pleno'), ('1966', 'responsabilidad'), ('1966', 'serviço'), ('1966', 'verb'), ('1972', 'alteraçõ'), ('1972', 'parágrafo'), ('1977', 'definiu'), ('1977', 'órgão'), ('1979', 'órgão'), ('1988', 'administração'), ('1988', 'nortear'), ('1990', 'fgts'), ('1991', 'alteraçõ'), ('1991', 'assim'), ('1991', 'contrato'), ('1991', 'contribuint'), ('1991', 'deix'), ('1991', 'entreg'), ('1991', 'impossibilidad'), ('1991', 'lançada'), ('1991', 'lançado'), ('1991', 'novembro'), ('1991', 'publicação'), ('1991', 'quinhento'), ('1991', 'receit'), ('1991', 'redação'), ('1991', 'sançõ'), ('1991', 'sido'), ('1991', 'sujeito'), ('1991', 'toda'), ('1991', 'vimo'), ('1992', 'lavratur'), ('1996', 'const'), ('1996', 'incluído'), ('1996', 'responsabilidad'), ('1996', 'trat'), ('1998', '2006'), ('1998', 'ausênc'), ('1998', 'completa'), ('1998', 'destart'), ('1998', 'diverso'), ('1998', 'exata'), ('1998', 'meno'), ('1998', 'supremo'), ('1999', 'aplicação'), ('1999', 'gfip'), ('1999', 'recurso'), ('1999', 'toda'), ('2000', '002606'), ('2000', '036354'), ('2001', '003902'), ('2002', '2003'), ('2002', 'denúnc'), ('2002', 'utilização'), ('2003', '00058'), ('2003', '000926'), ('2003', 'artigo'), ('2003', 'destac'), ('200338030009266', '2003'), ('2004', '00036'), ('2004', '037804'), ('2005', '2005'), ('2005', '70012144739'), ('2005', 'agravo'), ('2005', 'circul'), ('2005', 'conform'), ('2005', 'correção'), ('2005', 'fato'), ('2005', 'grifo'), ('2005', 'partir'), ('2005', 'prescrição'), ('2005', 'primeir'), ('2005', 'resp'), ('2005', 'rest'), ('2005', 'sefip'), ('2005', 'vigésim'), ('200500183819', '2007'), ('2006', '00059'), ('2006', 'aplicação'), ('2006', 'citação'), ('2006', 'entendeu'), ('2006', 'falt'), ('2006', 'incluído'), ('2006', 'pedido'), ('2006', 'prevê'), ('2006', 'primeir'), ('2006', 'página'), ('2006', 'redação'), ('2006', 'teor'), ('200600039161', '2007'), ('200602048298', '2007'), ('2007', '2007'), ('2007', 'decisõ'), ('2007', 'francisco'), ('2007', 'humberto'), ('2007', 'luiz'), ('2007', 'observemo'), ('2007', 'rddt'), ('2008', '2009'), ('2008', '2010'), ('2008', 'aind'), ('2008', 'aplicam'), ('2008', 'aprovou'), ('2008', 'caso'), ('2008', 'constam'), ('2008', 'convertid'), ('2008', 'dest'), ('2008', 'gfip'), ('2008', 'responsável'), ('2009', '1075'), ('2009', '2009'), ('2009', '2010'), ('2009', '2013'), ('2009', '2014'), ('2009', '2015'), ('2009', 'abrangido'), ('2009', 'agor'), ('2009', 'aplic'), ('2009', 'aplicação'), ('2009', 'artigo'), ('2009', 'assim')]\n"
     ]
    }
   ],
   "source": [
    "categories = load_list_from_file('categories.pickle')\n",
    "all_ngrams = load_list_from_file('all_ngrams.pickle')\n",
    "docs = load_list_from_file('docs.pickle')\n",
    "print ('\\nCarregou ',len(categories),' categorias: ',categories)\n",
    "print ('\\nCarregou ',len(docs),' Docs (primeiro): ',docs[:1])\n",
    "# Calcula frequências e remove duplicidade\n",
    "freq = Counter(all_ngrams)\n",
    "all_ngrams = [i[0] for i in freq.most_common(LIMITE)]\n",
    "all_ngrams = sorted(list(set(all_ngrams)))\n",
    "print ('\\nLimitando para apenas ',len(all_ngrams),' n-grams mais frequentes.')\n",
    "print ('\\nCarregou ',len(all_ngrams),' N-Grams (primeiros): ',all_ngrams[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do bag of n-grams para cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando montagem de 1577 documentos\n",
      "   processando documento 100\n",
      "   processando documento 200\n",
      "   processando documento 300\n",
      "   processando documento 400\n",
      "   processando documento 500\n",
      "   processando documento 600\n",
      "   processando documento 700\n",
      "   processando documento 800\n",
      "   processando documento 900\n",
      "   processando documento 1000\n",
      "   processando documento 1100\n",
      "   processando documento 1200\n",
      "   processando documento 1300\n",
      "   processando documento 1400\n",
      "   processando documento 1500\n",
      "Criou  1577  dados para treinamento e teste\n"
     ]
    }
   ],
   "source": [
    "# Cria as listas para os dados de treino e teste\n",
    "training_test = []\n",
    "count = 0\n",
    "print('Iniciando montagem de',len(docs),'documentos')\n",
    "for doc in docs:\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print('   processando documento',count)\n",
    "    # Inicializa o bag of words para cada documento da lista\n",
    "    bow = []\n",
    "    out = []\n",
    "    # Lista de palavras tokenizadas\n",
    "    ngrs = doc[0]\n",
    "    # Cria um array com o bag of words\n",
    "    for w in all_ngrams:\n",
    "        bow.append(1) if w in ngrs else bow.append(0)\n",
    "    # Lista de categorias\n",
    "    cats = doc[1]\n",
    "    txt = doc[2]\n",
    "    for c in categories:\n",
    "        out.append(1) if c in cats else out.append(0)\n",
    "    # Nosso conjunto de treinamento conterá um modelo bag of words e a linha de saída que informa a qual sentença pertence.\n",
    "    training_test.append([bow, out, txt])    \n",
    "print('Criou ',len(training_test),' dados para treinamento e teste')\n",
    "training_test_backup = training_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação dos dados de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train data: X_train( 1056 ), y_train( 1056 )\n",
      " Test data: X_test( 521 ), y_test( 521 )\n",
      "Exemplo de X_train:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ...\n",
      "Exemplo de y_train:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Embaralha e transforma em np.array enquanto o TensorFlow recebe uma matriz numérica\n",
    "random.shuffle(training_test)\n",
    "training_test = np.array(training_test)\n",
    "# trainX contém o bag of words e train_y contém os labels/categorias\n",
    "X = list(training_test[:, 0])\n",
    "y = list(training_test[:, 1])\n",
    "z = list(training_test[:, 2])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(' Train data: X_train(',len(X_train),'), y_train(',len(y_train),')')\n",
    "print(' Test data: X_test(',len(X_test),'), y_test(',len(y_test),')')\n",
    "print('Exemplo de X_train: ',X_train[0][:100],'...')\n",
    "print('Exemplo de y_train: ',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de uma Rede Neural DNN para ser treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criada a net1\n"
     ]
    }
   ],
   "source": [
    "# Reset do grafo\n",
    "tf.reset_default_graph()\n",
    "# Cria a rede neural\n",
    "net = None\n",
    "if REDE == 'net1':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='sigmoid')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net2':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='sigmoid')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net3':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='sigmoid')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net4':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, 50)\n",
    "    net = tflearn.fully_connected(net, 25)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='sigmoid')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net5':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.embedding(net, input_dim=len(X_train[0]), output_dim=len(y_train[0]))\n",
    "    net = tflearn.lstm(net, 8)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='sigmoid')\n",
    "    net = tflearn.regression(net, optimizer='adam',loss='binary_crossentropy')\n",
    "elif REDE == 'net6':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.embedding(net, input_dim=len(X_train[0]), output_dim=len(y_train[0]))\n",
    "    net = tflearn.lstm(net, 8)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='sigmoid')\n",
    "    net = tflearn.regression(net, optimizer='adam',loss='binary_crossentropy')\n",
    "# Define o modelo e configura o tensorboard\n",
    "print ('Criada a',REDE)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=3, tensorboard_dir='/home/03662232677/NLP/RFB/tflearn_logs_'+str(DATASET)+'/'+str(DATASET)+'_n'+str(LIMITE)+'_st'+str(STEPS)+'_'+str(REDE)+'_b'+str(BATCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da Rede Neural com os dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 22789  | total loss: \u001b[1m\u001b[32m6.98164\u001b[0m\u001b[0m | time: 1.195s\n",
      "| Adam | epoch: 215 | loss: 6.98164 - acc: 0.6468 -- iter: 1050/1056\n",
      "Training Step: 22790  | total loss: \u001b[1m\u001b[32m6.74398\u001b[0m\u001b[0m | time: 1.206s\n",
      "| Adam | epoch: 215 | loss: 6.74398 - acc: 0.6621 -- iter: 1056/1056\n",
      "--\n",
      "Epoch  215  with Accuracy  0.6621267199516296\n",
      "Successfully left training! Final model accuracy: 0.6621267199516296\n",
      "Caught callback exception. Returning control to user program.\n"
     ]
    }
   ],
   "source": [
    "# Initialize our callback with desired accuracy threshold.  \n",
    "early_stopping_cb = EarlyStoppingCallback(val_epoch_thresh=MIN_EPOCHS,val_acc_thresh=0.6)\n",
    "# Treinamento\n",
    "try:\n",
    "    model.fit(X_train, y_train, n_epoch = STEPS, batch_size = BATCH, show_metric = True, callbacks=early_stopping_cb)\n",
    "except StopIteration:\n",
    "    print(\"Caught callback exception. Returning control to user program.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a acurácia com dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia para 521 dados de teste: 0.5911708253358925\n"
     ]
    }
   ],
   "source": [
    "# Testando o modelo com a base de testes\n",
    "total = 0\n",
    "acertos = 0\n",
    "for x, y in zip(X_test, y_test):\n",
    "    result = np.rint(model.predict([x]))[0]\n",
    "    if result.astype(int).tolist() == y:\n",
    "       acertos += 1 \n",
    "    total += 1\n",
    "print(\"Acurácia para %s dados de teste: %s\" % (total, str(acertos/total)))  # only show first 3 probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a acurácia com dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia para 1056 dados de treino: 0.8115530303030303\n"
     ]
    }
   ],
   "source": [
    "# Testando o modelo com a base de testes\n",
    "total_train = 0\n",
    "acertos_train = 0\n",
    "for x, y in zip(X_train, y_train):\n",
    "    result = np.rint(model.predict([x]))[0]\n",
    "    if result.astype(int).tolist() == y:\n",
    "       acertos_train += 1 \n",
    "    total_train += 1\n",
    "print(\"Acurácia para %s dados de treino: %s\" % (total_train, str(acertos_train/total_train)))  # only show first 3 probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a acurácia com todos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia para 1577 dados de treino: 0.7577679137603044\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia para %s dados de treino: %s\" % \n",
    "      ((total+total_train), str((acertos+acertos_train)/(total+total_train))))  # only show first 3 probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando um teste de um documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1\n",
    "#print(X_test[N])\n",
    "print(np.rint(model.predict([X_test[N]]))[0])\n",
    "print(y_test[N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando um modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and pickle files\n",
    "import os\n",
    "model_name = 'model_'+str(DATASET)+'_n'+str(LIMITE)+'_st'+str(STEPS)+'_'+str(REDE)+'_b'+str(BATCH)\n",
    "directory = '/home/03662232677/NLP/RFB/'+model_name\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory) \n",
    "model.save(directory+'/'+model_name+'.tflearn')\n",
    "from shutil import copyfile\n",
    "copyfile('categories.pickle',directory+'/categories.pickle')\n",
    "save_list_to_file(all_ngrams,directory+'/all_ngrams.pickle')\n",
    "copyfile('docs.pickle',directory+'/docs.pickle')\n",
    "save_list_to_file(training_test_backup,directory+'/training_test_backup.pickle')\n",
    "save_list_to_file(training_test,directory+'/training_test.pickle')\n",
    "save_list_to_file(X_train,directory+'/X_train.pickle')\n",
    "save_list_to_file(X_test,directory+'/X_test.pickle')\n",
    "save_list_to_file(y_train,directory+'/y_train.pickle')\n",
    "save_list_to_file(y_test,directory+'/y_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
