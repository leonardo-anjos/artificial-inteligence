{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando Modelos com Python\n",
    "\n",
    "Uma forma também conhecida é treinar seu próprio modelo com sentenças conhecidas para que sirva como um classificador entre as classes desejadas, tais como de sentimento('positivo', 'neutro' e 'negativo'), assim como para emoções ('raiva', 'medo', 'alegria', 'tristeza', etc.)\n",
    "\n",
    "Existem basicamente duas formas de implementar:\n",
    "\n",
    "- Uso de modelo de Machine Learning tradicional (Ex: Naive Bayes)\n",
    "- Uso de Redes Neurais\n",
    "\n",
    "**Nota: Este Notebook utiliza algumas técnicas de processamento de texto em linguagem natural ou PLN, as quais poder ser conhecidas com maior profundidade no Curso de PLN.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um modelo com Naive Bayes\n",
    "\n",
    "**APRESENTAÇÃO**:\n",
    "\n",
    "No contexto de aprendizado de máquina, o [classificador Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) é um classificador probabilístico baseado no [teorema de Bayes](https://pt.wikipedia.org/wiki/Teorema_de_Bayes), que constrói um modelo de classificação a partir de dados de treinamento. Esse classificador aprende a classificar as revisões como positivas ou negativas usando o mecanismo de aprendizado supervisionado. O processo de aprendizagem começa alimentando dados de amostra que ajudam o classificador a construir um modelo para classificar essas revisões.\n",
    "\n",
    "**ESPECIFICAÇÕES**:\n",
    "\n",
    "Vamos utilizar a implementação do classificador Naive Bayes que existe na biblioteca NLTK do Python, a qual dispõe dos seguintes métodos principais:\n",
    "- **train()** utilizado para treinar o modelo\n",
    "- **classify()** utilizado após o treinamento, para classificar uma sentença\n",
    "\n",
    "Nota: Pelo fato de exigir um treinamento, um dos grandes custos dessa abordagem é ter que preparar uma base de treinamento contendo as sentenças que servirão de base para cada classe do modelo.\n",
    "\n",
    "**pode ser criado com sentenças em português**. \n",
    "\n",
    "- Site oficial: http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier\n",
    "- Documentação: https://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/03662232677/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 1\n",
    "#\n",
    "# Instalando a biblioteca nltk e baixando os pacotes necessários\n",
    "#\n",
    "!pip install --upgrade nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário: {'over', 'too', 'but', 'is', 'was', 'rice', 'so', 'you', 'not', 'great', 'being', 'spicy', 'be', 'ambience', 'visited', 'food', 'top', 'in', 'delicious', 'service', 'people', 'limited', 'are', 'slow', 'easy', 'little', 'fried', 'loved', 'i', 'because', 'seating', 'bangalore', 'to', 'renovated', 'when', 'probably', '.', 'place', 'mushroom', '-', ',', 'locate', 'the', 'many'}\n",
      "-------------------------------------------------------------\n",
      "Primeiro dado de treinamento: ({'over': False, 'too': False, 'but': False, 'is': False, 'was': False, 'rice': False, 'so': False, 'you': True, 'not': False, 'great': False, 'being': False, 'spicy': False, 'be': True, 'ambience': False, 'visited': False, 'food': False, 'top': False, 'in': True, 'delicious': False, 'service': False, 'people': False, 'limited': False, 'are': True, 'slow': False, 'easy': False, 'little': False, 'fried': False, 'loved': False, 'i': False, 'because': False, 'seating': False, 'bangalore': False, 'to': True, 'renovated': False, 'when': True, 'probably': False, '.': True, 'place': True, 'mushroom': False, '-': False, ',': False, 'locate': False, 'the': False, 'many': False}, 'pos')\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 2\n",
    "#\n",
    "# Preparando os dados para treino\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Array contendo as tuplas de dados no formato (sentença,classe)\n",
    "train = [(\"Great place to be when you are in Bangalore.\", \"pos\"),\n",
    "  (\"The place was being renovated when I visited so the seating was limited.\", \"neg\"),\n",
    "  (\"Loved the ambience, loved the food\", \"pos\"),\n",
    "  (\"The food is delicious but not over the top.\", \"neg\"),\n",
    "  (\"Service - Little slow, probably because too many people.\", \"neg\"),\n",
    "  (\"The place is not easy to locate\", \"neg\"),\n",
    "  (\"Mushroom fried rice was spicy\", \"pos\"),\n",
    "]\n",
    "# Criando um dicionário com todas as palavras\n",
    "dictionary = set(word.lower() for data_tuple in train for word in word_tokenize(data_tuple[0]))\n",
    "print('Dicionário: {}'.format(dictionary))\n",
    "print('-------------------------------------------------------------')\n",
    "# Criando os dados de treinamento\n",
    "train_data = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in train]\n",
    "# Imprimindo o primeiro dado de treinamento\n",
    "print('Primeiro dado de treinamento: {}'.format(train_data[0]))\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que os dados de treinamento foram criados criando-se um array com duas posições:\n",
    "- Na primeira existe um dicionário com todas as palavras e a indicação se a palavra está ou não presente com True/False\n",
    "- Na segunda a classe 'pos' ou 'neg'\n",
    "\n",
    "Este é um exemplo simples e não deve ser utilizado em modelos maiores, caso contrário o vetor cresce indefinidamente e se forna ineficiente. Nesse caso o ideal é limitar a quantidade desse vetor, como poderemos ver no próximo exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 3\n",
    "#\n",
    "# Treinamento e uso do modelo\n",
    "#\n",
    "# Treinamento do modelo\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
    "print('Modelo treinado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: {'over': False, 'too': False, 'but': False, 'is': False, 'was': True, 'rice': False, 'so': False, 'you': False, 'not': False, 'great': False, 'being': False, 'spicy': True, 'be': False, 'ambience': False, 'visited': False, 'food': False, 'top': False, 'in': False, 'delicious': False, 'service': False, 'people': False, 'limited': False, 'are': False, 'slow': False, 'easy': False, 'little': False, 'fried': False, 'loved': False, 'i': False, 'because': False, 'seating': False, 'bangalore': False, 'to': False, 'renovated': False, 'when': False, 'probably': False, '.': False, 'place': False, 'mushroom': False, '-': False, ',': False, 'locate': False, 'the': False, 'many': False}\n",
      "-------------------------------------------------------------\n",
      "Classificação: pos\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 4\n",
    "#\n",
    "# Executando um teste simples\n",
    "test_data = \"Manchurian was hot and spicy\"\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "print('Test data: {}'.format(test_data_features))\n",
    "print('-------------------------------------------------------------')\n",
    "print ('Classificação: {}'.format(classifier.classify(test_data_features)))\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 1\n",
    "\n",
    "Copie e cole as células dos Passos 2, 3 e 4 acima e coloque-as em sequência após as células acima.\n",
    "\n",
    "Adicione novas sentenças no Corpus no Passo 2 para tentar gerar um modelo melhor. Crie pelo menos 10 sentenças, sendo 5 positivas e 5 negativas.\n",
    "\n",
    "Treine o modelo no Passo 3\n",
    "\n",
    "No Passo 4 crie uma lista de três sentenças strings para serem testadas. Em seguida, crie um for para avaliar cada uma das sentenças\n",
    "\n",
    "O aumento do Corpus melhorou a acurácia do modelo? Por quê?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um exemplo simples de implementação. Veja no link abaixo como realizar melhorias no modelo:\n",
    "\n",
    "https://medium.com/@martinpella/naive-bayes-for-sentiment-analysis-49b37db18bf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um modelo com Redes Neurais\n",
    "\n",
    "**APRESENTAÇÃO**:\n",
    "\n",
    "Existem várias possibilidades de se criar um modelo de Rede Neural para criar um classificador que funcione adequadamente para análise de sentimento ou emoção. Podemos utilizar vários tipos de redes neurais, sendo que um dos modelos mais utilizados atualmente para PLN é do tipo [Rede Neural Recorrente ou RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) ou mais especificamente as  long [Long Short-Term Memory Networks ou LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory).\n",
    "\n",
    "Os seres humanos não começam a pensar desde zero a cada segundo. Ao ler este ensaio, você entende cada palavra com base a sua compreensão de palavras anteriores. Você não esquece tudo e começa a pensar do zero novamente. Os seus pensamentos têm persistência. As redes neurais tradicionais não podem fazer isso, e parece ser uma deficiência importante. As redes neurais recorrentes abordam (RNN) esta questão. São redes com loops, permitindo que as informações persistam.\n",
    "\n",
    "As redes de memória de longo prazo – geralmente chamadas de “LSTMs” – são um tipo especial de RNN, capaz de aprender dependências de longo prazo. Elas funcionam bem em problemas que exigem processamento de linguagem natural.\n",
    "\n",
    "**ESPECIFICAÇÕES**:\n",
    "\n",
    "Vamos utilizar na implementação desse modelo algumas bibliotecas Python:\n",
    "- **Pandas** pacote utilizado para lidar com grandes arquivos de dados\n",
    "- **Keras** framework simplificado para uso de redes neurais baseado no TensorFlow\n",
    "- **TensorFlow** framework para criação, treinamento e uso de Redes Neurais\n",
    "- **Scikit-Learn** pacote de ciência de dados que contém funções relevantes para a implementação\n",
    "\n",
    "**pode ser criado com sentenças em português**. \n",
    "\n",
    "- Sites oficiais e documentação: \n",
    "  - https://keras.io/\n",
    "  - https://www.tensorflow.org/?hl=pt-br\n",
    "  - https://pandas.pydata.org/\n",
    "  - https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo com Keras e LSTM\n",
    "\n",
    "Este exemplo consiste em utilizar uma base de avaliação de tweets dada pelo arquivo **Sentiment.csv**. \n",
    "\n",
    "Veja abaixo um exemplo de uma linha desse arquivo:\n",
    "\n",
    "```\n",
    "id,candidate,candidate_confidence,relevant_yn,relevant_yn_confidence,sentiment,sentiment_confidence,subject_matter,subject_matter_confidence,candidate_gold,name,relevant_yn_gold,retweet_count,sentiment_gold,subject_matter_gold,text,tweet_coord,tweet_created,tweet_id,tweet_location,user_timezone\n",
    "2,Scott Walker,1.0,yes,1.0,Positive,0.6333,None of the above,1.0,,PeacefulQuest,,26,,,RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfF…,,2015-08-07 09:54:46 -0700,629697199560069120,,\n",
    "```\n",
    "\n",
    "Nesse arquivo nos interessamos apenas pelas colunas 'text' e 'sentiment'. As demais colunas serão ignoradas.\n",
    "\n",
    "```\n",
    "text = \"RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfF…\"\n",
    "sentiment = Positive\n",
    "```\n",
    "\n",
    "Vamos utilizar o pacote **Pandas** para abrir e manipular o arquivo. \n",
    "\n",
    "Em seguida vamos utilizar o **Keras** para facilitar a criação de uma Rede Neural, tendo como base o **TensorFlow**.\n",
    "\n",
    "Também vamos utilizar o pacote **scikit-learn** para separar os dados de treino com a função train_test_split(). \n",
    "\n",
    "Fonte: https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando os pacotes para rodar o Keras com Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/8d/443591dd9f42cdde966a14ea2d59e7a781b77a8f09652288af61bec93b81/Keras_Preprocessing-1.0.8-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/f9/28787754923612ca9bfdffc588daa05580ed70698add063a5629d1a4209d/protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (0.32.3)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/a3/489ce6a67047263e9b0da8784b2925c4f89b688a6e33073c5bb6c4c2867f/grpcio-1.18.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: keras-preprocessing, absl-py, protobuf, werkzeug, markdown, grpcio, tensorboard, gast, astor, h5py, keras-applications, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.18.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.8 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-0.14.1\n"
     ]
    }
   ],
   "source": [
    "# Rode apenas uma vez para instalar o TensorFlow. Após rodar, comente esta linha\n",
    "# \n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'TensorFlow is working!'\n",
      "\n",
      "TensorFlow Version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Testando a instalação do TensorFlow\n",
    "# \n",
    "import tensorflow as tf\n",
    "with tf.Graph().as_default():\n",
    "    hello_constant = tf.constant('TensorFlow is working!')\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(hello_constant))\n",
    "tf.reset_default_graph()\n",
    "print('\\nTensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Collecting pyyaml (from keras)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.2.4 pyyaml-3.13\n"
     ]
    }
   ],
   "source": [
    "# Rode apenas uma vez para instalar o Keras. Após rodar, comente esta linha\n",
    "#\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras is working!\n",
      "\n",
      "Keras version: 2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Testando a instalação do Keras\n",
    "# \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "print('Keras is working!')\n",
    "print('\\nKeras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/f9/e1/4a63ed31e1b1362d40ce845a5735c717a959bda992669468dae3420af2cd/pandas-0.24.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from pandas) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-0.24.0\n",
      "Collecting sklearn\n",
      "Collecting scikit-learn (from sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Installing collected packages: scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.20.2 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# Rode apenas uma vez para instalar os pacotes pandas e scikit-learn. Após rodar, comente estas linhas\n",
    "# \n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando o exemplo Keras e LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo Keras e LSTM - Passo 1\n",
    "#\n",
    "# Realizando importações\n",
    "#\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Positivos: 4472\n",
      "Número de Negativos: 16986\n",
      "-------------------------------------------------------------\n",
      "Primeira linha do arquivo sem preparação: \n",
      "RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfF…\n",
      "-------------------------------------------------------------\n",
      "Primeira linha do arquivo após preparação: \n",
      "  scottwalker didnt catch the full gopdebate last night here are some of scotts best lines in 90 seconds walker16 httptcozsff\n",
      "-------------------------------------------------------------\n",
      "Vetor relativo à primeira linha do arquivo após utilizar Tokenizer:\n",
      "[363, 122, 1, 703, 2, 39, 58, 237, 37, 210, 6, 174, 1761, 12, 1324, 1409, 743]\n",
      "Tamanho: 17\n",
      "-------------------------------------------------------------\n",
      "Forma da matriz 2D Numpy\n",
      "(10729, 28)\n",
      "-------------------------------------------------------------\n",
      "Vetor relativo à primeira linha do arquivo após utilizar a função pad_sequences:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0  363  122    1\n",
      "  703    2   39   58  237   37  210    6  174 1761   12 1324 1409  743]\n",
      "-------------------------------------------------------------\n",
      "Sentimento associado à primeira linha do arquivo sem preparação:\n",
      "Positive\n",
      "-------------------------------------------------------------\n",
      "Sentimento associado à primeira linha do arquivo após preparação:\n",
      "[0 1]\n",
      "-------------------------------------------------------------\n",
      "Forma da matriz de resultado\n",
      "(10729, 2)\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 2\n",
    "#\n",
    "# Abrindo, manipulando o arquivo e preparando os dados\n",
    "#\n",
    "# Abrindo o arquivo com o Pandas\n",
    "data = pd.read_csv('Sentiment.csv')\n",
    "# Vamos manter apenas as colunas 'text' e 'sentiment'\n",
    "data = data[['text','sentiment']]\n",
    "# Removendo o sentimento do tipo 'Neutral'\n",
    "# Exibindo a quantidade de Positivos e Negativos\n",
    "print('Número de Positivos: {}'.format(data[ data['sentiment'] == 'Positive'].size))\n",
    "print('Número de Negativos: {}'.format(data[ data['sentiment'] == 'Negative'].size))\n",
    "print('-------------------------------------------------------------')\n",
    "data = data[data.sentiment != \"Neutral\"]\n",
    "print('Primeira linha do arquivo sem preparação: \\n{}'.format(data['text'][1]))\n",
    "print('-------------------------------------------------------------')\n",
    "# Transformando o texto em minúsculo e removendo caracteres especiais\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "# Removendo o início 'rt'\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "print('Primeira linha do arquivo após preparação: \\n{}'.format(data['text'][1]))\n",
    "print('-------------------------------------------------------------')\n",
    "# Limite de features (palavras) a serem consideradas no vocabulário\n",
    "MAX_FEATURES = 2000\n",
    "# Utiliza a função Tokenizer (https://keras.io/preprocessing/text/) para preparar os dados\n",
    "# Essa classe permite vetorizar um corpus de texto, transformando cada texto em uma sequência de inteiros \n",
    "# (cada inteiro sendo o índice de um token em um dicionário) ou em um vetor em que o coeficiente de cada\n",
    "# token pode ser binário, com base na contagem de palavras. , baseado em tf-idf ...\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "print('Vetor relativo à primeira linha do arquivo após utilizar Tokenizer:')\n",
    "print(X[0])\n",
    "print('Tamanho: {}'.format(len(X[0])))\n",
    "print('-------------------------------------------------------------')\n",
    "# A função pad_sequences (https://keras.io/preprocessing/sequence/) transforma uma lista de seqüências \n",
    "# de num_samples (listas de números inteiros) em uma matriz 2D Numpy de forma (num_samples, num_timesteps). \n",
    "# - num_timesteps é o argumento maxlen, se fornecido, ou o comprimento da sequência mais longa.\n",
    "# - sequências menores que num_timesteps são preenchidas com valor no final.\n",
    "X = pad_sequences(X)\n",
    "print('Forma da matriz 2D Numpy')\n",
    "print(X.shape)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Vetor relativo à primeira linha do arquivo após utilizar a função pad_sequences:')\n",
    "print(X[0])\n",
    "print('-------------------------------------------------------------')\n",
    "print('Sentimento associado à primeira linha do arquivo sem preparação:')\n",
    "print(data['sentiment'].values[0])\n",
    "# Formatação do vetor de resultado\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "print('-------------------------------------------------------------')\n",
    "print('Sentimento associado à primeira linha do arquivo após preparação:')\n",
    "print(Y[0])\n",
    "print('-------------------------------------------------------------')\n",
    "print('Forma da matriz de resultado')\n",
    "print(Y.shape)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que primeiro definimos que vamos considerar no vocabulário apenas as 2000 palavras (tokens) mais frequentes\n",
    "\n",
    "Em seguida criamos uma a matriz com os 10.729 tweets de exemplos, cuja dimensão máxima foi 28 (máximo de 28 tokens ou palavras)\n",
    "\n",
    "Nesses 28 tokens está o número relativo à posição da palavra no vetor das 2000 palavras mais frequentes (MAX_FEATURES)\n",
    "\n",
    "Veja que o primeiro exemplo que tinha apenas 17 tokens foi preenchido com '0' (zeros) no início para ficar com tamanho 28.\n",
    "\n",
    "Logo, a matriz de dados tem o formato de 10.729 exemplos e dimensão 28.\n",
    "\n",
    "A matriz de resultado tem o formato de 10.729 exemplos e dimensão 2.\n",
    "\n",
    "Nota: a etapa de preparação dos dados pode ser a mais complexa de todo o processo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 28, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 28, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 3\n",
    "#\n",
    "# Criando a Rede Neural. \n",
    "#\n",
    "tf.reset_default_graph()\n",
    "EMBEDDING_OUT_DIM = 128\n",
    "LSTM_OUT_DIM = 196\n",
    "model = Sequential()\n",
    "# Criando uma camada Embedding (https://keras.io/layers/embeddings/)\n",
    "# O primeiro atributo diz qual o tamanho do vocabulário, que no nosso caso é 2000 (MAX_FEATURES)\n",
    "# O segundo atributo é a dimensão dessa camada (quantidade de neurônios)\n",
    "# O parâmetro input_length é o tamanho dos dados de entrada, que no nosso caso é 28\n",
    "# Essa camada recebe os índices de tamanho 28 de cada tweet e monta o tensor de acordo com o vocabulário\n",
    "model.add(Embedding(MAX_FEATURES, EMBEDDING_OUT_DIM ,input_length = X.shape[1]))\n",
    "# Em seguida criamos uma camada SpatialDropout1D (https://keras.io/layers/core/) para evitar overfitting\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "# Finalmente criamos a camada LSTM com dimensão 196 (quantidade de neurônios)\n",
    "model.add(LSTM(LSTM_OUT_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "# Para a saída, criamos uma camada totalmente conectada com saída de dimensão 2 com softmax (retorna apenas uma classe)\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# Finalmente definimos a função de perda, de otimização e que métrica vamos utilizar\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# Imprime o blueprint da rede neural\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de treino: (7188, 28) (7188, 2)\n",
      "Dados de teste:  (3541, 28) (3541, 2)\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 4\n",
    "#\n",
    "# Aqui utilizamos a função train_test_split() do scikit-learn para separar adequadamente os dados de treino e teste\n",
    "#\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print('Dados de treino:',X_train.shape,Y_train.shape)\n",
    "print('Dados de teste: ',X_test.shape,Y_test.shape)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "7188/7188 [==============================] - 5s 715us/step - loss: 0.4386 - acc: 0.8158\n",
      "Epoch 2/7\n",
      "7188/7188 [==============================] - 5s 658us/step - loss: 0.3244 - acc: 0.8648\n",
      "Epoch 3/7\n",
      "7188/7188 [==============================] - 5s 657us/step - loss: 0.2863 - acc: 0.8762\n",
      "Epoch 4/7\n",
      "7188/7188 [==============================] - 5s 655us/step - loss: 0.2609 - acc: 0.8927\n",
      "Epoch 5/7\n",
      "7188/7188 [==============================] - 5s 663us/step - loss: 0.2298 - acc: 0.9076\n",
      "Epoch 6/7\n",
      "7188/7188 [==============================] - 5s 658us/step - loss: 0.2118 - acc: 0.9132\n",
      "Epoch 7/7\n",
      "7188/7188 [==============================] - 5s 661us/step - loss: 0.1876 - acc: 0.9257\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 5\n",
    "#\n",
    "# Realizando o treinamento da rede utilizando os dados de treinamento\n",
    "#\n",
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "# Ativando o Tensorboard para poder monitorar com o comando abaixo em uym outro terminal:\n",
    "# > tensorboard --logdir=logs/\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(datetime.datetime.today().strftime('%Y%m%d_%H%M')))\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 7\n",
    "model.fit(X_train, Y_train, epochs = EPOCHS, batch_size=BATCH_SIZE, verbose = 1, callbacks=[tensorboard])\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante ou após o treinamento você poderá acompanhar o resultado utilizando o TensorBoard\n",
    "\n",
    "Para isso abra outro terminal, ative o ambiente anaconda, navegue até a pasta imediatamente antes do diretório 'logs' e execute o comando abaixo:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=logs/\n",
    "```\n",
    "\n",
    "O resultado é que apareça uma mensagem como a abaixo:\n",
    "\n",
    "```\n",
    "TensorBoard 1.12.2 at http://serpro-1540796:6006 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "Copie o link acima e abra em um navegador para poder ver graficamente os valores de perda (loss) e acurácia (accuracy) além do grafo da rede. Serão exibidas as várias execuções, cada uma com uma cor de linha diferente\n",
    "\n",
    "- **Acurácia (acc)** Mede o percentual de acerto das previsões tendo como base os dados de entrada e os valores esperados. Quando maior melhor\n",
    "- **Perda (loss)** Função de perda: esse é o objetivo que o modelo tentará minimizar. Quanto menor melhor\n",
    "\n",
    "**Visualização da Perda e Acurácia**\n",
    "\n",
    "![Visualização da Perda e Acurácia](tensorboard_01.png)\n",
    "\n",
    "**Visualização do Grafo da Rede**\n",
    "\n",
    "![Visualização do Grafo da Rede](tensorboard_02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.83\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 6\n",
    "#\n",
    "# Validando o modelo com os dados de teste (dados inéditos)\n",
    "#\n",
    "loss,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"Acurácia: %.2f\" % (acc))\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o resultado da validação foi diferente do treinamento, o que era esperado, visto que os dados de teste nunca foram submetidos à rede durante o treinamento. Entretanto esse resultado da validação é o que devemos considerar pois o modelo uma vez treinado será submetido à dados inéditos e não conhecidos no processo de treinamento. Nosso objetivo é ter um modelo genérico.\n",
    "\n",
    "Podemos tentar melhorar o modelo realizando ajuste nas variáveis, tais como MAX_FEATURES, BATCH_SIZE, EPOCHS, ou mesmo modificando a quantidade de neurônios na rede e testando novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet original:\n",
      "Meetings: Because none of us is as dumb as all of us.\n",
      "-------------------------------------------------------------\n",
      "Tweet preparado:\n",
      "meetings because none of us is as dumb as all of us\n",
      "-------------------------------------------------------------\n",
      "Tweet vetorizado:\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  206  633    6  150    5   55 1055   55   46    6  150]]\n",
      "-------------------------------------------------------------\n",
      "Previsão:\n",
      "Negativo\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 7\n",
    "#\n",
    "# Executando o modelo treinado para um Tweet específico\n",
    "#\n",
    "print('Tweet original:')\n",
    "twt = 'Meetings: Because none of us is as dumb as all of us.'\n",
    "print(twt)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Tweet preparado:')\n",
    "# Preparando o texto\n",
    "twt = re.sub('[^a-zA-z0-9\\s]','',twt.lower())\n",
    "print(twt)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Tweet vetorizado:')\n",
    "twt = tokenizer.texts_to_sequences([twt])\n",
    "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "print (twt)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Previsão:')\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"Negativo\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"Positivo\")\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classe Positivo: 62.77777777777778 %\n",
      "Acurácia da classe Negativo: 88.0538816022687 %\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 8\n",
    "#\n",
    "# Validando o modelo para verificar a acurácia por cada classe (pos,neg)\n",
    "#\n",
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_test[x]):\n",
    "        if np.argmax(Y_test[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "    if np.argmax(Y_test[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "# Imprimindo os valores\n",
    "print(\"Acurácia da classe Positivo:\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"Acurácia da classe Negativo:\", neg_correct/neg_cnt*100, \"%\")\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto acima, a acurácia não está balanceada, o que era esperado, visto que temos mais amostras negativas do que positivas, como foi possível ver no Passo 2 deste tutorial:\n",
    "\n",
    "```\n",
    "Número de Positivos: 4472\n",
    "Número de Negativos: 16986\n",
    "```\n",
    "\n",
    "Uma forma de evitar isso é justamente balancear as amostras, o que poderá ser feito posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 2\n",
    "\n",
    "Como pode ser visto no código do Passo 2, o único pré-processamento de texto que está sendo feito se resume a:\n",
    "\n",
    "- transformar em minúsculo\n",
    "- remover caracteres especiais\n",
    "- remover o início 'rt'\n",
    "\n",
    "Veja que não aplicamos várias das técnicas de pré-processamento aprendidas no curso PLN Básico.\n",
    "\n",
    "Crie uma função conforme abaixo:\n",
    "\n",
    "    def preprocessing(text):\n",
    "    # Escrever aqui a função\n",
    "\n",
    "Implemente na função acima as seguintes técnicas:\n",
    "  \n",
    "- remover pontuação\n",
    "- remover stopwords para o idioma 'english'\n",
    "- aplicar o stemming (LancasterStemmer)\n",
    "\n",
    "Em seguida, adicione às linhas de pré-processamento no Passo 2 a linha abaixo, para aplicar a função de pré-processamento criada a toda a coluna 'text' do DataFrame:\n",
    "\n",
    "    data['text'] = data['text'].apply(preprocessing)\n",
    "\n",
    "Execute novamente os Passos de 2 a 6\n",
    "\n",
    "O resultado melhorou? Por quê?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 3\n",
    "\n",
    "Tente modificar os hiper parâmetros da rede neural para tentar melhorar o modelo:\n",
    "    \n",
    "- BATCH_SIZE\n",
    "- EPOCHS \n",
    "- quantidade de neurônios na rede\n",
    "\n",
    "Execute novamente os Passos de 2 a 6 e veja se surtiu algum efeito. Anote o resultado para cada modificação que fizer.\n",
    "\n",
    "Tente modificar também o parâmetro MAX_FEATURES.\n",
    "\n",
    "Qual o melhor resultado obtido? Quais parâmetros foram modificados?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma de muitas possíveis implementações de modelos preditivos de análise de sentimento com Redes Neurais em Python.\n",
    "\n",
    "No exemplo acima utilizamos apenas duas classes ('Positivo', 'Negativo'), mas nada impede que você construa modelos de análise de emoção com mais de uma classe ('Raiva', 'Medo', 'Alegria', 'Tristeza')\n",
    "\n",
    "Veja abaixo outros exemplos:\n",
    "    \n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "- https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
