{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando Modelos com Python\n",
    "\n",
    "Uma forma também conhecida é treinar seu próprio modelo com sentenças conhecidas para que sirva como um classificador entre as classes desejadas, tais como de sentimento('positivo', 'neutro' e 'negativo'), assim como para emoções ('raiva', 'medo', 'alegria', 'tristeza', etc.)\n",
    "\n",
    "Existem basicamente duas formas de implementar:\n",
    "\n",
    "- Uso de modelo de Machine Learning tradicional (Ex: Naive Bayes)\n",
    "- Uso de Redes Neurais\n",
    "\n",
    "**Nota: Este Notebook utiliza algumas técnicas de processamento de texto em linguagem natural ou PLN, as quais poder ser conhecidas com maior profundidade no Curso de PLN.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um modelo com Naive Bayes\n",
    "\n",
    "**APRESENTAÇÃO**:\n",
    "\n",
    "No contexto de aprendizado de máquina, o [classificador Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) é um classificador probabilístico baseado no [teorema de Bayes](https://pt.wikipedia.org/wiki/Teorema_de_Bayes), que constrói um modelo de classificação a partir de dados de treinamento. Esse classificador aprende a classificar as revisões como positivas ou negativas usando o mecanismo de aprendizado supervisionado. O processo de aprendizagem começa alimentando dados de amostra que ajudam o classificador a construir um modelo para classificar essas revisões.\n",
    "\n",
    "**ESPECIFICAÇÕES**:\n",
    "\n",
    "Vamos utilizar a implementação do classificador Naive Bayes que existe na biblioteca NLTK do Python, a qual dispõe dos seguintes métodos principais:\n",
    "- **train()** utilizado para treinar o modelo\n",
    "- **classify()** utilizado após o treinamento, para classificar uma sentença\n",
    "\n",
    "Nota: Pelo fato de exigir um treinamento, um dos grandes custos dessa abordagem é ter que preparar uma base de treinamento contendo as sentenças que servirão de base para cada classe do modelo.\n",
    "\n",
    "**pode ser criado com sentenças em português**. \n",
    "\n",
    "- Site oficial: http://www.nltk.org/api/nltk.classify.html?highlight=naivebayesclassifier\n",
    "- Documentação: https://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (3.4)\r\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from nltk) (3.4.0.3)\r\n",
      "Requirement already satisfied, skipping upgrade: six in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from nltk) (1.12.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/03662232677/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 1\n",
    "#\n",
    "# Instalando a biblioteca nltk e baixando os pacotes necessários\n",
    "#\n",
    "!pip install --upgrade nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário: {'being', 'not', 'to', 'many', 'ambience', 'slow', 'limited', 'loved', 'top', 'food', '-', 'renovated', 'visited', 'bangalore', 'probably', 'locate', 'is', 'over', 'little', 'fried', 'because', 'in', 'mushroom', 'spicy', 'people', '.', 'the', 'so', 'be', 'are', 'delicious', 'place', 'too', ',', 'easy', 'great', 'you', 'i', 'but', 'service', 'seating', 'when', 'rice', 'was'}\n",
      "-------------------------------------------------------------\n",
      "1 -------------------------------------------------------------\n",
      "({'being': False, 'not': False, 'to': True, 'many': False, 'ambience': False, 'slow': False, 'limited': False, 'loved': False, 'top': False, 'food': False, '-': False, 'renovated': False, 'visited': False, 'bangalore': True, 'probably': False, 'locate': False, 'is': False, 'over': False, 'little': False, 'fried': False, 'because': False, 'in': True, 'mushroom': False, 'spicy': False, 'people': False, '.': True, 'the': False, 'so': False, 'be': True, 'are': True, 'delicious': False, 'place': True, 'too': False, ',': False, 'easy': False, 'great': True, 'you': True, 'i': False, 'but': False, 'service': False, 'seating': False, 'when': True, 'rice': False, 'was': False}, 'pos')\n",
      "2 -------------------------------------------------------------\n",
      "({'being': True, 'not': False, 'to': False, 'many': False, 'ambience': False, 'slow': False, 'limited': True, 'loved': False, 'top': False, 'food': False, '-': False, 'renovated': True, 'visited': True, 'bangalore': False, 'probably': False, 'locate': False, 'is': False, 'over': False, 'little': False, 'fried': False, 'because': False, 'in': False, 'mushroom': False, 'spicy': False, 'people': False, '.': True, 'the': True, 'so': True, 'be': False, 'are': False, 'delicious': False, 'place': True, 'too': False, ',': False, 'easy': False, 'great': False, 'you': False, 'i': True, 'but': False, 'service': False, 'seating': True, 'when': True, 'rice': False, 'was': True}, 'neg')\n",
      "3 -------------------------------------------------------------\n",
      "({'being': False, 'not': False, 'to': False, 'many': False, 'ambience': True, 'slow': False, 'limited': False, 'loved': True, 'top': False, 'food': True, '-': False, 'renovated': False, 'visited': False, 'bangalore': False, 'probably': False, 'locate': False, 'is': False, 'over': False, 'little': False, 'fried': False, 'because': False, 'in': False, 'mushroom': False, 'spicy': False, 'people': False, '.': False, 'the': True, 'so': False, 'be': False, 'are': False, 'delicious': False, 'place': False, 'too': False, ',': True, 'easy': False, 'great': False, 'you': False, 'i': False, 'but': False, 'service': False, 'seating': False, 'when': False, 'rice': False, 'was': False}, 'pos')\n",
      "4 -------------------------------------------------------------\n",
      "({'being': False, 'not': True, 'to': False, 'many': False, 'ambience': False, 'slow': False, 'limited': False, 'loved': False, 'top': True, 'food': True, '-': False, 'renovated': False, 'visited': False, 'bangalore': False, 'probably': False, 'locate': False, 'is': True, 'over': True, 'little': False, 'fried': False, 'because': False, 'in': False, 'mushroom': False, 'spicy': False, 'people': False, '.': True, 'the': True, 'so': False, 'be': False, 'are': False, 'delicious': True, 'place': False, 'too': False, ',': False, 'easy': False, 'great': False, 'you': False, 'i': False, 'but': True, 'service': False, 'seating': False, 'when': False, 'rice': False, 'was': False}, 'neg')\n",
      "5 -------------------------------------------------------------\n",
      "({'being': False, 'not': False, 'to': False, 'many': True, 'ambience': False, 'slow': True, 'limited': False, 'loved': False, 'top': False, 'food': False, '-': True, 'renovated': False, 'visited': False, 'bangalore': False, 'probably': True, 'locate': False, 'is': False, 'over': False, 'little': True, 'fried': False, 'because': True, 'in': False, 'mushroom': False, 'spicy': False, 'people': True, '.': True, 'the': False, 'so': False, 'be': False, 'are': False, 'delicious': False, 'place': False, 'too': True, ',': True, 'easy': False, 'great': False, 'you': False, 'i': False, 'but': False, 'service': True, 'seating': False, 'when': False, 'rice': False, 'was': False}, 'neg')\n",
      "6 -------------------------------------------------------------\n",
      "({'being': False, 'not': True, 'to': True, 'many': False, 'ambience': False, 'slow': False, 'limited': False, 'loved': False, 'top': False, 'food': False, '-': False, 'renovated': False, 'visited': False, 'bangalore': False, 'probably': False, 'locate': True, 'is': True, 'over': False, 'little': False, 'fried': False, 'because': False, 'in': False, 'mushroom': False, 'spicy': False, 'people': False, '.': False, 'the': True, 'so': False, 'be': False, 'are': False, 'delicious': False, 'place': True, 'too': False, ',': False, 'easy': True, 'great': False, 'you': False, 'i': False, 'but': False, 'service': False, 'seating': False, 'when': False, 'rice': False, 'was': False}, 'neg')\n",
      "7 -------------------------------------------------------------\n",
      "({'being': False, 'not': False, 'to': False, 'many': False, 'ambience': False, 'slow': False, 'limited': False, 'loved': False, 'top': False, 'food': False, '-': False, 'renovated': False, 'visited': False, 'bangalore': False, 'probably': False, 'locate': False, 'is': False, 'over': False, 'little': False, 'fried': True, 'because': False, 'in': False, 'mushroom': True, 'spicy': True, 'people': False, '.': False, 'the': False, 'so': False, 'be': False, 'are': False, 'delicious': False, 'place': False, 'too': False, ',': False, 'easy': False, 'great': False, 'you': False, 'i': False, 'but': False, 'service': False, 'seating': False, 'when': False, 'rice': True, 'was': True}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 2\n",
    "#\n",
    "# Preparando os dados para treino\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Array contendo as tuplas de dados no formato (sentença,classe)\n",
    "train = [(\"Great place to be when you are in Bangalore.\", \"pos\"),\n",
    "  (\"The place was being renovated when I visited so the seating was limited.\", \"neg\"),\n",
    "  (\"Loved the ambience, loved the food\", \"pos\"),\n",
    "  (\"The food is delicious but not over the top.\", \"neg\"),\n",
    "  (\"Service - Little slow, probably because too many people.\", \"neg\"),\n",
    "  (\"The place is not easy to locate\", \"neg\"),\n",
    "  (\"Mushroom fried rice was spicy\", \"pos\"),\n",
    "]\n",
    "# Criando um dicionário com todas as palavras\n",
    "#sentencas = [tuple[0] for tuple in train]\n",
    "#print(sentencas)\n",
    "#words = [word_tokenize(sentenca) for sentenca in sentencas]\n",
    "#print(words)\n",
    "dictionary = set(word.lower() for data_tuple in train for word in word_tokenize(data_tuple[0]))\n",
    "print('Dicionário: {}'.format(dictionary))\n",
    "print('-------------------------------------------------------------')\n",
    "# Criando os dados de treinamento\n",
    "train_data = [({word: (word in word_tokenize(x[0].lower())) for word in dictionary}, x[1]) for x in train]\n",
    "# Imprimindo o primeiro dado de treinamento\n",
    "for i,data in enumerate(train_data):\n",
    "    print(i+1,'-------------------------------------------------------------')\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que os dados de treinamento foram criados criando-se um array com duas posições:\n",
    "- Na primeira existe um dicionário com todas as palavras e a indicação se a palavra está ou não presente com True/False\n",
    "- Na segunda a classe 'pos' ou 'neg'\n",
    "\n",
    "Este é um exemplo simples e não deve ser utilizado em modelos maiores, caso contrário o vetor cresce indefinidamente e se forna ineficiente. Nesse caso o ideal é limitar a quantidade desse vetor, como poderemos ver no próximo exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 3\n",
    "#\n",
    "# Treinamento e uso do modelo\n",
    "#\n",
    "# Treinamento do modelo\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
    "print('Modelo treinado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: {'being': False, 'not': False, 'to': False, 'many': False, 'ambience': False, 'slow': False, 'limited': False, 'loved': False, 'top': False, 'food': False, '-': False, 'renovated': False, 'visited': False, 'bangalore': False, 'probably': False, 'locate': False, 'is': False, 'over': False, 'little': False, 'fried': False, 'because': False, 'in': False, 'mushroom': False, 'spicy': True, 'people': False, '.': False, 'the': False, 'so': False, 'be': False, 'are': False, 'delicious': False, 'place': False, 'too': False, ',': False, 'easy': False, 'great': False, 'you': False, 'i': False, 'but': False, 'service': False, 'seating': False, 'when': False, 'rice': False, 'was': True}\n",
      "-------------------------------------------------------------\n",
      "Classificação: pos\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Executando um teste simples\n",
    "test_data = \"Manchurian was hot and spicy\"\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "print('Test data: {}'.format(test_data_features))\n",
    "print('-------------------------------------------------------------')\n",
    "print ('Classificação: {}'.format(classifier.classify(test_data_features)))\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário: {'gostei', 'jogo', 'história', 'restaurante', 'muitas', 'passeio', 'críticas', 'bicicleta', 'futebol', 'legal', 'teatro', 'quanto', 'super', 'bom', 'lugar', 'frio', 'programação', 'achei', 'viajar', '.', 'pois', 'adorei', 'pésssima', 'daquele', ',', 'fizemos', 'comida', 'divertido', 'ir', 'filme', 'cinema', 'massa', 'ótimo', 'ruim', 'detestei'}\n",
      "-------------------------------------------------------------\n",
      "({'gostei': False, 'jogo': False, 'história': False, 'restaurante': False, 'muitas': False, 'passeio': False, 'críticas': False, 'bicicleta': False, 'futebol': False, 'legal': False, 'teatro': False, 'quanto': False, 'super': False, 'bom': True, 'lugar': False, 'frio': False, 'programação': False, 'achei': False, 'viajar': False, '.': True, 'pois': False, 'adorei': True, 'pésssima': False, 'daquele': False, ',': False, 'fizemos': False, 'comida': False, 'divertido': False, 'ir': True, 'filme': False, 'cinema': True, 'massa': False, 'ótimo': False, 'ruim': False, 'detestei': False}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 2\n",
    "#\n",
    "# Preparando os dados para treino\n",
    "#\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "portuguese_stemmer = RSLPStemmer()\n",
    "# Array contendo as tuplas de dados no formato (sentença,classe)\n",
    "train = [(\"Este filme foi muito ruim\", \"neg\"),\n",
    "  (\"Achei que a história foi pésssima\", \"neg\"),\n",
    "  (\"Tenho muitas críticas quanto ao teatro que fomos\", \"neg\"),\n",
    "  (\"Detestei viajar para aquele lugar, pois estava muito frio\", \"neg\"),\n",
    "  (\"Não gostei da comida daquele restaurante\", \"neg\"),\n",
    "  (\"Adorei ir ao cinema. Foi muito bom\", \"pos\"),\n",
    "  (\"Foi muito legal o passeio de bicicleta\", \"pos\"),\n",
    "  (\"Achei ótimo e divertido o jogo de futebol\", \"pos\"),\n",
    "  (\"Foi massa ir ao teatro\", \"pos\"),\n",
    "  (\"Super legal a programação que fizemos\", \"pos\")\n",
    "]\n",
    "# Criando um dicionário com todas as palavras\n",
    "#sentencas = [tuple[0] for tuple in train]\n",
    "#print(sentencas)\n",
    "#words = [word_tokenize(sentenca) for sentenca in sentencas]\n",
    "#print(words)\n",
    "dictionary = set(word.lower() for data_tuple in train for word in word_tokenize(data_tuple[0]))\n",
    "palavras_sem_stopwords = [palavra for palavra in dictionary if palavra not in portuguese_stops]\n",
    "dictionary = set(palavras_sem_stopwords)\n",
    "print('Dicionário: {}'.format(dictionary))\n",
    "print('-------------------------------------------------------------')\n",
    "# Criando os dados de treinamento\n",
    "train_data = [({word: (word in word_tokenize(x[0].lower())) for word in dictionary}, x[1]) for x in train]\n",
    "# Imprimindo o primeiro dado de treinamento\n",
    "#for i,data in enumerate(train_data):\n",
    "#    print(i+1,'-------------------------------------------------------------')\n",
    "#    print(data)\n",
    "print(train_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Naive Bayes - Passo 3\n",
    "#\n",
    "# Treinamento e uso do modelo\n",
    "#\n",
    "# Treinamento do modelo\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
    "print('Modelo treinado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: {'gostei': False, 'jogo': False, 'história': False, 'restaurante': False, 'muitas': False, 'passeio': False, 'críticas': False, 'bicicleta': False, 'futebol': False, 'legal': False, 'teatro': False, 'quanto': False, 'super': False, 'bom': False, 'lugar': False, 'frio': False, 'programação': False, 'achei': True, 'viajar': False, '.': False, 'pois': False, 'adorei': False, 'pésssima': False, 'daquele': False, ',': False, 'fizemos': False, 'comida': False, 'divertido': False, 'ir': False, 'filme': True, 'cinema': False, 'massa': False, 'ótimo': True, 'ruim': False, 'detestei': False}\n",
      "-------------------------------------------------------------\n",
      "Classificação: neg\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Executando um teste simples\n",
    "test_data = \"Achei o filme ótimo\"\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "print('Test data: {}'.format(test_data_features))\n",
    "print('-------------------------------------------------------------')\n",
    "print ('Classificação: {}'.format(classifier.classify(test_data_features)))\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um exemplo simples de implementação. Veja no link abaixo como realizar melhorias no modelo:\n",
    "\n",
    "https://medium.com/@martinpella/naive-bayes-for-sentiment-analysis-49b37db18bf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um modelo com Redes Neurais\n",
    "\n",
    "**APRESENTAÇÃO**:\n",
    "\n",
    "Existem várias possibilidades de se criar um modelo de Rede Neural para criar um classificador que funcione adequadamente para análise de sentimento ou emoção. Podemos utilizar vários tipos de redes neurais, sendo que um dos modelos mais utilizados atualmente para PLN é do tipo [Rede Neural Recorrente ou RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) ou mais especificamente as  long [Long Short-Term Memory Networks ou LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory).\n",
    "\n",
    "Os seres humanos não começam a pensar desde zero a cada segundo. Ao ler este ensaio, você entende cada palavra com base a sua compreensão de palavras anteriores. Você não esquece tudo e começa a pensar do zero novamente. Os seus pensamentos têm persistência. As redes neurais tradicionais não podem fazer isso, e parece ser uma deficiência importante. As redes neurais recorrentes abordam (RNN) esta questão. São redes com loops, permitindo que as informações persistam.\n",
    "\n",
    "As redes de memória de longo prazo – geralmente chamadas de “LSTMs” – são um tipo especial de RNN, capaz de aprender dependências de longo prazo. Elas funcionam bem em problemas que exigem processamento de linguagem natural.\n",
    "\n",
    "**ESPECIFICAÇÕES**:\n",
    "\n",
    "Vamos utilizar na implementação desse modelo algumas bibliotecas Python:\n",
    "- **Pandas** pacote utilizado para lidar com grandes arquivos de dados\n",
    "- **Keras** framework simplificado para uso de redes neurais baseado no TensorFlow\n",
    "- **TensorFlow** framework para criação, treinamento e uso de Redes Neurais\n",
    "- **Scikit-Learn** pacote de ciência de dados que contém funções relevantes para a implementação\n",
    "\n",
    "**pode ser criado com sentenças em português**. \n",
    "\n",
    "- Sites oficiais e documentação: \n",
    "  - https://keras.io/\n",
    "  - https://www.tensorflow.org/?hl=pt-br\n",
    "  - https://pandas.pydata.org/\n",
    "  - https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo com Keras e LSTM\n",
    "\n",
    "Este exemplo consiste em utilizar uma base de avaliação de tweets dada pelo arquivo **Sentiment.csv**. \n",
    "\n",
    "Veja abaixo um exemplo de uma linha desse arquivo:\n",
    "\n",
    "```\n",
    "id,candidate,candidate_confidence,relevant_yn,relevant_yn_confidence,sentiment,sentiment_confidence,subject_matter,subject_matter_confidence,candidate_gold,name,relevant_yn_gold,retweet_count,sentiment_gold,subject_matter_gold,text,tweet_coord,tweet_created,tweet_id,tweet_location,user_timezone\n",
    "2,Scott Walker,1.0,yes,1.0,Positive,0.6333,None of the above,1.0,,PeacefulQuest,,26,,,RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfF…,,2015-08-07 09:54:46 -0700,629697199560069120,,\n",
    "```\n",
    "\n",
    "Nesse arquivo nos interessamos apenas pelas colunas 'text' e 'sentiment'. As demais colunas serão ignoradas.\n",
    "\n",
    "```\n",
    "text = \"RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfF…\"\n",
    "sentiment = Positive\n",
    "```\n",
    "\n",
    "Vamos utilizar o pacote **Pandas** para abrir e manipular o arquivo. \n",
    "\n",
    "Em seguida vamos utilizar o **Keras** para facilitar a criação de uma Rede Neural, tendo como base o **TensorFlow**.\n",
    "\n",
    "Também vamos utilizar o pacote **scikit-learn** para separar os dados de treino com a função train_test_split(). \n",
    "\n",
    "Fonte: https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando os pacotes para rodar o Keras com Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.18.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.14.0,>=1.13.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Rode apenas uma vez para instalar o TensorFlow. Após rodar, comente esta linha\n",
    "# \n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'TensorFlow is working!'\n",
      "\n",
      "TensorFlow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Testando a instalação do TensorFlow\n",
    "# \n",
    "import tensorflow as tf\n",
    "with tf.Graph().as_default():\n",
    "    hello_constant = tf.constant('TensorFlow is working!')\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(hello_constant))\n",
    "tf.reset_default_graph()\n",
    "print('\\nTensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from keras) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Rode apenas uma vez para instalar o Keras. Após rodar, comente esta linha\n",
    "#\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras is working!\n",
      "\n",
      "Keras version: 2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Testando a instalação do Keras\n",
    "# \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "print('Keras is working!')\n",
    "print('\\nKeras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from pandas) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already up-to-date: sklearn in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from sklearn) (0.20.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Rode apenas uma vez para instalar os pacotes pandas e scikit-learn. Após rodar, comente estas linhas\n",
    "# \n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando o exemplo Keras e LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo Keras e LSTM - Passo 1\n",
    "#\n",
    "# Realizando importações\n",
    "#\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# Estrutura para armazenar pontuações\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# Função para remover pontuações das sentenças\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def preprocessing(text):\n",
    "        \n",
    "    #Remover pontuação\n",
    "    text= remove_punctuation(text)\n",
    "    \n",
    "    #Tokenização\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    # Coloca em minúscula\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    # Existe um pacote de stop words padrão para português -> stopwords.words('portuguese')\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    #Realizar o Stem\n",
    "    stemmer = LancasterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatizer: opção em relação ao stem \n",
    "    #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    #Retonar o texto preprocessado agrupando as tokens\n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Positivos: 4472\n",
      "Número de Negativos: 16986\n",
      "-------------------------------------------------------------\n",
      "Primeira linha do arquivo sem preparação: \n",
      "RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfF…\n",
      "-------------------------------------------------------------\n",
      "Primeira linha do arquivo após preparação: \n",
      "  scottwalk didnt catch ful gopdeb last night scot best lin 90 second walker16 httptcozsff\n",
      "-------------------------------------------------------------\n",
      "Tamanho do Vocabulário:  12045\n",
      "Vetor relativo à primeira linha do arquivo após utilizar Tokenizer:\n",
      "[264, 72, 1203, 591, 1, 12, 11, 209, 125, 370, 1129, 483, 644]\n",
      "Tamanho: 13\n",
      "-------------------------------------------------------------\n",
      "Forma da matriz 2D Numpy\n",
      "(10729, 22)\n",
      "-------------------------------------------------------------\n",
      "Vetor relativo à primeira linha do arquivo após utilizar a função pad_sequences:\n",
      "[   0    0    0    0    0    0    0    0    0  264   72 1203  591    1\n",
      "   12   11  209  125  370 1129  483  644]\n",
      "-------------------------------------------------------------\n",
      "Sentimento associado à primeira linha do arquivo sem preparação:\n",
      "Positive\n",
      "-------------------------------------------------------------\n",
      "Sentimento associado à primeira linha do arquivo após preparação:\n",
      "[0 1]\n",
      "-------------------------------------------------------------\n",
      "Forma da matriz de resultado\n",
      "(10729, 2)\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 2\n",
    "#\n",
    "# Abrindo, manipulando o arquivo e preparando os dados\n",
    "#\n",
    "# Abrindo o arquivo com o Pandas\n",
    "data = pd.read_csv('Sentiment.csv')\n",
    "# Vamos manter apenas as colunas 'text' e 'sentiment'\n",
    "data = data[['text','sentiment']]\n",
    "# Removendo o sentimento do tipo 'Neutral'\n",
    "# Exibindo a quantidade de Positivos e Negativos\n",
    "print('Número de Positivos: {}'.format(data[ data['sentiment'] == 'Positive'].size))\n",
    "print('Número de Negativos: {}'.format(data[ data['sentiment'] == 'Negative'].size))\n",
    "print('-------------------------------------------------------------')\n",
    "data = data[data.sentiment != \"Neutral\"]\n",
    "print('Primeira linha do arquivo sem preparação: \\n{}'.format(data['text'][1]))\n",
    "print('-------------------------------------------------------------')\n",
    "# Transformando o texto em minúsculo e removendo caracteres especiais\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['text'] = data['text'].apply(preprocessing)\n",
    "# Removendo o início 'rt'\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "print('Primeira linha do arquivo após preparação: \\n{}'.format(data['text'][1]))\n",
    "print('-------------------------------------------------------------')\n",
    "# Limite de features (palavras) a serem consideradas no vocabulário\n",
    "MAX_FEATURES = 2000\n",
    "# Utiliza a função Tokenizer (https://keras.io/preprocessing/text/) para preparar os dados\n",
    "# Essa classe permite vetorizar um corpus de texto, transformando cada texto em uma sequência de inteiros \n",
    "# (cada inteiro sendo o índice de um token em um dicionário) ou em um vetor em que o coeficiente de cada\n",
    "# token pode ser binário, com base na contagem de palavras. , baseado em tf-idf ...\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "print(\"Tamanho do Vocabulário: \", len(tokenizer.word_index))\n",
    "#print(\"Palavras do Vocabulário: \", tokenizer.word_index)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "print('Vetor relativo à primeira linha do arquivo após utilizar Tokenizer:')\n",
    "print(X[0])\n",
    "print('Tamanho: {}'.format(len(X[0])))\n",
    "print('-------------------------------------------------------------')\n",
    "# A função pad_sequences (https://keras.io/preprocessing/sequence/) transforma uma lista de seqüências \n",
    "# de num_samples (listas de números inteiros) em uma matriz 2D Numpy de forma (num_samples, num_timesteps). \n",
    "# - num_timesteps é o argumento maxlen, se fornecido, ou o comprimento da sequência mais longa.\n",
    "# - sequências menores que num_timesteps são preenchidas com valor no final.\n",
    "X = pad_sequences(X)\n",
    "print('Forma da matriz 2D Numpy')\n",
    "print(X.shape)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Vetor relativo à primeira linha do arquivo após utilizar a função pad_sequences:')\n",
    "print(X[0])\n",
    "print('-------------------------------------------------------------')\n",
    "print('Sentimento associado à primeira linha do arquivo sem preparação:')\n",
    "print(data['sentiment'].values[0])\n",
    "# Formatação do vetor de resultado\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "print('-------------------------------------------------------------')\n",
    "print('Sentimento associado à primeira linha do arquivo após preparação:')\n",
    "print(Y[0])\n",
    "print('-------------------------------------------------------------')\n",
    "print('Forma da matriz de resultado')\n",
    "print(Y.shape)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que primeiro definimos que vamos considerar no vocabulário apenas as 2000 palavras (tokens) mais frequentes\n",
    "\n",
    "Em seguida criamos uma a matriz com os 10.729 tweets de exemplos, cuja dimensão máxima foi 28 (máximo de 28 tokens ou palavras)\n",
    "\n",
    "Nesses 28 tokens está o número relativo à posição da palavra no vetor das 2000 palavras mais frequentes (MAX_FEATURES)\n",
    "\n",
    "Veja que o primeiro exemplo que tinha apenas 17 tokens foi preenchido com '0' (zeros) no início para ficar com tamanho 28.\n",
    "\n",
    "Logo, a matriz de dados tem o formato de 10.729 exemplos e dimensão 28.\n",
    "\n",
    "A matriz de resultado tem o formato de 10.729 exemplos e dimensão 2.\n",
    "\n",
    "Nota: a etapa de preparação dos dados pode ser a mais complexa de todo o processo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/03662232677/anaconda3/envs/pln_ase/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 22, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 387,842\n",
      "Trainable params: 387,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 3\n",
    "#\n",
    "# Criando a Rede Neural. \n",
    "#\n",
    "tf.reset_default_graph()\n",
    "EMBEDDING_OUT_DIM = 128\n",
    "LSTM_OUT_DIM = 128\n",
    "model = Sequential()\n",
    "# Criando uma camada Embedding (https://keras.io/layers/embeddings/)\n",
    "# O primeiro atributo diz qual o tamanho do vocabulário, que no nosso caso é 2000 (MAX_FEATURES)\n",
    "# O segundo atributo é a dimensão dessa camada (quantidade de neurônios)\n",
    "# O parâmetro input_length é o tamanho dos dados de entrada, que no nosso caso é 28\n",
    "# Essa camada recebe os índices de tamanho 28 de cada tweet e monta o tensor de acordo com o vocabulário\n",
    "model.add(Embedding(MAX_FEATURES, EMBEDDING_OUT_DIM ,input_length = X.shape[1]))\n",
    "# Em seguida criamos uma camada SpatialDropout1D (https://keras.io/layers/core/) para evitar overfitting\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "# Finalmente criamos a camada LSTM com dimensão 196 (quantidade de neurônios)\n",
    "model.add(LSTM(LSTM_OUT_DIM, dropout=0.5, recurrent_dropout=0.5))\n",
    "# Para a saída, criamos uma camada totalmente conectada com saída de dimensão 2 com softmax (retorna apenas uma classe)\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# Finalmente definimos a função de perda, de otimização e que métrica vamos utilizar\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# Imprime o blueprint da rede neural\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de treino: (7188, 22) (7188, 2)\n",
      "Dados de teste:  (3541, 22) (3541, 2)\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 4\n",
    "#\n",
    "# Aqui utilizamos a função train_test_split() do scikit-learn para separar adequadamente os dados de treino e teste\n",
    "#\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print('Dados de treino:',X_train.shape,Y_train.shape)\n",
    "print('Dados de teste: ',X_test.shape,Y_test.shape)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7188/7188 [==============================] - 3s 451us/step - loss: 0.2341 - acc: 0.9032\n",
      "Epoch 2/10\n",
      "7188/7188 [==============================] - 3s 479us/step - loss: 0.2197 - acc: 0.9083\n",
      "Epoch 3/10\n",
      "7188/7188 [==============================] - 3s 475us/step - loss: 0.2168 - acc: 0.9073\n",
      "Epoch 4/10\n",
      "7188/7188 [==============================] - 4s 550us/step - loss: 0.2040 - acc: 0.9128\n",
      "Epoch 5/10\n",
      "7188/7188 [==============================] - 3s 481us/step - loss: 0.1960 - acc: 0.9175\n",
      "Epoch 6/10\n",
      "7188/7188 [==============================] - 4s 521us/step - loss: 0.1927 - acc: 0.9162\n",
      "Epoch 7/10\n",
      "7188/7188 [==============================] - 4s 524us/step - loss: 0.1817 - acc: 0.9224\n",
      "Epoch 8/10\n",
      "7188/7188 [==============================] - 4s 489us/step - loss: 0.1784 - acc: 0.9242\n",
      "Epoch 9/10\n",
      "7188/7188 [==============================] - 4s 488us/step - loss: 0.1713 - acc: 0.9277\n",
      "Epoch 10/10\n",
      "7188/7188 [==============================] - 3s 470us/step - loss: 0.1638 - acc: 0.9325\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 5\n",
    "#\n",
    "# Realizando o treinamento da rede utilizando os dados de treinamento\n",
    "#\n",
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "# Ativando o Tensorboard para poder monitorar com o comando abaixo em uym outro terminal:\n",
    "# > tensorboard --logdir=logs/\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(datetime.datetime.today().strftime('%Y%m%d_%H%M')))\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "model.fit(X_train, Y_train, epochs = EPOCHS, batch_size=BATCH_SIZE, verbose = 1, callbacks=[tensorboard])\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante ou após o treinamento você poderá acompanhar o resultado utilizando o TensorBoard\n",
    "\n",
    "Para isso abra outro terminal, ative o ambiente anaconda, navegue até a pasta imediatamente antes do diretório 'logs' e execute o comando abaixo:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=logs/\n",
    "```\n",
    "\n",
    "O resultado é que apareça uma mensagem como a abaixo:\n",
    "\n",
    "```\n",
    "TensorBoard 1.12.2 at http://serpro-1540796:6006 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "Copie o link acima e abra em um navegador para poder ver graficamente os valores de perda (loss) e acurácia (accuracy) além do grafo da rede. Serão exibidas as várias execuções, cada uma com uma cor de linha diferente\n",
    "\n",
    "- **Acurácia (acc)** Mede o percentual de acerto das previsões tendo como base os dados de entrada e os valores esperados. Quando maior melhor\n",
    "- **Perda (loss)** Função de perda: esse é o objetivo que o modelo tentará minimizar. Quanto menor melhor\n",
    "\n",
    "**Visualização da Perda e Acurácia**\n",
    "\n",
    "![Visualização da Perda e Acurácia](tensorboard_01.png)\n",
    "\n",
    "**Visualização do Grafo da Rede**\n",
    "\n",
    "![Visualização do Grafo da Rede](tensorboard_02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.84\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Keras e LSTM - Passo 6\n",
    "#\n",
    "# Validando o modelo com os dados de teste (dados inéditos)\n",
    "#\n",
    "loss,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"Acurácia: %.2f\" % (acc))\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o resultado da validação foi diferente do treinamento, o que era esperado, visto que os dados de teste nunca foram submetidos à rede durante o treinamento. Entretanto esse resultado da validação é o que devemos considerar pois o modelo uma vez treinado será submetido à dados inéditos e não conhecidos no processo de treinamento. Nosso objetivo é ter um modelo genérico.\n",
    "\n",
    "Podemos tentar melhorar o modelo realizando ajuste nas variáveis, tais como MAX_FEATURES, BATCH_SIZE, EPOCHS, ou mesmo modificando a quantidade de neurônios na rede e testando novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo Keras e LSTM - Passo 7\n",
    "#\n",
    "# Executando o modelo treinado para um Tweet específico\n",
    "#\n",
    "print('Tweet original:')\n",
    "twt = 'We love awsome working with learning!'\n",
    "print(twt)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Tweet preparado:')\n",
    "# Preparando o texto\n",
    "twt = re.sub('[^a-zA-z0-9\\s]','',twt.lower())\n",
    "twt = twt.replace('rt',' ')\n",
    "print(twt)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Tweet vetorizado:')\n",
    "twt = tokenizer.texts_to_sequences([twt])\n",
    "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "print (twt)\n",
    "print('-------------------------------------------------------------')\n",
    "print('Previsão:')\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"Negativo\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"Positivo\")\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exemplo Keras e LSTM - Passo 8\n",
    "#\n",
    "# Validando o modelo para verificar a acurácia por cada classe (pos,neg)\n",
    "#\n",
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_test[x]):\n",
    "        if np.argmax(Y_test[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "    if np.argmax(Y_test[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "# Imprimindo os valores\n",
    "print(\"Acurácia da classe Positivo:\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"Acurácia da classe Negativo:\", neg_correct/neg_cnt*100, \"%\")\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto acima, a acurácia não está balanceada, o que era esperado, visto que temos mais amostras negativas do que positivas, como foi possível ver no Passo 2 deste tutorial:\n",
    "\n",
    "```\n",
    "Número de Positivos: 4472\n",
    "Número de Negativos: 16986\n",
    "```\n",
    "\n",
    "Uma forma de evitar isso é justamente balancear as amostras, o que poderá ser feito posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma de muitas possíveis implementações de modelos preditivos de análise de sentimento com Redes Neurais em Python.\n",
    "\n",
    "No exemplo acima utilizamos apenas duas classes ('Positivo', 'Negativo'), mas nada impede que você construa modelos de análise de emoção com mais de uma classe ('Raiva', 'Medo', 'Alegria', 'Tristeza')\n",
    "\n",
    "Veja abaixo outros exemplos:\n",
    "    \n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "- https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
