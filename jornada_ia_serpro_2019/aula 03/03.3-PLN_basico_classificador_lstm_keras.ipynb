{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN) - Básico\n",
    "\n",
    "SERPRO - SUPSD - DIVISÃO DE DESENVOLVIMENTO E SUSTENTAÇÃO DE PRODUTOS COGNITIVOS\n",
    "\n",
    "\n",
    "## Parte 3: Exemplo - Classificação de Sentenças com LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neurais Recorrentes (RNNs - Recurrent Neural Networks) são uma poderosa família de redes neurais que são amplamente utilizadas para tarefas de modelagem de sequência (por exemplo, previsão de preço de ações, modelagem de linguagem, séries temporais, etc...). A capacidade das RNNs de explorar dependências temporais de entidades em uma sequência as torna poderosas. Neste exercício, utilizaremos uma LSTM, que é uma RNN com células de memória para classificação de sentenças."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na classificação da sentenças, uma dada sentença deve ser classificada em uma classe (categoria). Usaremos um banco de dados de perguntas, onde cada pergunta é rotulada pelo assunto da pergunta. Por exemplo, a pergunta \"Quem foi Abraham Lincoln?\" será uma pergunta e seu rótulo será Pessoa. Para isso, usaremos um conjunto de dados de classificação de frases disponível em http://cogcomp.org/Data/QA/QC/; aqui você encontrará sentenças de treinamento e seus respectivos rótulos e 500 sentenças de teste.\n",
    "\n",
    "Categorias: http://cogcomp.org/Data/QA/QC/definition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos a seguinte estratégia:  \n",
    " - Extrair os dados\n",
    " - Pré-processamento dos dados\n",
    " - Construir a Rede Neural LSTM\n",
    " - Treinamento do modelo\n",
    " - Avaliar o modelo\n",
    " - Realizando Predições com dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout,TimeDistributed,Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando o Dataset\n",
    "\n",
    "Usaremos o __[dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/)__ que é composto de perguntas como entradas e seu respectivo tipo como saída. Por exemplo: quem era Abraham Lincon? E a saída ou rótulo seria Humano.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório\n",
    "dir_name = 'data/question-classif-data'\n",
    "\n",
    "# Dataset de treino\n",
    "#train_filename = 'train_1000.label'\n",
    "train_filename = 'train_5500.label'\n",
    "\n",
    "# Dataset de teste\n",
    "test_filename = 'TREC_10.label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos encontrados e verificados!\n"
     ]
    }
   ],
   "source": [
    "# Verifica se os arquivos existem\n",
    "filenames = [train_filename,test_filename]\n",
    "num_files = len(filenames)\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('Arquivos encontrados e verificados!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e visualizando os dados\n",
    "\n",
    "Abaixo nós carregamos o texto e visualizar alguns dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category Subcategory                                           Question\n",
      "0     DESC      manner  how did serfdom develop in and then leave russ...\n",
      "1     ENTY      cremat  what films featured the character popeye doyle...\n",
      "2     DESC      manner  how can i find a list of celebrities ' real na...\n",
      "3     ENTY      animal  what fowl grabs the spotlight after the chines...\n",
      "4     ABBR         exp                  what is the full form of .com ?\\n\n"
     ]
    }
   ],
   "source": [
    "# Função para leitura dos dados\n",
    "def read_data_into_pandas(filename):\n",
    "    col_names =  ['Category', 'Subcategory', 'Question']\n",
    "    data  = pd.DataFrame(columns = col_names)\n",
    "    with open(filename,'r',encoding='latin-1') as f: \n",
    "        count=1;\n",
    "        for row in f:\n",
    "            #Quebra a linha em duas no primeiro espaço (category:subcategory and question)\n",
    "            row_str = row.split(\" \",1)\n",
    "            #Separa a categoria da subcategoria\n",
    "            row_category = row_str[0].split(\":\")            \n",
    "            data.loc[len(data)] = [row_category[0],row_category[1],row_str[1].lower()]\n",
    "            count+=1\n",
    "    return data\n",
    "# Carrega os dados em DataFrames específicos para treinamento e teste\n",
    "data_train = read_data_into_pandas(os.path.join(dir_name,train_filename))\n",
    "data_test = read_data_into_pandas(os.path.join(dir_name,test_filename))\n",
    "\n",
    "# Concatena os dados de treinamento e teste. Será utilizado para construir um vocabulário único\n",
    "data = pd.concat([data_train,data_test])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento\n",
      " - Número questões na categoria HUMAN : 3669\n",
      " - Exemplos de questões na categoria HUMAN :\n",
      "5     what contemptible scoundrel stole the cork fro...\n",
      "6     what team did baseball 's st. louis browns bec...\n",
      "7                     what is the oldest profession ?\\n\n",
      "9     name the scar-faced bounty hunter of the old w...\n",
      "12                 who was the pride of the yankees ?\\n\n",
      "Name: Question, dtype: object\n",
      "\n",
      "Teste\n",
      " - Número questões na categoria HUMAN : 195\n",
      " - Exemplos de questões na categoria HUMAN :\n",
      "2                                   who was galileo ?\\n\n",
      "6     george bush purchased a small interest in whic...\n",
      "11                 what person 's head is on a dime ?\\n\n",
      "13    who was the first man to fly across the pacifi...\n",
      "17      who developed the vaccination against polio ?\\n\n",
      "Name: Question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Visualizando algumas informações sobre os dados\n",
    "# Exemplo Treinamento\n",
    "print('Treinamento')\n",
    "print(' - Número questões na categoria HUMAN : {}'.format(data_train[data_train['Category'] == 'HUM'].size))\n",
    "print(' - Exemplos de questões na categoria HUMAN :')\n",
    "print(data_train[data_train['Category'] == 'HUM']['Question'].head())\n",
    "print('')\n",
    "# Exemplo Teste\n",
    "print('Teste')\n",
    "print(' - Número questões na categoria HUMAN : {}'.format(data_test[ data_test['Category'] == 'HUM'].size))\n",
    "print(' - Exemplos de questões na categoria HUMAN :')\n",
    "print(data_test[data_test['Category'] == 'HUM']['Question'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento e Vetorização dos Dados para o Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos a classe Tokenizer do __[Keras](https://keras.io/)__ para montar nosso vocabulário. Ela permite vetorizar o texto do corpus, transformando o texto na sequência de inteiros (cada inteiro representa o índice do token em um dicionário) \n",
    "https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Vocabulário:  8761\n",
      "10 primeiras palavras do vocabulário: \n",
      "('the', 1)\n",
      "('what', 2)\n",
      "('is', 3)\n",
      "('of', 4)\n",
      "('in', 5)\n",
      "('a', 6)\n",
      "('how', 7)\n",
      "(\"'s\", 8)\n",
      "('was', 9)\n",
      "('who', 10)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Question'].values)\n",
    "print(\"Tamanho do Vocabulário: \", len(tokenizer.word_index))\n",
    "print(\"10 primeiras palavras do vocabulário: \")\n",
    "iterator = iter(tokenizer.word_index.items())\n",
    "for i in range(10):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de frase vetorizada: [7, 15, 3656, 2318, 5, 14, 440, 852, 1033]\n",
      "Exemplo de frase vetorizada: [2, 853, 639, 1, 134, 1319, 3657]\n",
      "Exemplo de frase vetorizada: [2, 3658, 3659, 1, 3660, 137, 1, 499, 46, 4, 1, 1683]\n",
      "Exemplo de frase vetorizada preenchida com zeros:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    7   15 3656 2318\n",
      "    5   14  440  852 1033]\n",
      "Shape do vetor de treinamento: (5452, 33)\n",
      "Tamanho da maior frase: 33\n",
      "Categorias:  Index(['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'], dtype='object')\n",
      "Exemplo vetor de label: [0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Vetorização dos dados de treinamento\n",
    "train_X = tokenizer.texts_to_sequences(data_train['Question'].values)\n",
    "print(\"Exemplo de frase vetorizada:\", train_X[0])\n",
    "print(\"Exemplo de frase vetorizada:\", train_X[1])\n",
    "print(\"Exemplo de frase vetorizada:\", train_X[3])\n",
    "\n",
    "# Padding dos dados de treinamento: preencher com zeros para que todas as frases fiquem do mesmo tamanho\n",
    "# Necessário para alimentar a rede neural  \n",
    "train_X = pad_sequences(train_X,value=0.)\n",
    "\n",
    "print(\"Exemplo de frase vetorizada preenchida com zeros: \", train_X[0])\n",
    "print(\"Shape do vetor de treinamento:\",train_X.shape)\n",
    "print(\"Tamanho da maior frase:\",train_X.shape[1])\n",
    "\n",
    "# Vetorização dos labels para os dados de treinamento\n",
    "train_labels = pd.get_dummies(data_train['Category'])\n",
    "print(\"Categorias: \",train_labels.columns)\n",
    "train_Y = pd.get_dummies(data_train['Category']).values\n",
    "print(\"Exemplo vetor de label:\",train_Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do vetor de teste (500, 33)\n"
     ]
    }
   ],
   "source": [
    "# Vetorização dos dados de teste\n",
    "test_X = tokenizer.texts_to_sequences(data_test['Question'].values)\n",
    "\n",
    "# Temos que usar o mesmo tamanho de frases pros dados de testes, por isso usamos o tamanho dos dados de treinamento\n",
    "test_X = pad_sequences(test_X,maxlen=train_X.shape[1],value=0.)\n",
    "print(\"Shape do vetor de teste\",test_X.shape)\n",
    "test_labels = pd.get_dummies(data_test['Category']).columns\n",
    "test_Y = pd.get_dummies(data_test['Category']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção da Rede Neural (LSTM)\n",
    "http://tflearn.org/layers/recurrent/#lstm\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma rede LSTM com a seguinte arquitetura:\n",
    "- O input será uma matriz com dimensão X (onde x é maior número de palavras de um documento)\n",
    "- Camada de entrada terá o número de neurônio igual ao número de features (words embeddings) \n",
    "- Uma camada LSTM com 128 unidades nas camadas ocultas nas células LSTM. Geralmente quanto maior melhor é desempenho. Os valores comuns são 128, 256, 512, etc.\n",
    "- Camada Dropout para evitar o overfitting\n",
    "- E uma camada totalmente conectada com 6 neurônios, referente as quantidade de categorias, função de ativação Softmax com as probabilidades para cada classe\n",
    "- Por fim uma camada de regressão com a função de perda e otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "#Size of the vocabulary, i.e. maximum integer index + 1\n",
    "max_features=len(tokenizer.word_index)+1\n",
    "\n",
    "#Construção da Rede\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=256, input_length=train_X.shape[1]))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 33, 256)           2243072   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 2,440,966\n",
      "Trainable params: 2,440,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar o modelo vamos utilizar 90% dos dados para treinamento e 10% para validação. \n",
    "\n",
    "Lembrado que os hiperparâmetros do treinamento podem ser ajustados de acordo com a necessidade e com o intuito de melhorar a acurácia.\n",
    "\n",
    "Vamos criar um callback para visualizar os dados do treinamento no tesorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4906 samples, validate on 546 samples\n",
      "Epoch 1/5\n",
      "4906/4906 [==============================] - 5s 1ms/step - loss: 1.3478 - acc: 0.4609 - val_loss: 0.8664 - val_acc: 0.6868\n",
      "Epoch 2/5\n",
      "4906/4906 [==============================] - 5s 947us/step - loss: 0.6183 - acc: 0.7945 - val_loss: 0.6462 - val_acc: 0.7967\n",
      "Epoch 3/5\n",
      "4906/4906 [==============================] - 5s 939us/step - loss: 0.3281 - acc: 0.8997 - val_loss: 0.5528 - val_acc: 0.8352\n",
      "Epoch 4/5\n",
      "4906/4906 [==============================] - 5s 919us/step - loss: 0.1973 - acc: 0.9452 - val_loss: 0.7781 - val_acc: 0.7894\n",
      "Epoch 5/5\n",
      "4906/4906 [==============================] - 5s 931us/step - loss: 0.1178 - acc: 0.9662 - val_loss: 0.5727 - val_acc: 0.8205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95f57e1f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "# Ativando o Tensorboard para poder monitorar com o comando abaixo em uym outro terminal:\n",
    "# > tensorboard --logdir=logskeras/\n",
    "tensorboard = TensorBoard(log_dir='logskeras/{}'.format(datetime.datetime.today().strftime('%Y%m%d_%H%M')))\n",
    "\n",
    "model.fit(x=train_X, y=train_Y, batch_size=32, epochs=5, verbose=1, validation_split=0.1,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando o modelo com os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.87\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(test_X, test_Y, verbose = 2)\n",
    "print(\"Acurácia: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando Predições com dados de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar algumas predições dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase -> how far is it from denver to aspen \n",
      "    Label category-> NUM\n",
      "    Label predict-> NUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> what county is modesto california in \n",
      "    Label category-> LOC\n",
      "    Label predict-> LOC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> who was galileo \n",
      "    Label category-> HUM\n",
      "    Label predict-> HUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> what is an atom \n",
      "    Label category-> DESC\n",
      "    Label predict-> DESC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> when did hawaii become a state \n",
      "    Label category-> NUM\n",
      "    Label predict-> NUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> how tall is the sears building \n",
      "    Label category-> NUM\n",
      "    Label predict-> NUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> george bush purchased a small interest in which baseball team \n",
      "    Label category-> HUM\n",
      "    Label predict-> ENTY\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> what is australia 's national flower \n",
      "    Label category-> ENTY\n",
      "    Label predict-> DESC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> why does the moon turn orange \n",
      "    Label category-> DESC\n",
      "    Label predict-> DESC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> what is autism \n",
      "    Label category-> DESC\n",
      "    Label predict-> DESC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Predição dos dados de testes\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "# Criação do dicionário reverso (key -> Índice ; value -> Token)\n",
    "reverse_dictionary = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "\n",
    "#print(reverse_dictionary)\n",
    "\n",
    "# Varre os dados de teste e imprime a predição\n",
    "for i in range(len(test_X[:10])):\n",
    "    frase='';\n",
    "    for j in test_X[i]:\n",
    "        if j != 0:\n",
    "            frase+=reverse_dictionary[j]+' '\n",
    "    print('Frase ->',frase)\n",
    "    print('    Label category->',data_test['Category'][i])\n",
    "    print('    Label predict->', test_labels[np.argmax(predictions[i])])\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando predições com um dado qualquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.002 0.002 0.003 0.002 0.982 0.01 ]]\n",
      "Labels -> Index(['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'], dtype='object')\n",
      "Label predict-> LOC\n"
     ]
    }
   ],
   "source": [
    "test = tokenizer.texts_to_sequences([\"where is Brazil?\"])\n",
    "test = pad_sequences(test,maxlen=train_X.shape[1],value=0.)\n",
    "\n",
    "predictions = model.predict(test)\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(predictions)\n",
    "    print('Labels ->',test_labels)\n",
    "    print('Label predict->', test_labels[np.argmax(predictions[0])])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empilhamento de camadas LSTM para classificação\n",
    "\n",
    "É possível fazer o empilhamento de camadas LSTM , para isso é necessário ativar o parâmetro __return_sequences__. Ele informa se a camada LSTM irá retornar apenas o última saída da sequência ou a sequência inteira (caso true). \n",
    "\n",
    "Para mais informações acessar o Sequencial Guide Mode(https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help\n",
    "?LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 33, 256)           2243072   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 33, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 33, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 2,704,134\n",
      "Trainable params: 2,704,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=256, input_length=train_X.shape[1]))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True)) \n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4906 samples, validate on 546 samples\n",
      "Epoch 1/5\n",
      "4906/4906 [==============================] - 10s 2ms/step - loss: 1.2757 - acc: 0.4670 - val_loss: 1.0806 - val_acc: 0.5769\n",
      "Epoch 2/5\n",
      "4906/4906 [==============================] - 9s 2ms/step - loss: 0.6695 - acc: 0.7693 - val_loss: 0.6968 - val_acc: 0.7784\n",
      "Epoch 3/5\n",
      "4906/4906 [==============================] - 9s 2ms/step - loss: 0.3917 - acc: 0.8732 - val_loss: 0.6750 - val_acc: 0.7894\n",
      "Epoch 4/5\n",
      "4906/4906 [==============================] - 9s 2ms/step - loss: 0.2405 - acc: 0.9266 - val_loss: 0.6690 - val_acc: 0.8059\n",
      "Epoch 5/5\n",
      "4906/4906 [==============================] - 9s 2ms/step - loss: 0.1583 - acc: 0.9515 - val_loss: 0.7159 - val_acc: 0.8278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95a06d8908>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, batch_size=32, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.85\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(test_X, test_Y, verbose = 2)\n",
    "print(\"Acurácia: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 1\n",
    "\n",
    "Modifique este jupyter para que o modelo obtenha um melhor desempenho.\n",
    "\n",
    "Primeiro crie uma função de pré-processamento dos dados para realizar as seguintes tarefas:\n",
    "\n",
    "- Remoção de Pontuação\n",
    "- Tokenização\n",
    "- Remoção de Stop words\n",
    "- Aplicação de Stemmer\n",
    "\n",
    "Dica: Essa função foi criada e utilizada no jupyter anterior.\n",
    "\n",
    "Reprocesse e veja os resultados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 2\n",
    "\n",
    "Faça modificando para tentar melhorar a acurácia do modelo:\n",
    "\n",
    "- Uso de Lemmatization ao invés de Stemming\n",
    "- Modificações dos hiperparâmetros da rede neural, tais como 'n_epoch' e 'batch_size'\n",
    "- Modifique a arquitetura da rede neural, tal como a quantidade de unidades da célula LSTM\n",
    "\n",
    "Qual configuração obteve melhores resultados?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "464px",
    "left": "56px",
    "top": "111.133px",
    "width": "181px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
