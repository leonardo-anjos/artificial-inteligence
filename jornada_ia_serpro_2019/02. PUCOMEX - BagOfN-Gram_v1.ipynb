{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estratégia de utilização de Bag Of N-gram com Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports e definição de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importações e criação de funções realizadas com sucesso\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Função para salvar uma lista em um binário\n",
    "def save_list_to_file(list_to_save,file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(list_to_save, f)\n",
    "# Função para carregar uma lista \n",
    "def load_list_from_file(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)     \n",
    "# Criando a classe de parada antecipada\n",
    "class EarlyStoppingCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self, val_epoch_thresh, val_acc_thresh):\n",
    "        self.val_epoch_thresh = val_epoch_thresh\n",
    "        self.val_acc_thresh = val_acc_thresh\n",
    "    def on_epoch_end(self, training_state):\n",
    "        print(\"Epoch \", training_state.epoch, \" with Accuracy \", training_state.acc_value)\n",
    "        if training_state.epoch >= self.val_epoch_thresh and training_state.acc_value >= self.val_acc_thresh:\n",
    "            raise StopIteration\n",
    "    def on_train_end(self, training_state):\n",
    "        print(\"Successfully left training! Final model accuracy:\", training_state.acc_value)\n",
    "print ('Importações e criação de funções realizadas com sucesso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurando o Pickle contendo o DataFrame trabalhado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classificacao_grupo</th>\n",
       "      <th>classificacao</th>\n",
       "      <th>grupo</th>\n",
       "      <th>descricao_demanda2</th>\n",
       "      <th>solucao</th>\n",
       "      <th>solucao_usuario2</th>\n",
       "      <th>solucao_script2</th>\n",
       "      <th>descricao_demanda</th>\n",
       "      <th>solucao_usuario</th>\n",
       "      <th>solucao_script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - UNACDF...</td>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO</td>\n",
       "      <td>UNACDFPUCOMEX</td>\n",
       "      <td>bom dia ter dois processo voar nao conseguir c...</td>\n",
       "      <td></td>\n",
       "      <td>prezar usuario carga estar american airlines t...</td>\n",
       "      <td></td>\n",
       "      <td>Bom Dia Tenho dois processos o qual ja voo , n...</td>\n",
       "      <td>Prezado Usuário\\n\\nAs cargas estão com a Améri...</td>\n",
       "      <td>#N/DISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCT - CONTROLE DE CARGA E TRÂNSITO - UNACDFPUC...</td>\n",
       "      <td>CCT - CONTROLE DE CARGA E TRÂNSITO</td>\n",
       "      <td>UNACDFPUCOMEX</td>\n",
       "      <td>bom dia enviar umar noto parir siscomex pesar ...</td>\n",
       "      <td>2016SOL/00396862</td>\n",
       "      <td>prezadoa usuarioa problema resolver favor test...</td>\n",
       "      <td>usuarioa conformar contato telefonico parir pr...</td>\n",
       "      <td>Bom dia, Enviei uma nota para o Siscomex e seu...</td>\n",
       "      <td>\\n\\nPrezado(a) usuário(a),\\nProblema resolvido...</td>\n",
       "      <td>Sr.(a) usuário(a), conforme contato telefônic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - CAGSBR...</td>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO</td>\n",
       "      <td>CAGSBRPUCOMEX</td>\n",
       "      <td></td>\n",
       "      <td>2016SOL/00399811</td>\n",
       "      <td>prezar usuariosegue informacao desenvolvimento...</td>\n",
       "      <td>usuarioaconforme informar atraves contato tele...</td>\n",
       "      <td></td>\n",
       "      <td>Prezado usuario,segue a informação do nosso de...</td>\n",
       "      <td>Sr.(a) usuário(a),conforme informado através d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - UNACDF...</td>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO</td>\n",
       "      <td>UNACDFPUCOMEX</td>\n",
       "      <td>fazer varios tentativo parir obter informacao ...</td>\n",
       "      <td>2017SOL/00003655</td>\n",
       "      <td>prezar usuario voce poder estar tirar suar duv...</td>\n",
       "      <td>usuarioasua solicitacao ser iosamentecentral s...</td>\n",
       "      <td>Srs., Fizemos várias tentativas para obter inf...</td>\n",
       "      <td>Prezado Usuário,\\n\\nVocê pode estar tirando as...</td>\n",
       "      <td>\\nSr(a). usuário(a),Sua solicitação foi atend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCT - CONTROLE DE CARGA E TRÂNSITO - UNACDFPUC...</td>\n",
       "      <td>CCT - CONTROLE DE CARGA E TRÂNSITO</td>\n",
       "      <td>UNACDFPUCOMEX</td>\n",
       "      <td>nro solicitacao comer solicitar nao conseguir ...</td>\n",
       "      <td>2017SOL/0000002832</td>\n",
       "      <td>caro usuario due encontrar averbar</td>\n",
       "      <td>usuarioae necessario parir analise situacao ac...</td>\n",
       "      <td>Nro da Solicitação: 2017SS/0001012491: Como so...</td>\n",
       "      <td>Caro usuário,\\n\\nA DUE 17BR0000079935 já se en...</td>\n",
       "      <td>Sr(a). Usuário(a),É necessário, para análise d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 classificacao_grupo  \\\n",
       "0  DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - UNACDF...   \n",
       "1  CCT - CONTROLE DE CARGA E TRÂNSITO - UNACDFPUC...   \n",
       "2  DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - CAGSBR...   \n",
       "3  DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - UNACDF...   \n",
       "4  CCT - CONTROLE DE CARGA E TRÂNSITO - UNACDFPUC...   \n",
       "\n",
       "                           classificacao          grupo  \\\n",
       "0  DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO  UNACDFPUCOMEX   \n",
       "1     CCT - CONTROLE DE CARGA E TRÂNSITO  UNACDFPUCOMEX   \n",
       "2  DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO  CAGSBRPUCOMEX   \n",
       "3  DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO  UNACDFPUCOMEX   \n",
       "4     CCT - CONTROLE DE CARGA E TRÂNSITO  UNACDFPUCOMEX   \n",
       "\n",
       "                                  descricao_demanda2             solucao  \\\n",
       "0  bom dia ter dois processo voar nao conseguir c...                       \n",
       "1  bom dia enviar umar noto parir siscomex pesar ...    2016SOL/00396862   \n",
       "2                                                       2016SOL/00399811   \n",
       "3  fazer varios tentativo parir obter informacao ...    2017SOL/00003655   \n",
       "4  nro solicitacao comer solicitar nao conseguir ...  2017SOL/0000002832   \n",
       "\n",
       "                                    solucao_usuario2  \\\n",
       "0  prezar usuario carga estar american airlines t...   \n",
       "1  prezadoa usuarioa problema resolver favor test...   \n",
       "2  prezar usuariosegue informacao desenvolvimento...   \n",
       "3  prezar usuario voce poder estar tirar suar duv...   \n",
       "4                 caro usuario due encontrar averbar   \n",
       "\n",
       "                                     solucao_script2  \\\n",
       "0                                                      \n",
       "1  usuarioa conformar contato telefonico parir pr...   \n",
       "2  usuarioaconforme informar atraves contato tele...   \n",
       "3  usuarioasua solicitacao ser iosamentecentral s...   \n",
       "4  usuarioae necessario parir analise situacao ac...   \n",
       "\n",
       "                                   descricao_demanda  \\\n",
       "0  Bom Dia Tenho dois processos o qual ja voo , n...   \n",
       "1  Bom dia, Enviei uma nota para o Siscomex e seu...   \n",
       "2                                                      \n",
       "3  Srs., Fizemos várias tentativas para obter inf...   \n",
       "4  Nro da Solicitação: 2017SS/0001012491: Como so...   \n",
       "\n",
       "                                     solucao_usuario  \\\n",
       "0  Prezado Usuário\\n\\nAs cargas estão com a Améri...   \n",
       "1  \\n\\nPrezado(a) usuário(a),\\nProblema resolvido...   \n",
       "2  Prezado usuario,segue a informação do nosso de...   \n",
       "3  Prezado Usuário,\\n\\nVocê pode estar tirando as...   \n",
       "4  Caro usuário,\\n\\nA DUE 17BR0000079935 já se en...   \n",
       "\n",
       "                                      solucao_script  \n",
       "0                                            #N/DISP  \n",
       "1   Sr.(a) usuário(a), conforme contato telefônic...  \n",
       "2  Sr.(a) usuário(a),conforme informado através d...  \n",
       "3   \\nSr(a). usuário(a),Sua solicitação foi atend...  \n",
       "4  Sr(a). Usuário(a),É necessário, para análise d...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restaurando o DataFrame gerado no Passo 1 - Preparação\n",
    "FILENAME = 'dataframe.pickle'\n",
    "df = pd.read_pickle(FILENAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classificacao_grupo</th>\n",
       "      <th>classificacao</th>\n",
       "      <th>grupo</th>\n",
       "      <th>descricao_demanda2</th>\n",
       "      <th>solucao</th>\n",
       "      <th>solucao_usuario2</th>\n",
       "      <th>solucao_script2</th>\n",
       "      <th>descricao_demanda</th>\n",
       "      <th>solucao_usuario</th>\n",
       "      <th>solucao_script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "      <td>18160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>103</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>16411</td>\n",
       "      <td>174</td>\n",
       "      <td>8009</td>\n",
       "      <td>124</td>\n",
       "      <td>17059</td>\n",
       "      <td>9208</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>PORTAL ÚNICO DE COMÉRCIO EXTERIOR - ATGSMG</td>\n",
       "      <td>DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO</td>\n",
       "      <td>ATGSMG</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>#N/DISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3559</td>\n",
       "      <td>6902</td>\n",
       "      <td>4654</td>\n",
       "      <td>490</td>\n",
       "      <td>6564</td>\n",
       "      <td>2240</td>\n",
       "      <td>6739</td>\n",
       "      <td>473</td>\n",
       "      <td>2118</td>\n",
       "      <td>6737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classificacao_grupo  \\\n",
       "count                                        18160   \n",
       "unique                                         103   \n",
       "top     PORTAL ÚNICO DE COMÉRCIO EXTERIOR - ATGSMG   \n",
       "freq                                          3559   \n",
       "\n",
       "                                classificacao   grupo descricao_demanda2  \\\n",
       "count                                   18160   18160              18160   \n",
       "unique                                     21      33              16411   \n",
       "top     DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO  ATGSMG                      \n",
       "freq                                     6902    4654                490   \n",
       "\n",
       "       solucao solucao_usuario2 solucao_script2 descricao_demanda  \\\n",
       "count    18160            18160           18160             18160   \n",
       "unique     174             8009             124             17059   \n",
       "top                                                                 \n",
       "freq      6564             2240            6739               473   \n",
       "\n",
       "       solucao_usuario solucao_script  \n",
       "count            18160          18160  \n",
       "unique            9208            149  \n",
       "top                           #N/DISP  \n",
       "freq              2118           6737  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descrevendo os dados do DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do Bag Of N-gram para X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criação de X em 1.25 segundos\n",
      "Formato de X_vet:  (18160, 20000)\n"
     ]
    }
   ],
   "source": [
    "# Se essa linha der erro, reinicie o Jupyter com o comando abaixo:\n",
    "#\n",
    "#  jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "#\n",
    "start_time = time.time()\n",
    "MAX_FEATURES=20000\n",
    "# Cria um ngram vectorizer\n",
    "# Vamos criar o BOW dos 2 documentos\n",
    "# Usamos n_gram = 2\n",
    "#vectorizer = CountVectorizer(ngram_range=(2,2), max_features=MAX_FEATURES)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),stop_words=stopwords.words('portuguese'),strip_accents='unicode',max_features=MAX_FEATURES)\n",
    "# Contabilizamos as combinações de caracteres nas palavras\n",
    "X_vet = vectorizer.fit_transform(df['descricao_demanda2'])\n",
    "print('Criação de X em %.2f segundos' % (time.time() - start_time))\n",
    "print('Formato de X_vet: ',X_vet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "dues respectivo                Score: 0.3161551209918496\n",
      "sair estocar                   Score: 0.3045893329234719\n",
      "datar horario                  Score: 0.29999936792939363\n",
      "dois processo                  Score: 0.2959507318751046\n",
      "processed listener             Score: 0.2789149279685556\n",
      "efetuar manifestacao           Score: 0.27408945926724537\n",
      "manifestacao ser               Score: 0.2534386524829566\n",
      "dar ser                        Score: 0.2398729366579298\n",
      "ter dois                       Score: 0.23913368930058923\n",
      "concluir manifestacao          Score: 0.2356586440070707\n",
      "nao concluir                   Score: 0.21757488972029793\n",
      "dia ter                        Score: 0.21387522800613093\n",
      "conseguir concluir             Score: 0.21085511011858324\n",
      "manifestacao dar               Score: 0.17406928228528926\n",
      "ser informar                   Score: 0.17384964142768913\n",
      "listener attachment            Score: 0.1487872207995438\n",
      "nao conseguir                  Score: 0.11273795474123287\n",
      "bom dia                        Score: 0.09676537999372144\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo os valores para cada token no primeiro documento\n",
    "for doc in X_vet:\n",
    "    print('------------')\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(doc.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    for item in sorted_scores:\n",
    "        if item[1] > 0.0:\n",
    "            print (\"{0:30} Score: {1}\".format(item[0], item[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do One Hot Encoding para X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 20000\n",
      "Imprimindo os 100 primeiros:\n",
      "['abaixar agente', 'abaixar aguardar', 'abaixar algum', 'abaixar anexar', 'abaixar aparecer', 'abaixar apos', 'abaixar apresentar', 'abaixar arquivar', 'abaixar atenciosamente', 'abaixar att', 'abaixar campar', 'abaixar carga', 'abaixar comer', 'abaixar conformar', 'abaixar constar', 'abaixar consulto', 'abaixar cpf', 'abaixar dar', 'abaixar descricao', 'abaixar desde', 'abaixar destacar', 'abaixar due', 'abaixar dues', 'abaixar duexer', 'abaixar entrar', 'abaixar enviar', 'abaixar errar', 'abaixar estar', 'abaixar etapalendo', 'abaixar exemplo', 'abaixar exportador', 'abaixar favor', 'abaixar fazer', 'abaixar ficar', 'abaixar fiscal', 'abaixar gentileza', 'abaixar grato', 'abaixar image', 'abaixar imagem', 'abaixar informacao', 'abaixar informar', 'abaixar item', 'abaixar lpco', 'abaixar mencionar', 'abaixar mensagem', 'abaixar momento', 'abaixar msg', 'abaixar nao', 'abaixar notar', 'abaixar noto', 'abaixar numero', 'abaixar obrigar', 'abaixar obs', 'abaixar ocorrer', 'abaixar onde', 'abaixar parir', 'abaixar pedir', 'abaixar pelar', 'abaixar poder', 'abaixar pois', 'abaixar porem', 'abaixar portal', 'abaixar precisar', 'abaixar print', 'abaixar problema', 'abaixar quantidade', 'abaixar questao', 'abaixar quote', 'abaixar relacionar', 'abaixar screenshot', 'abaixar sds', 'abaixar seguir', 'abaixar ser', 'abaixar sistema', 'abaixar situacao', 'abaixar solicitacao', 'abaixar solicitar', 'abaixar solucao', 'abaixar tela', 'abaixar tentar', 'abaixar ter', 'abaixar terminal', 'abaixar todo', 'abaixar umar', 'abaixar unidade', 'abaixar vez', 'abar carga', 'abar consulto', 'abar dar', 'abar documento', 'abar due', 'abar historico', 'abar incluir', 'abar item', 'aberto anteriormente', 'aberto ate', 'aberto chamar', 'aberto dia', 'aberto parir', 'aberto pedir']\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Formato de X (18160, 20000):\n",
      "Imprimindo One-Hot Encoding dos 5 primeiros:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Criação de Y em 1.66 segundos\n"
     ]
    }
   ],
   "source": [
    "# Preparando dados de X\n",
    "start_time = time.time()\n",
    "X_vocabulary = vectorizer.get_feature_names()\n",
    "NUM_FEATURES = len(X_vocabulary)\n",
    "print('Tamanho do vocabulário: {}'.format(len(X_vocabulary)))\n",
    "print('Imprimindo os 100 primeiros:')\n",
    "print(X_vocabulary[:100])\n",
    "print('--------------------------------------------------------------------------------------------------------------')\n",
    "X_values = X_vet.toarray().astype(float)\n",
    "print('Formato de X {}:'.format(X_values.shape))\n",
    "print('Imprimindo One-Hot Encoding dos 5 primeiros:')\n",
    "print(X_values[:5])\n",
    "print('Criação de Y em %.2f segundos' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do One Hot Encoding para Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de targets: 18160\n",
      "Imprimindo os targets:\n",
      "Targets:  Index(['AMBIENTE', 'ANÁLISE URC', 'APLICAÇÃO',\n",
      "       'CA - CONFERÊNCIA ADUANEIRA EXPORTAÇÃO',\n",
      "       'CADASTRO DE INTERVENIENTES - RFB',\n",
      "       'CCT - CONTROLE DE CARGA E TRÂNSITO', 'DA - DESPACHO ADUANEIRO',\n",
      "       'DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO',\n",
      "       'DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - MDIC',\n",
      "       'DUEX - DECLARAÇÃO ÚNICA DE EXPORTAÇÃO - RFB', 'DÚVIDA',\n",
      "       'DÚVIDA OU INFORMAÇÕES', 'GR – GERENCIAMENTO DE RISCO ALEATÓRIOS',\n",
      "       'OPERADOR ECONÔMICO AUTORIZADO (OEA) - RFB',\n",
      "       'PORTAL ÚNICO DE COMÉRCIO EXTERIOR',\n",
      "       'PORTAL ÚNICO SISCOMEX ANUENTES INTERNET (PUCOMEX)',\n",
      "       'PORTAL ÚNICO SISCOMEX RFB INTRANET(PUCOMEX)',\n",
      "       'PROSPECÇÃO - CLASSIF - CLASSIFICAÇÃO FISCAL DE MERCADORIAS RFB',\n",
      "       'SITUAÇÃO NÃO PREVISTA', 'TA-LPCO - TRATAMENTO ADMIN',\n",
      "       'VICOMEX - VISÃO INTEGRADA DO COMÉRCIO EXTERIOR - MDIC'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Formato de Y (18160, 21):\n",
      "Imprimindo One-Hot Encoding do primeiro Y:\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Preparando dados de Y\n",
    "#Vetorização dos labels (Target)\n",
    "Y_labels = pd.get_dummies(df['classificacao'])\n",
    "print('Quantidade de targets: {}'.format(len(Y_labels)))\n",
    "print('Imprimindo os targets:')\n",
    "print('Targets: ',Y_labels.columns)\n",
    "print('--------------------------------------------------------------------------------------------------------------')\n",
    "Y_values = pd.get_dummies(df['classificacao']).values\n",
    "print('Formato de Y {}:'.format(Y_values.shape))\n",
    "print('Imprimindo One-Hot Encoding do primeiro Y:')\n",
    "print(Y_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação dos dados de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train data: X_train  (12167, 20000) , y_train (12167, 21)\n",
      " Test data: X_test  (5993, 20000) , y_test  (5993, 21)\n",
      "Exemplo de X_train:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] ...\n",
      "Exemplo de y_train:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Separando dados de treinamento e testes\n",
    "if 'X_values' in locals():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_values, Y_values, test_size=0.33, random_state=42)\n",
    "    print(' Train data: X_train ',X_train.shape,', y_train',y_train.shape)\n",
    "    print(' Test data: X_test ',X_test.shape,', y_test ',y_test.shape)\n",
    "    print('Exemplo de X_train: ',X_train[0][:100],'...')\n",
    "    print('Exemplo de y_train: ',y_train[0])\n",
    "    del X_values, Y_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de uma Rede Neural DNN para ser treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada no formato (indice:valor)\n",
      "286 : 0.3023616380474234\n",
      "1820 : 0.31948917354902495\n",
      "3141 : 0.2914867102735966\n",
      "7138 : 0.23506512297941448\n",
      "8132 : 0.29243335715329116\n",
      "9523 : 0.3017668707214662\n",
      "11911 : 0.29717893474093193\n",
      "12139 : 0.2329235157226456\n",
      "13337 : 0.3025609988309158\n",
      "14249 : 0.2912306952134304\n",
      "16703 : 0.29526075355654563\n",
      "18124 : 0.2891321652888649\n",
      "Resultado esperado:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo primeiro dado de treino\n",
    "print('Entrada no formato (indice:valor)')\n",
    "i = 0\n",
    "for x in X_train[0]:\n",
    "    if x != 0.0:\n",
    "        print (i,\":\",x)\n",
    "    i += 1\n",
    "print('Resultado esperado: ',y_train[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros setados\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'pucomex'\n",
    "REDE = 'net_3'\n",
    "BATCH = 32\n",
    "EPOCHS = 100\n",
    "MIN_EPOCHS = 5\n",
    "MIN_ACCURACY = 0.99\n",
    "print ('Parâmetros setados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criada a net_3\n"
     ]
    }
   ],
   "source": [
    "# Reset do grafo\n",
    "tf.reset_default_graph()\n",
    "# Cria a rede neural\n",
    "net = None\n",
    "if REDE == 'net1':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_1':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_2':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_3':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_3_1':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.dropout(net, 0.4)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.dropout(net, 0.4)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_3_2':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.dropout(net, 0.3)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.dropout(net, 0.3)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_4':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_5':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_6':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 128)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 64)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.dropout(net, 0.5)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net_cnn_1':\n",
    "    net = input_data(shape=[None, X_train.shape[1]], name='input')\n",
    "    net = tflearn.embedding(net, input_dim=NUM_FEATURES, output_dim=128)\n",
    "    branch1 = conv_1d(net, 32, 3, padding='same', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(net, 32, 5, padding='same', activation='relu', regularizer=\"L2\")\n",
    "    net = merge([branch1, branch2], mode='concat', axis=1)\n",
    "    net = tf.expand_dims(net, 2)\n",
    "    net = global_max_pool(net)\n",
    "    net = dropout(net, 0.5)\n",
    "    net = fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                     loss='categorical_crossentropy', name='target')\n",
    "elif REDE == 'net_8':\n",
    "    net = input_data(shape=[None, X_train.shape[1]], name='input')\n",
    "    net = tflearn.embedding(net, input_dim=NUM_FEATURES, output_dim=128)\n",
    "    branch1 = conv_1d(net, 128, 3, padding='same', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(net, 128, 5, padding='same', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(net, 128, 7, padding='same', activation='relu', regularizer=\"L2\")\n",
    "    net = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    net = tf.expand_dims(net, 2)\n",
    "    net = global_max_pool(net)\n",
    "    net = dropout(net, 0.5)\n",
    "    net = fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                     loss='categorical_crossentropy', name='target')\n",
    "elif REDE == 'net2':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.fully_connected(net, 16)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net3':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net4':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, 50)\n",
    "    net = tflearn.fully_connected(net, 25)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "elif REDE == 'net5':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.embedding(net, input_dim=len(X_train[0]), output_dim=len(y_train[0]))\n",
    "    net = tflearn.lstm(net, 8)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam',loss='binary_crossentropy')\n",
    "elif REDE == 'net6':\n",
    "    net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "    net = tflearn.embedding(net, input_dim=len(X_train[0]), output_dim=len(y_train[0]))\n",
    "    net = tflearn.lstm(net, 8)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam',loss='binary_crossentropy')\n",
    "# Define o modelo e configura o tensorboard\n",
    "print ('Criada a',REDE)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=3, tensorboard_dir='/home/03662232677/c4/PUCOMEX/tflearn_logs_'+str(DATASET)+'/'+str(DATASET)+'_n'+str(NUM_FEATURES)+'_st'+str(EPOCHS)+'_'+str(REDE)+'_b'+str(BATCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da Rede Neural com os dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 38099  | total loss: \u001b[1m\u001b[32m0.22486\u001b[0m\u001b[0m | time: 10.443s\n",
      "| Adam | epoch: 100 | loss: 0.22486 - acc: 0.9228 -- iter: 12160/12167\n",
      "Training Step: 38100  | total loss: \u001b[1m\u001b[32m0.26372\u001b[0m\u001b[0m | time: 10.469s\n",
      "| Adam | epoch: 100 | loss: 0.26372 - acc: 0.9086 -- iter: 12167/12167\n",
      "--\n",
      "Epoch  100  with Accuracy  0.9086063504219055\n",
      "Successfully left training! Final model accuracy: 0.9086063504219055\n",
      "Finalização do treinamento em 1081.06 segundos\n"
     ]
    }
   ],
   "source": [
    "# Iniciando o treinamento da rede neural. Acompanhe o treinamento pelo TensorBoard utilizando o comando abaixo via terminal:\n",
    "#\n",
    "# tensorboard --logdir tflearn_logs_...\n",
    "# \n",
    "start_time = time.time()\n",
    "early_stopping_cb = EarlyStoppingCallback(val_epoch_thresh=MIN_EPOCHS,val_acc_thresh=MIN_ACCURACY)\n",
    "# Treinamento\n",
    "try:\n",
    "    model.fit(X_train, y_train, n_epoch = EPOCHS, batch_size = BATCH, show_metric = True, callbacks=early_stopping_cb)\n",
    "except StopIteration:\n",
    "    print(\"Caught callback exception. Returning control to user program.\")\n",
    "print('Finalização do treinamento em %.2f segundos' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia para os dados de teste: [0.5344568668413536]\n",
      "Acurácia para os dados de treinamento: [0.9413166762652428]\n",
      "Acurácia para todos os dados: 0.7378867715532982\n"
     ]
    }
   ],
   "source": [
    "# Testando o modelo com a base de testes\n",
    "total = 0\n",
    "acertos = 0\n",
    "score1 = model.evaluate(X_test, y_test)\n",
    "print('Acurácia para os dados de teste: {}'.format(score1))\n",
    "score2 = model.evaluate(X_train, y_train)\n",
    "print('Acurácia para os dados de treinamento: {}'.format(score2))\n",
    "print('Acurácia para todos os dados: {}'.format(stat.mean(score1+score2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando um teste de um documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "#print(X_test[N])\n",
    "print(np.rint(model.predict([X_test[N]]))[0])\n",
    "print(y_test[N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando um modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/03662232677/c4/PUCOMEX/model_pucomex_n10000_st100_net_3_b32/model_pucomex_n10000_st100_net_3_b32.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Save model and pickle files\n",
    "model_name = 'model_'+str(DATASET)+'_n'+str(MAX_FEATURES)+'_st'+str(EPOCHS)+'_'+str(REDE)+'_b'+str(BATCH)\n",
    "directory = '/home/03662232677/c4/PUCOMEX/'+model_name\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory) \n",
    "model.save(directory+'/'+model_name+'.tflearn')\n",
    "save_list_to_file(X_vocabulary,directory+'/X_vocabulary.pickle')\n",
    "#save_list_to_file(X_values,directory+'/X_values.pickle')\n",
    "#save_list_to_file(Y_labels,directory+'/Y_labels.pickle')\n",
    "#save_list_to_file(Y_values,directory+'/Y_values.pickle')\n",
    "save_list_to_file(X_train,directory+'/X_train.pickle')\n",
    "save_list_to_file(X_test,directory+'/X_test.pickle')\n",
    "save_list_to_file(y_train,directory+'/y_train.pickle')\n",
    "save_list_to_file(y_test,directory+'/y_test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
