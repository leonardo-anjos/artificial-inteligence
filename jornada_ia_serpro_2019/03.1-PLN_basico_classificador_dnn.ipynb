{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN) - Básico \n",
    "\n",
    "SERPRO - SUPSS - DIVISÃO DE DESENVOLVIMENTO E SUSTENTAÇÃO DE PRODUTOS COGNITIVOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Exemplo - Classificador de Texto com Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook iremos apresentar um exemplo de classificador de texto com Deep Learning utilizando os vários conceitos apresentados no curso até aqui.\n",
    "\n",
    "Os dados serão sentenças categorizadas em tempo, desculpa, agradecimento, despedida e idade.\n",
    "\n",
    "Etapas:\n",
    "1. Leitura: Ler sentenças de um arquivo JSON para realizar o treinamento.\n",
    "2. Pré processamento dos dados: Remover pontuação e tokenizando as sentenças\n",
    "3. Criação do Vocabulário: Remover as palavras duplicadas e aplicar o Stem\n",
    "4. Preparar os dados para o Treinamento: Gerar as Bag of Words e vetorização das categorias.\n",
    "5. Montar a estrutura da rede neural\n",
    "6. Treinamento\n",
    "7. Realizar alguns testes\n",
    "8. Visualização com o TensorBoard \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar tflearn que irá ser utilizado para criação da rede neural \n",
    "#!pip install tensorflow\n",
    "#!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import nltk\n",
    "import json\n",
    "import tflearn\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "portuguese_stops = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura para armazenar pontuações\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover pontuações das sentenças\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o stemmer\n",
    "stemmer = SnowballStemmer(language='portuguese')\n",
    "data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos utilizar um pequeno conjunto de sentenças categorizadas para treinar modelo.\n",
    "\n",
    "As categorias são: tempo, desculpa, agradecimento, despedida e idade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tempo': ['Pode informar as horas?', 'Quanto tempo passou desde que começamos?', 'isso ocorreu algum tempo atrás', 'eu falei com você semana passada', 'eu vi você ontem'], 'desculpa': ['Me desculpe', 'Ele pediu desculpas?', 'Eu não devia ter sido rude'], 'agradecimento': ['Olá, tudo bem? Muito obrigado!', 'Agradeço pela sua ajuda?', 'Muito obrigado'], 'despedida': ['Foi um prazer conhecer você', 'Adeus.', 'Vejo você em breve', 'Preciso ir agora.', 'Até logo'], 'idade': ['qual sua idade?', 'Quantos anos você tem?', 'Eu sou alguns anos mais velho que ela', 'você parece mais velho!', 'Eu tenho 30 anos. Eu sou o mais velho']}\n"
     ]
    }
   ],
   "source": [
    "# Leitura do arquivo JSON com os dados de treino\n",
    "with open('data/data.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tempo', 'desculpa', 'agradecimento', 'despedida', 'idade']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gera uma lista com todas as categorias\n",
    "categories = list(data.keys())\n",
    "words = []\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar o pré processamento do texto removendo as pontuações e tokenizando as sentenças. Além disso, precisamos criar uma lista que irá conter os documentos com seu texto tokenizado e suas respectivas categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de Sentença Categorizada: (['Pode', 'informar', 'as', 'horas'], 'tempo')\n",
      "Número de frases no corpus: 21\n",
      "Corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['Pode', 'informar', 'as', 'horas'], 'tempo'),\n",
       " (['Quanto', 'tempo', 'passou', 'desde', 'que', 'começamos'], 'tempo'),\n",
       " (['isso', 'ocorreu', 'algum', 'tempo', 'atrás'], 'tempo'),\n",
       " (['eu', 'falei', 'com', 'você', 'semana', 'passada'], 'tempo'),\n",
       " (['eu', 'vi', 'você', 'ontem'], 'tempo'),\n",
       " (['Me', 'desculpe'], 'desculpa'),\n",
       " (['Ele', 'pediu', 'desculpas'], 'desculpa'),\n",
       " (['Eu', 'não', 'devia', 'ter', 'sido', 'rude'], 'desculpa'),\n",
       " (['Olá', 'tudo', 'bem', 'Muito', 'obrigado'], 'agradecimento'),\n",
       " (['Agradeço', 'pela', 'sua', 'ajuda'], 'agradecimento'),\n",
       " (['Muito', 'obrigado'], 'agradecimento'),\n",
       " (['Foi', 'um', 'prazer', 'conhecer', 'você'], 'despedida'),\n",
       " (['Adeus'], 'despedida'),\n",
       " (['Vejo', 'você', 'em', 'breve'], 'despedida'),\n",
       " (['Preciso', 'ir', 'agora'], 'despedida'),\n",
       " (['Até', 'logo'], 'despedida'),\n",
       " (['qual', 'sua', 'idade'], 'idade'),\n",
       " (['Quantos', 'anos', 'você', 'tem'], 'idade'),\n",
       " (['Eu', 'sou', 'alguns', 'anos', 'mais', 'velho', 'que', 'ela'], 'idade'),\n",
       " (['você', 'parece', 'mais', 'velho'], 'idade'),\n",
       " (['Eu', 'tenho', '30', 'anos', 'Eu', 'sou', 'o', 'mais', 'velho'], 'idade')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uma lista de tuplas com as palavras das sentenças e o nome da categoria\n",
    "docs = []\n",
    "\n",
    "for each_category in data.keys():\n",
    "    for each_sentence in data[each_category]:\n",
    "        # Remove a pontuação\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "        #print(\"Sentença:\",each_sentence)\n",
    "        # Extrai as palavras de cada sentença e armazena na lista\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "        #print(\"Palavras tokenizadas:\", w)\n",
    "        #print('')\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n",
    "\n",
    "print(\"Exemplo de Sentença Categorizada:\",docs[0])\n",
    "print(\"Número de frases no corpus:\",len(docs))\n",
    "print(\"Corpus\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos remover as palavras duplicadas e aplicar o Stem. Assim teremos o __vocabulário__ relativo ao corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30', 'adeus', 'agor', 'agradec', 'ajud', 'algum', 'alguns', 'anos', 'as', 'atrás', 'até', 'bem', 'brev', 'com', 'comec', 'conhec', 'desculp', 'desd', 'dev', 'ela', 'ele', 'em', 'eu', 'fal', 'foi', 'hor', 'idad', 'inform', 'ir', 'isso', 'log', 'mais', 'me', 'muit', 'nã', 'o', 'obrig', 'ocorr', 'olá', 'ontem', 'parec', 'pass', 'ped', 'pel', 'pod', 'praz', 'precis', 'qual', 'quant', 'que', 'rud', 'seman', 'sid', 'sou', 'sua', 'tem', 'temp', 'tenh', 'ter', 'tud', 'um', 'vej', 'velh', 'vi', 'voc']\n",
      "Tamanho Vocabulário: 65\n"
     ]
    }
   ],
   "source": [
    "# Stem de cada palavra, converte para minúsculo e remove duplicidades \n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "print(words)\n",
    "print('Tamanho Vocabulário:',len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados para o Treinamento \n",
    "\n",
    "Vamos gerar os dados que serão utilizados no treinamento. Eles serão formados pelo Bag of Words (Matriz Binária) para cada frase e o vetor com sua respectiva categorias/labels, onde índice com valor 1 representará a categoria definida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: ['tempo', 'desculpa', 'agradecimento', 'despedida', 'idade']\n",
      "output_empty: [0, 0, 0, 0, 0]\n",
      "Exemplo de dados de treino: \n",
      "Frase:  (['Pode', 'informar', 'as', 'horas'], 'tempo')\n",
      " - BoW:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " - Categoria: vetor - [1, 0, 0, 0, 0] , label - tempo \n"
     ]
    }
   ],
   "source": [
    "# Cria as listas para os dados de treino\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "# Cria um array para o output com tamanho igual ao número de categorias\n",
    "output_empty = [0] * len(categories)\n",
    "print('categories:',categories)\n",
    "print('output_empty:',output_empty)\n",
    "\n",
    "for doc in docs:\n",
    "    # Inicializa o bag of words para cada documento da lista\n",
    "    bow = []\n",
    "    # Lista de palavras tokenizadas\n",
    "    token_words = doc[0]\n",
    "    # Stem de cada palavra usando list comprehension\n",
    "    token_words = [stemmer.stem(word.lower()) for word in token_words]\n",
    "    # Cria um array com o bag of words\n",
    "    for w in words:\n",
    "        bow.append(1) if w in token_words else bow.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(doc[1])] = 1\n",
    "\n",
    "    # Nosso conjunto de treinamento conterá um modelo bag of words e a linha de saída que informa a qual sentença pertence.\n",
    "    training.append([bow, output_row])\n",
    "    \n",
    "print(\"Exemplo de dados de treino: \")# categoria \n",
    "print(\"Frase: \",docs[0])\n",
    "print(\" - BoW: \",training[0][0])\n",
    "print(\" - Categoria: vetor - {} , label - {} \".format(training[0][1], categories[np.argmax(training[0][1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle das nossas features e transforma em np.array enquanto o TensorFlow recebe uma matriz numérica\n",
    "random.shuffle(training)\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo: train_x (BOW) ->  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Exemplo: train_y (Labes) ->  [0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# train_x contém o bag of word e train_y contém os labels/categorias\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "print(\"Exemplo: train_x (BOW) -> \",train_x[0])\n",
    "print(\"Exemplo: train_y (Labes) -> \",train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando a da Rede Neural Profunda\n",
    "\n",
    "Com os dados preparados vamos criar a rede neural:\n",
    "\n",
    "- Os dados de entrada terá um shape igual ao número de features (palavras do vocabulário)\n",
    "- Depois cria-se duas camadas totalmente conectadas com número de neurônios igual a 8\n",
    "- A camada de saída terá uma quantidade de neurônios igual ao número de labels/categorias (5)\n",
    "- Será utilizada a função de ativação softmax para gerar as probabilidades para cada categoria\n",
    "- E por fim uma camada de regressão espeficando o otmizador que irá minimar a função de perda, a função de perda e a taxa de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tflearn.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "#Features (words) 65\n",
      "Tamanho da camada de saída (categorias): 5\n"
     ]
    }
   ],
   "source": [
    "# Reset do grafo tenforflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Cria a rede neural\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net,optimizer='adam',loss='categorical_crossentropy',learning_rate=0.001)\n",
    "\n",
    "# Define o modelo e configura o tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "print(\"#Features (words)\",len(train_x[0]))\n",
    "print(\"Tamanho da camada de saída (categorias):\", len(train_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando o Treinamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.49909\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 100 | loss: 0.49909 - acc: 0.9808 -- iter: 16/21\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.50101\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 100 | loss: 0.50101 - acc: 0.9827 -- iter: 21/21\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Treinamento\n",
    "model.fit(train_x, train_y, n_epoch=100, batch_size=8, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/04449579445/workspaces/workspace_cognitiva/cursos_ia/pln_basico/models/model_dnn.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "#Salva o modelo\n",
    "model.save('models/model_dnn.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar o modelo para algumas frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ajustar os dados de teste antes de aplicar o modelo\n",
    "def get_tf_record(sentence):\n",
    "    global words\n",
    "    # Tokenização\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # Stem\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    # Bag of words\n",
    "    bow = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bow[i] = 1\n",
    "\n",
    "    return(np.array(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = \"você falou com ele ontem?\"\n",
    "sent_2 = \"você precisa ir agora?\"\n",
    "sent_3 = \"gostaria de me desculpar pelas falhas no relatório!\"\n",
    "sent_4 = \"você parece alguns anos mais velho que ela!\"\n",
    "sent_5 = \"Estou muito agradecido pelo sua ajuda no trabalho\"\n",
    "sent_6 = \"No passado joguei muito futebol\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imprimindo a previsão da classe das 6 sentenças de teste: \n",
      "você falou com ele ontem? ->  tempo\n",
      "você precisa ir agora? ->  despedida\n",
      "gostaria de me desculpar pelas falhas no relatório!  ->  desculpa\n",
      "você parece alguns anos mais velho que ela!  ->  idade\n",
      "Estou muito agradecido pelo sua ajuda no trabalho  ->  agradecimento\n",
      "No passado joguei muito futebol  ->  desculpa\n"
     ]
    }
   ],
   "source": [
    "# Previsões\n",
    "print(\"\\nImprimindo a previsão da classe das 6 sentenças de teste: \")\n",
    "print(sent_1,\"-> \",categories[np.argmax(model.predict([get_tf_record(sent_1)]))])\n",
    "print(sent_2,\"-> \",categories[np.argmax(model.predict([get_tf_record(sent_2)]))])\n",
    "print(sent_3,\" -> \",categories[np.argmax(model.predict([get_tf_record(sent_3)]))])\n",
    "print(sent_4,\" -> \",categories[np.argmax(model.predict([get_tf_record(sent_4)]))])\n",
    "print(sent_5,\" -> \",categories[np.argmax(model.predict([get_tf_record(sent_5)]))])\n",
    "print(sent_6,\" -> \",categories[np.argmax(model.predict([get_tf_record(sent_6)]))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando o Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tensorboard é uma ferramenta gráfica para visualizar os dados do treinamento gerados pelo TensorFlow. Por exemplos: os grafos da rede neural, métricas e dados de treinamento como imagens.\n",
    "\n",
    "Então, vamos executar o Tensorboard para poder comparar o desempenho dos diversos treinamentos. Para executar, abra um novo terminal, ative o ambiente anaconda 'pln_basico', navegue até a pasta onde está localizado este jupyter, verifique que existe o diretório 'tflearn_logs' e execute o comando abaixo:\n",
    "\n",
    "    tensorboard --logdir=tflearn_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercício 1\n",
    "\n",
    "Abra o arquivo data/data.json com um editor de texto e adicione novos dados para treinamento.\n",
    "\n",
    "Execute novamente este jupyter e veja os novos resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercício 2\n",
    "\n",
    "Veja que não estamos removendo stopwords neste exemplo. \n",
    "\n",
    "Modifique este jupyter notebook para remover stopwords tanto na preparação dos dados de treinamento quanto de teste.\n",
    "\n",
    "Dica: a lista 'portuguese_stops' já possui as stopwords para português.\n",
    "    \n",
    "Execute novamente este jupyter e veja os novos resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercício 3\n",
    "\n",
    "Modifique o desenho da rede neural alterando a quantidade de camadas e neurônios. \n",
    "\n",
    "Modifique também alguns parâmetros do treinamento como número de épocas, taxa de aprendizado e batch_size. \n",
    "\n",
    "Faça novas execuções do jupyter e procurando alcançar um desempenho melhor do modelo. \n",
    "\n",
    "Dica: É recomendável fazer modificações isoladas e executar o treinamento incrementalmente até atingir a acurácia desejada.\n",
    "\n",
    "Execute o Tensorboard para poder comparar o desempenho dos diversos treinamentos. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
