{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN) - Básico\n",
    "\n",
    "\n",
    "SERPRO - SUPSD - DIVISÃO DE DESENVOLVIMENTO E SUSTENTAÇÃO DE PRODUTOS COGNITIVOS\n",
    "\n",
    "\n",
    "## Parte 2 - Modelagem Estatística dos Dados\n",
    "\n",
    "Esta é a segunda parte do curso básico sobre PLN. Vimos até então como realizar o pré-processamento dos dados realizando operações básicas e essenciais para o PLN. Agora vamos abordar algumas modelagens estatísticas de forma que possamos estruturar os dados (textos). Os dados estruturados são a entrada para a próxima etapa onde iremos de fato criar um modelo para o processamento de linguagem natural. \n",
    "\n",
    "Conteúdo:\n",
    " - Modelagem Estatística da Linguagem\n",
    "     - N-Gramas\n",
    "     - Bag of Words (BoW)             \n",
    "     - TF-IDF \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos n-gramas\n",
    "\n",
    "Um  n-grama  é  simplesmente  uma  sequência  de  tokens. No  contexto  da  linguística computacional,  esses  tokens  são  geralmente  palavras,  embora  possam  ser  caracteres  ou subconjuntos de caracteres. O n simplesmente se refere ao número de tokens.\n",
    "\n",
    "Se estivermos contando palavras, a string \"Amanhã vai chover forte\" é um 4-gramas. Este 4-gramas contém os 3-gramas “Amanhã vai chover” e “vai chover forte”. Seguindo, o 3-gramas “Amanhã vai chover” contém os 2-gramas “Amanhã vai” e “vai chover”. Um unigrama é um único token, por exemplo, “Amanhã”.\n",
    "\n",
    "Mais precisamente, podemos usar modelos n-gramas para derivar uma probabilidade da sentença, W, como a probabilidade conjunta de cada palavra individual na sentença, wi: __P(W) = P(w1, w2, ..., wn)__.  \n",
    "\n",
    "Para  que  podemos  usar  modelos  n-grama?  Dadas  as  probabilidades  de  uma  frase, podemos determinar a probabilidade de uma tradução automática automatizada estar correta, podemos prever que a próxima palavra mais provável ocorrerá em uma frase, podemos gerar automaticamente texto da fala, automatizar correção ortográfica ou determinar o sentimento relativo de um texto.\n",
    "\n",
    "Um exemplo de problema que necessita de inferência estatística é a previsão da palavra seguinte em uma frase, dadas as palavras anteriores. Uma sequência de palavras pode começar de uma maneira conhecida, mas terminar por uma palavra desconhecida.\n",
    "\n",
    "Os casos de n-gramas mais utilizados são com n = 2, 3 e 4, particularmente denominados bigramas, trigramas e tetragramas. Quanto maior o valor de n, isto é, maior o número de classes que dividem os dados, maior a confiabilidade da inferência. No entanto, o número de parâmetros a serem estimados cresce  exponencialmente  em  relação  a  n.  Por  isso,  geralmente  são  utilizados  bigramas  ou trigramas em sistemas dessa natureza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo utilizando N-grams com a biblioteca NLTK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "?ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto: \n",
      "eu preciso escrever um programa em nltk que quebra um corpus (uma grande coleção de arquivos txt) em unigramas, bigramas, trigramas e quatro gramas. eu preciso escrever um programa em nltk que quebra um corpus\n",
      "\n",
      "Bigramas: \n",
      "Counter({('eu', 'preciso'): 2, ('preciso', 'escrever'): 2, ('escrever', 'um'): 2, ('um', 'programa'): 2, ('programa', 'em'): 2, ('em', 'nltk'): 2, ('nltk', 'que'): 2, ('que', 'quebra'): 2, ('quebra', 'um'): 2, ('um', 'corpus'): 2, ('corpus', '('): 1, ('(', 'uma'): 1, ('uma', 'grande'): 1, ('grande', 'coleção'): 1, ('coleção', 'de'): 1, ('de', 'arquivos'): 1, ('arquivos', 'txt'): 1, ('txt', ')'): 1, (')', 'em'): 1, ('em', 'unigramas'): 1, ('unigramas', ','): 1, (',', 'bigramas'): 1, ('bigramas', ','): 1, (',', 'trigramas'): 1, ('trigramas', 'e'): 1, ('e', 'quatro'): 1, ('quatro', 'gramas'): 1, ('gramas', '.'): 1, ('.', 'eu'): 1})\n",
      "\n",
      "\n",
      "\n",
      "Trigramas: \n",
      "Counter({('eu', 'preciso', 'escrever'): 2, ('preciso', 'escrever', 'um'): 2, ('escrever', 'um', 'programa'): 2, ('um', 'programa', 'em'): 2, ('programa', 'em', 'nltk'): 2, ('em', 'nltk', 'que'): 2, ('nltk', 'que', 'quebra'): 2, ('que', 'quebra', 'um'): 2, ('quebra', 'um', 'corpus'): 2, ('um', 'corpus', '('): 1, ('corpus', '(', 'uma'): 1, ('(', 'uma', 'grande'): 1, ('uma', 'grande', 'coleção'): 1, ('grande', 'coleção', 'de'): 1, ('coleção', 'de', 'arquivos'): 1, ('de', 'arquivos', 'txt'): 1, ('arquivos', 'txt', ')'): 1, ('txt', ')', 'em'): 1, (')', 'em', 'unigramas'): 1, ('em', 'unigramas', ','): 1, ('unigramas', ',', 'bigramas'): 1, (',', 'bigramas', ','): 1, ('bigramas', ',', 'trigramas'): 1, (',', 'trigramas', 'e'): 1, ('trigramas', 'e', 'quatro'): 1, ('e', 'quatro', 'gramas'): 1, ('quatro', 'gramas', '.'): 1, ('gramas', '.', 'eu'): 1, ('.', 'eu', 'preciso'): 1})\n",
      "\n",
      "\n",
      "\n",
      "Tetragramas: \n",
      "Counter({('eu', 'preciso', 'escrever', 'um'): 2, ('preciso', 'escrever', 'um', 'programa'): 2, ('escrever', 'um', 'programa', 'em'): 2, ('um', 'programa', 'em', 'nltk'): 2, ('programa', 'em', 'nltk', 'que'): 2, ('em', 'nltk', 'que', 'quebra'): 2, ('nltk', 'que', 'quebra', 'um'): 2, ('que', 'quebra', 'um', 'corpus'): 2, ('quebra', 'um', 'corpus', '('): 1, ('um', 'corpus', '(', 'uma'): 1, ('corpus', '(', 'uma', 'grande'): 1, ('(', 'uma', 'grande', 'coleção'): 1, ('uma', 'grande', 'coleção', 'de'): 1, ('grande', 'coleção', 'de', 'arquivos'): 1, ('coleção', 'de', 'arquivos', 'txt'): 1, ('de', 'arquivos', 'txt', ')'): 1, ('arquivos', 'txt', ')', 'em'): 1, ('txt', ')', 'em', 'unigramas'): 1, (')', 'em', 'unigramas', ','): 1, ('em', 'unigramas', ',', 'bigramas'): 1, ('unigramas', ',', 'bigramas', ','): 1, (',', 'bigramas', ',', 'trigramas'): 1, ('bigramas', ',', 'trigramas', 'e'): 1, (',', 'trigramas', 'e', 'quatro'): 1, ('trigramas', 'e', 'quatro', 'gramas'): 1, ('e', 'quatro', 'gramas', '.'): 1, ('quatro', 'gramas', '.', 'eu'): 1, ('gramas', '.', 'eu', 'preciso'): 1, ('.', 'eu', 'preciso', 'escrever'): 1})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Texto\n",
    "text = \"Eu preciso escrever um programa em NLTK que quebra um corpus (uma grande coleção de arquivos txt) em unigramas, bigramas, trigramas e quatro gramas. Eu preciso escrever um programa em NLTK que quebra um corpus\"\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenização\n",
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "# N-gramas\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "\n",
    "# Imprime na tela os n-gramas e suas respectivas ocorrências\n",
    "print(\"\\nTexto: \")\n",
    "print(text)\n",
    "print(\"\\nBigramas: \")\n",
    "print (Counter(bigrams))\n",
    "print(\"\\n\")\n",
    "print(\"\\nTrigramas: \")\n",
    "print (Counter(trigrams))\n",
    "print(\"\\n\")\n",
    "print(\"\\nTetragramas: \")\n",
    "print (Counter(fourgrams))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercício 1\n",
    "\n",
    "Copie e cole a célula acima.\n",
    "\n",
    "Modifique a nova célula para que antes da criação dos N-gramas sejam removidas as stopwords e feito o stemming com RSLPStemmer\n",
    "\n",
    "Dica:\n",
    "1. portuguese_stemmer = RSLPStemmer()\n",
    "2. portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "\n",
    "Execute novamente a célula e verifique os resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Conceituação\n",
    "\n",
    "O modelo de \"saco de palavras\" é uma representação simplificada usada no processamento de linguagem natural e recuperação de informação. Neste modelo, um texto (como uma sentença ou um documento) é representado como o saco (multiset) de suas palavras, desconsiderando a gramática e até a ordem das palavras, mas mantendo a multiplicidade.\n",
    "\n",
    "Na classificação de documentos, um saco de palavras é um vetor esparso de ocorrência de contagens de palavras; Ou seja, um histograma esparso sobre o vocabulário.\n",
    "\n",
    "O BOW facilita  nossa  vida  porque  simplifica  a representação usada em PLN. Vamos mostrar um exemplo para você entender melhor o  BOW. Vamos pegar o seguinte conjunto de documentos:\n",
    "\n",
    " - Documento de texto 1: __Joao gosta de assistir ao futebol. Jose gosta de futebol também__. \n",
    " - Documento de texto 2: __Joao também gosta de assistir a filmes__. \n",
    "\n",
    "Com base nesses dois documentos de texto, você pode gerar a seguinte lista de palavras (vocabulário):\n",
    "\n",
    "Lista  de  palavras (Vocabulário)= __[\"Joao\",  \"gosta\",  \"de\", \"assistir\", \"ao\", \"futebol\",  \"Jose\", \"também\", \"a\", \"filmes\"]__\n",
    "\n",
    "Para  os  documentos  anteriores,  você  pode gerar  a  seguinte  lista  de  frequências: \n",
    "- Contagem de frequência para o documento 1: __[1, 2, 2, 1, 1, 2, 1, 1, 0, 0]__\n",
    "- Contagem de frequência para o Documento 2: __[1, 1, 1, 1, 0, 0, 0, 0, 1, 1]__\n",
    "\n",
    "Esta lista é chamada BOW (Bag of Words). Aqui, não estamos considerando a gramática das sentenças. Nós também não estamos incomodados com a ordem das palavras. \n",
    "\n",
    "Então,  como  geramos  a  lista  de  contagens  de  frequência?  Para  gerar  a contagem de frequência do Documento 1, considere a lista de palavras e verifique quantas vezes cada uma das palavras listadas aparece no Documento 1. \n",
    "\n",
    "Aqui, primeiro pegamos a palavra __\"Joao\"__, que aparece no Documento 1 uma única  vez;  a  contagem  de  frequência  para  o  Documento  1  é  1.  Contagem  de frequência para o Documento 1: __[1]__. \n",
    "\n",
    "Para a segunda entrada, a palavra __\"gosta\"__ aparece duas vezes no Documento 1, portanto, a  contagem  de  frequência  é  2.  Contagem  de frequência para  o Documento 1: __[1, 2]__. \n",
    "\n",
    "Então seguindo a lógica geramos a contagem de frequência para o Documento 1 e Documento 2. \n",
    "\n",
    "Depois de gerar a BOW, podemos  derivar  o  termo-frequência (Term-Frequency) de  cada  palavra  no documento,  que  pode  ser  posteriormente  alimentado em um  algoritmo  de aprendizado de máquina. Vamos aprender mais sobre frequência quando estudarmos TF-IDF, próximo tópico.\n",
    "\n",
    "Agora, seguem alguns exemplos para criação do BoW:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando o modelo de BoW\n",
    "Neste exemplo o BoW irá conter a contagem da ocorrência das features (palavras) no texto do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras tokenizadas doc1: ['Joao', 'gosta', 'de', 'assistir', 'ao', 'futebol', '.', 'Jose', 'gosta', 'de', 'futebol', 'também', '.']\n",
      "Palavras tokenizadas doc2: ['Joao', 'também', 'gosta', 'de', 'assistir', 'a', 'filmes', '.']\n",
      "Lista de palavras (Vocabulário): ['.', 'Joao', 'Jose', 'a', 'ao', 'assistir', 'de', 'filmes', 'futebol', 'gosta', 'também']\n",
      "Bow documento 1 [2, 1, 1, 0, 1, 1, 2, 0, 2, 2, 1]\n",
      "Bow documento 2 [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "doc1 = 'Joao gosta de assistir ao futebol. Jose gosta de futebol também.' \n",
    "doc2 = 'Joao também gosta de assistir a filmes.'\n",
    "\n",
    "# Tokenizacao dos documentos\n",
    "words_doc1 = nltk.word_tokenize(doc1)\n",
    "print(\"Palavras tokenizadas doc1:\", words_doc1)\n",
    "\n",
    "words_doc2 = nltk.word_tokenize(doc2)\n",
    "print(\"Palavras tokenizadas doc2:\", words_doc2)\n",
    "\n",
    "# Vocabulário com todas as Palavras dos documentos\n",
    "words = words_doc1+words_doc2\n",
    "\n",
    "# Obtem a lista  as palavras dos documentos (remove as repetidas e ordena)\n",
    "words = sorted(list(set(words)))\n",
    "print (\"Lista de palavras (Vocabulário):\",words)\n",
    "\n",
    "bow = []\n",
    "\n",
    "for w in words:\n",
    "        #bow.append(1) if w in words_doc1 else bow.append(0) # Binário\n",
    "        bow.append(words_doc1.count(w)) #Contagem da palavra\n",
    "\n",
    "print('Bow documento 1',bow)\n",
    "\n",
    "bow = []\n",
    "for w in words:\n",
    "        #bow.append(1) if w in words_doc2 else bow.append(0)\n",
    "        bow.append(words_doc2.count(w))\n",
    "\n",
    "print('Bow documento 2',bow)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Exercício 2\n",
    "\n",
    "Copie e cole a célula acima.\n",
    "\n",
    "Modifique a nova célula para que sejam removidas as stopwords e feito o stemming com RSLPStemmer para cada documento.\n",
    "\n",
    "Inclua um novo documento com uma frase à sua escolha e faça o BoW com o vocabulário criado anteriormente com os outros 2 documentos.\n",
    "\n",
    "Execute novamente a célula e verifique os resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando BoW a partir do CountVectorizer \n",
    "\n",
    "Neste exemplo criamos um BoW com as frequências das ocorrências das features do documento. As features neste caso serão os bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (bigramas) - Vocabulário:\n",
      "['ao futebol', 'assistir ao', 'assistir filmes', 'de assistir', 'de futebol', 'futebol jose', 'futebol também', 'gosta de', 'joao gosta', 'joao também', 'jose gosta', 'também gosta']\n",
      "Bow - BagOfN-Gram:\n",
      "[[1 1 0 1 1 1 1 2 1 0 1 0]\n",
      " [0 0 1 1 0 0 0 1 0 1 0 1]]\n",
      "(2, 12)\n"
     ]
    }
   ],
   "source": [
    "doc1 = 'Joao gosta de assistir ao futebol. Jose gosta de futebol também' \n",
    "doc2 = 'Joao também gosta de assistir a filmes'\n",
    "\n",
    "# Cria um ngram vectorizer\n",
    "# Vamos criar o BOW dos 2 documentos\n",
    "# Usamos n_gram = (2,2)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Contabilizamos as combinações de caracteres nas palavras\n",
    "bows = ngram_vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "# Imprimimir as features\n",
    "print ('Features (bigramas) - Vocabulário:')\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "\n",
    "# Imprimir o BOW\n",
    "print ('Bow - BagOfN-Gram:')\n",
    "print (bows.toarray().astype(int))\n",
    "print(bows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Exercício 3\n",
    "\n",
    "Copie e cole a célula acima.\n",
    "\n",
    "Inclua mais um documento com uma frase à sua escolha.\n",
    "\n",
    "Consulte a documentação da classe CountVectorizer para adicionar dois novos parâmetros:\n",
    "    \n",
    "- stop_words para poder remover stopwords em português\n",
    "- max_features para limitar em 5 o número de palavras do vocabulário\n",
    "\n",
    "Execute novamente a célula e verifique os resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra forma é passar um vocabulário para limitar o BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW para o vocabulário: [[1 0 1 1 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Outra forma é passar um vocabulário para limitar o BOW\n",
    "vocabulary = ['ao futebol', 'assistir filmes', 'de futebol', 'futebol também', 'gosta de', 'joao também', 'também gosta']\n",
    "\n",
    "vocabulary_vec = CountVectorizer(ngram_range=(2,2), vocabulary = vocabulary)\n",
    "print(\"BOW para o vocabulário:\",vocabulary_vec.fit_transform([doc1]).toarray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também obter o BOW do documento usando apenas as features mais relevantes com nltk.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joao', 'gosta', 'de', 'assistir', 'ao', 'futebol', '.', 'jose', 'gosta', 'de', 'futebol', 'também', '.']\n",
      "['joao', 'também', 'gosta', 'de', 'assistir', 'a', 'filmes', '.']\n",
      "Features mais frequentes: ['gosta', 'de', '.', 'joao', 'assistir', 'futebol', 'também', 'ao', 'jose', 'a']\n",
      "BOW com relação ao vocabulário: [[2 2 0 1 1 2 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Documentos\n",
    "words_doc1 = [w.lower() for w in words_doc1]\n",
    "words_doc2 = [w.lower() for w in words_doc2]\n",
    "print(words_doc1)\n",
    "print(words_doc2)\n",
    "\n",
    "# Obtem o vocabulário com as features mais relevante\n",
    "words_freq=nltk.FreqDist(words_doc1+words_doc2)\n",
    "word_features_top = [i[0] for i in words_freq.most_common(10)]\n",
    "print(\"Features mais frequentes:\", word_features_top)\n",
    "\n",
    "# Gera o Bow limitando as 10 features mais utilizadas\n",
    "relevant_vec = CountVectorizer(vocabulary = word_features_top)\n",
    "bow_top = relevant_vec.fit_transform([doc1]).toarray().astype(int)\n",
    "print(\"BOW com relação ao vocabulário:\",bow_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos N-grama x Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gramas e BOW são diferentes, entretanto possuem relacionamentos. \n",
    "\n",
    "<p style='text-align: justify;'>A diferença está em termos de seu uso em  aplicativos de PLN.  Em  n-gramas,  a  ordem  das  palavras  é importante, enquanto que na BOW não é importante manter a ordem das palavras. Durante a aplicação de PLN, n-gram é usado para considerar as palavras em sua ordem real, para que possamos ter uma ideia sobre o contexto da palavra em particular. BOW é usado para construir vocabulário para o seu conjunto de dados de texto.</p>\n",
    "\n",
    "<p style='text-align: justify;'>N-gramas e BOW estão relacionados entre si. Considerando-se n-grama como um recurso, então BOW é a representação de texto derivada usando um unigrama. Portanto, nesse caso, um n-grama é igual a um recurso e o BOW é igual a uma representação de texto usando um unigrama (um grama) contido nele. Da mesma forma podemos ter um BOW utilizando um bigrama seria uma representação do texto usando os bigramas contidos nele.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrizes de Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de estudar TF-IDF vamos compreender o que são matrizes de documentos.\n",
    "\n",
    "<p style='text-align: justify;'>Ao trabalhar com arquivos de texto estamos trabalhando com dados não estruturados. Entretanto,  nós precisamos antes transformar esses dados não estruturados colocando algum nível de estrutura neles. Para então aplicar técnicas de análise de dados e construir aplicações de PLN. Uma das formas de se aplicar uma estrutura é utilizar uma matriz de documentos. </p>\n",
    "\n",
    "Um conceito importante é que a representação de um documento, de forma geral, se dá utilizando um conjunto de palavras-chaves(ou termos).\n",
    "\n",
    "<p style='text-align: justify;'>Para modelar uma matriz de documentos antes é importante aplicar técnicas de pré-processamento, como remoção de stopwords, steeming etc melhorar a sua representação. É possível modelar cada documento como um vetor numa matriz de dimensão n_x_m, onde n é o conjunto de documentos e m é o número de termos. Os vetores podem ser:</p>\n",
    "\n",
    "- Binários\n",
    "- Ternários\n",
    "- Frequência Absoluta\n",
    "- Frequência Relativa\n",
    "\n",
    "__Matriz Binária (Booleana)__ \n",
    "\n",
    "Os vetores são binários: 0 para ausência do termo e 1 para presença do termo no documento.  \n",
    "Segue exemplo de uma matriz binária onde t representa os termos (linhas) e d os documentos (colunas):\n",
    "![matriz_binaria.png](images/matriz_binaria.png)\n",
    "\n",
    "__Matriz Ternária__ \n",
    "\n",
    "Na matriz ternária adiciona-se um terceiro valor ao vetor: 0 para ausência do termo, 1 para presença do termo no documento e 2 para quando o termo ocorre mais de uma vez.\n",
    "\n",
    "__Matriz de Frequência Absoluta__\n",
    "\n",
    "Os vetores contém a quantidade de vezes (frequência absoluta) que o termo aparece em cada documento. O que pode ser mais relevante que saber apenas se o termo está presente ou não. Exemplo:\n",
    "![matriz_freq_abs.png](images/matriz_freq_abs.png)\n",
    "\n",
    "__Matriz de Frequência Relativa__\n",
    "\n",
    "Os vetores contém a frequência relativa de cada termo no documento, ou seja, a frequência absoluta dividido pelo número total das ocorrências de todos os termos no documento. Exemplo:\n",
    "![matriz_freq_rel.png](images/matriz_freq_rel.png)\n",
    "\n",
    "\n",
    "Como podemos perceber, o modelo BOW é uma matriz de documentos. \n",
    "\n",
    "Uma vez obtida a matriz de documentos é possível aplicar qualquer métrica de distância, para verificar documentos similares, uma vez que o esperado é que tais documentos possuam frequências similares. \n",
    "\n",
    "Além disso é possível modificar a matriz de frequência de forma a considerar a importância percebida daquele termo. A formulação TF-IDF então é utilizada para computar pesos ou scores para os termos e indicando para a análise quais desses termos são mais relevantes para o documento. \n",
    "\n",
    "Vamos então entender o que é o TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__TF-IDF__ (abreviação do inglês _term frequency-inverse document frequency_, que significa frequência do termo-inverso da frequência nos documentos), é uma medida estatística que tem o objetivo de indicar a importância de uma palavra de um documento em relação a uma coleção de documentos.  \n",
    "\n",
    "__Características da Métrica TF-IDF__: \n",
    "- Quando o termo aparece em muitos documentos ele é considerado irrelevante e o fator de escala é diminuído, tendendo a zero.\n",
    "- Quando o termo é relativamente único e aparece em poucos documentos o fator de escala aumenta uma vez que ele parece ser importante \n",
    "- O resultado desse processo é um score positivo que substitui a frequência em célula em nossa tabela/matrix. \n",
    "- Quanto maior o score mais importante seu valor esperado para o método de aprendizado\n",
    " \n",
    " _Em resumo, quanto maior a pontuação TF*IDF (peso), mais raro o termo e vice-versa._\n",
    "\n",
    " \n",
    "__Derivando o Score TF-IDF__:\n",
    "\n",
    "Uma Frequência de Termo (__TF__) é uma contagem de quantas vezes uma palavra ocorre num determinado documento (sinônimo de Bag of Words). Normalmente é usado a frequência relativa.\n",
    "\n",
    "A Frequência Inversa do Documento (__IDF__) é o número de vezes que uma palavra ocorre em um Corpus de documentos. \n",
    "\n",
    "\n",
    "\n",
    "__TF-IDF__ é usado para ponderar as palavras de acordo com a importância delas. Palavras que são usadas com frequência em muitos documentos terão uma ponderação mais baixa, enquanto as menos frequentes terão uma ponderação mais alta. \n",
    "\n",
    "Assim, __TF-IDF__ é uma técnica de recuperação de informações que pesa a frequência de um termo (__TF__) e sua frequência inversa no documento (__IDF__). Cada palavra ou termo tem sua respectiva pontuação __TF__ e __IDF__. O produto das pontuações __TF__ e __IDF__ é chamado de __peso TF*IDF__ deste termo. \n",
    "\n",
    "\n",
    "O algoritmo __TF-IDF__ é usado para pesar um palavra-chave em qualquer conteúdo e atribuir a sua importância com base no número de vezes que ela aparece no documento. Mais importante, verifica a relevância da palavra chave em todo o Corpus. Ou seja, saber a partir de uma contagem, quais são as palavras ou termos mais relevantes dentro de um documento ou de uma coleção de documentos.\n",
    "\n",
    "Os três scores podem ser calculados pelas seguintes fórmulas:\n",
    "\n",
    "- __TF__  = número de vezes que o termo t aparece no documento sobre o número total de termos do documento\n",
    "- __IDF__ = log do número total de documentos (N) sobre o número de documentos com o termo t (log * N/Nt)\n",
    "- __TF-IDF__ = TF * IDF\n",
    "\n",
    "Essa funções já estão disponíveis em vários pacotes da linguagem Python que iremos ver em seguida.\n",
    "\n",
    "\n",
    "__Aplicações do TF-IDF__\n",
    "\n",
    "- Análise de texto: pode-se obter informações sobre as palavras mais precisas para o seu conjunto de dados.\n",
    "- Gerar resumo de texto em que você tenha uma abordagem estatística\n",
    "- Em mecanismos de busca para descobrir a pontuação e a classificação da relevância de um documento para uma determinada consulta do usuário. \n",
    "- Classificação de documentos junto com o BOW\n",
    "- Identificação de documentos similares\n",
    "\n",
    "A seguir vamos ver alguns exemplos de implementação e uso do TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF com Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos implementar o TF-IDF utilizando a lib Textblob (TextBlob: Simplified Text Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (0.15.3)\r\n",
      "Requirement already satisfied: nltk>=3.1 in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (from textblob) (3.4)\r\n",
      "Requirement already satisfied: six in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.12.0)\r\n",
      "Requirement already satisfied: singledispatch in /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages (from nltk>=3.1->textblob) (3.4.0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "\n",
    "# Imports\n",
    "from textblob import TextBlob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de Funções TF-IDF\n",
    "\n",
    "#TF: Calculo da frequência relativa \n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "# Retorna o número de documento que contém a palavra\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "def idf(word, bloblist):\n",
    "    #Adiciona 1 para previnir divição por zero\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "# Score TF-IDF\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto\n",
    "text1 = 'tf idf, forma abreviada de frequência de termo, frequência de documento inversa'\n",
    "text2 = 'é uma estatística numérica que se destina a refletir o quão importante'\n",
    "text3 = 'uma palavra é para um documento em uma coleção ou corpus'\n",
    "\n",
    "blob = TextBlob(text1)\n",
    "blob2 = TextBlob(text2)\n",
    "blob3 = TextBlob(text3)\n",
    "\n",
    "# Corpus\n",
    "bloblist = [blob, blob2, blob3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores para o documento text1:\n",
      "\n",
      "\n",
      "Scores para a palavra: abreviada\n",
      "TF: 0.08333333333333333\n",
      "IDF: 0.4054651081081644\n",
      "TF-IDF: 0.033788759009013694\n",
      "\n",
      "\n",
      "Scores para a palavra: frequência\n",
      "TF: 0.16666666666666666\n",
      "IDF: 0.4054651081081644\n",
      "TF-IDF: 0.06757751801802739\n",
      "\n",
      "\n",
      "Scores para a palavra: destina\n",
      "TF: 0.0\n",
      "IDF: 0.4054651081081644\n",
      "TF-IDF: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_scores(word,blob,bloblist):\n",
    "    # Scores para a palavra\n",
    "    tf_score = tf(word, blob)\n",
    "    idf_score = idf(word, bloblist)\n",
    "    tfidf_score = tfidf(word, blob, bloblist)\n",
    "    \n",
    "    # Print\n",
    "    print(\"Scores para a palavra:\",word)\n",
    "    print (\"TF:\" ,str(tf_score))\n",
    "    print (\"IDF:\",str(idf_score))\n",
    "    print (\"TF-IDF:\",str(tfidf_score))\n",
    "    print (\"\\n\")\n",
    "    \n",
    "print('Scores para o documento text1:')\n",
    "print (\"\\n\")\n",
    "print_scores('abreviada',blob,bloblist)\n",
    "print_scores('frequência',blob,bloblist)\n",
    "print_scores('destina',blob,bloblist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos ver um exemplo com texto onde usam a palavra \"Python\" em 3 contextos. Queremos saber quais palavras são mais significativas para cada texto em seus respectivos contextos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: python, TF-IDF: 0.01662\n",
      "\tWord: films, TF-IDF: 0.00997\n",
      "\tWord: made-for-TV, TF-IDF: 0.00665\n",
      "\tWord: film, TF-IDF: 0.00665\n",
      "\tWord: on, TF-IDF: 0.00665\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.02317\n",
      "\tWord: from, TF-IDF: 0.01158\n",
      "\tWord: Greek, TF-IDF: 0.01158\n",
      "\tWord: word, TF-IDF: 0.01158\n",
      "\tWord: πύθων/πύθωνας, TF-IDF: 0.01158\n",
      "Top words in document 3\n",
      "\tWord: Colt, TF-IDF: 0.01367\n",
      "\tWord: Magnum, TF-IDF: 0.01367\n",
      "\tWord: revolver, TF-IDF: 0.01367\n",
      "\tWord: 's, TF-IDF: 0.00911\n",
      "\tWord: 357, TF-IDF: 0.00456\n"
     ]
    }
   ],
   "source": [
    "document1 = TextBlob(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "document2 = TextBlob(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons found in Africa and Asia. Currently, 7 species are\n",
    "recognised. A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "document3 = TextBlob(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")\n",
    "\n",
    "bloblist = [document1, document2, document3]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:5]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem formas de melhorar o resultado do TF_IDF, uma delas é ignorar as stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF com scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos utilizar o TfidfVectorizer do _scikit-learn_ para obter o TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "Tamanho Vocabulário: 9\n",
      "TF-IDF\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 1\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print('Vocabulário:',vectorizer.get_feature_names())\n",
    "print('Tamanho Vocabulário:', len(vectorizer.get_feature_names()))\n",
    "print('TF-IDF')\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['car', 'driven', 'highway', 'is', 'on', 'road', 'the', 'truck']\n",
      "TF-IDF:   (0, 6)\t0.6043795515372431\n",
      "  (0, 0)\t0.42471718586982765\n",
      "  (0, 3)\t0.30218977576862155\n",
      "  (0, 1)\t0.30218977576862155\n",
      "  (0, 4)\t0.30218977576862155\n",
      "  (0, 5)\t0.42471718586982765\n",
      "  (1, 6)\t0.6043795515372431\n",
      "  (1, 3)\t0.30218977576862155\n",
      "  (1, 1)\t0.30218977576862155\n",
      "  (1, 4)\t0.30218977576862155\n",
      "  (1, 7)\t0.42471718586982765\n",
      "  (1, 2)\t0.42471718586982765\n",
      "TF-IDF BOW: [[0.42471719 0.30218978 0.         0.30218978 0.30218978 0.42471719\n",
      "  0.60437955 0.        ]\n",
      " [0.         0.30218978 0.42471719 0.30218978 0.30218978 0.\n",
      "  0.60437955 0.42471719]]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 2\n",
    "sent1 = \"The car is driven on the road.\"\n",
    "sent2 = \"The truck is driven on the highway\"\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([sent1,sent2])\n",
    "print(\"Vocabulário:\",vectorizer.get_feature_names())\n",
    "print(\"TF-IDF:\",X)\n",
    "print(\"TF-IDF BOW:\",X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Features: 15\n",
      "['abreviada frequencia', 'colecao corpus', 'destina refletir', 'documento colecao', 'documento inversa', 'estatistica numerica', 'forma abreviada', 'frequencia documento', 'frequencia termo', 'idf forma', 'numerica destina', 'palavra documento', 'quao importante', 'refletir quao', 'termo frequencia']\n",
      "(3, 15)\n",
      "\n",
      "\n",
      "0 : tf idf, é uma forma abreviada de frequência de termo, frequência de documento inversa\n",
      "1 : é uma estatística numérica que se destina a refletir o quão importante\n",
      "2 : uma palavra é para um documento em uma coleção ou corpus\n",
      "------------\n",
      "abreviada frequencia           Score: 0.37796447300922725\n",
      "documento inversa              Score: 0.37796447300922725\n",
      "forma abreviada                Score: 0.37796447300922725\n",
      "frequencia documento           Score: 0.37796447300922725\n",
      "frequencia termo               Score: 0.37796447300922725\n",
      "idf forma                      Score: 0.37796447300922725\n",
      "termo frequencia               Score: 0.37796447300922725\n",
      "------------\n",
      "destina refletir               Score: 0.4472135954999579\n",
      "estatistica numerica           Score: 0.4472135954999579\n",
      "numerica destina               Score: 0.4472135954999579\n",
      "quao importante                Score: 0.4472135954999579\n",
      "refletir quao                  Score: 0.4472135954999579\n",
      "------------\n",
      "colecao corpus                 Score: 0.5773502691896257\n",
      "documento colecao              Score: 0.5773502691896257\n",
      "palavra documento              Score: 0.5773502691896257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 3\n",
    "# Criando um corpus\n",
    "text1 = 'tf idf, é uma forma abreviada de frequência de termo, frequência de documento inversa'\n",
    "text2 = 'é uma estatística numérica que se destina a refletir o quão importante'\n",
    "text3 = 'uma palavra é para um documento em uma coleção ou corpus'\n",
    "corpus = [text1,text2,text3]\n",
    "\n",
    "# Aplicando TF-IDF\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),stop_words=portuguese_stops,strip_accents='unicode',max_features=15)\n",
    "tfidf_result = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Imprimindo informações\n",
    "print(\"Total de Features:\",len(vectorizer.get_feature_names()))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(tfidf_result.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Imprimindo o corpus\n",
    "for i,text in enumerate(corpus):\n",
    "    print(i,\":\",text)\n",
    "    \n",
    "# Imprimindo os valores TF-IDF para cada token (TOKEN x TDIF)\n",
    "for doc in tfidf_result:\n",
    "    print('------------')\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(doc.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    for item in sorted_scores:\n",
    "        if item[1] > 0.0:\n",
    "            print (\"{0:30} Score: {1}\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos mostrar um exemplo utilizando um arquivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['this', 'young', 'gentlewoman', 'had', 'a', 'fathero', 'that', 'had', 'how', 'sad', 'a', 'passage', 'tiswhose', 'skill', 'was', 'almost', 'as', 'great', 'as', 'his', 'honesty', 'had', 'it', 'stretched', 'so', 'far', 'would', 'have', 'made', 'nature', 'immortal', 'and', 'death', 'should', 'have', 'play', 'for', 'lack', 'of', 'work', 'would', 'for', 'the', 'kings', 'sake', 'he', 'were', 'living', 'i', 'think'] ...\n",
      "Número de Tokens:  1675\n"
     ]
    }
   ],
   "source": [
    "# Função para obter os tokens\n",
    "def get_tokens():\n",
    "   with open('data/shakes/shakes1.txt', 'r') as shakes:\n",
    "    text = shakes.read()\n",
    "    lowers = text.lower()\n",
    "    \n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    no_punctuation = lowers.translate(table)    \n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "# Obtém os tokens\n",
    "tokens = get_tokens()\n",
    "print(\"Tokens: \", tokens[:50], \"...\")\n",
    "print(\"Número de Tokens: \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words:  ['young', 'gentlewoman', 'fathero', 'sad', 'passage', 'tiswhose', 'skill', 'almost', 'great', 'honesty', 'stretched', 'far', 'would', 'made', 'nature', 'immortal', 'death', 'play', 'lack', 'work', 'would', 'kings', 'sake', 'living', 'think', 'would', 'death', 'kings', 'disease', 'excellent', 'indeed', 'madam', 'king', 'lately', 'spoke', 'admiringly', 'mourningly', 'skilful', 'enough', 'lived', 'still', 'knowledge', 'could', 'set', 'mortality', 'sole', 'child', 'lord', 'bequeathed', 'overlooking'] ...\n",
      "Número de Tokens:  814\n"
     ]
    }
   ],
   "source": [
    "# Remove as Stop words\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "print(\"Tokens sem stop words: \", filtered[:50], \"...\")\n",
    "print(\"Número de Tokens: \", len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words e com stemming:  ['young', 'gentlewoman', 'fathero', 'sad', 'passag', 'tiswhos', 'skill', 'almost', 'great', 'honesti', 'stretch', 'far', 'would', 'made', 'natur', 'immort', 'death', 'play', 'lack', 'work', 'would', 'king', 'sake', 'live', 'think', 'would', 'death', 'king', 'diseas', 'excel', 'inde', 'madam', 'king', 'late', 'spoke', 'admiringli', 'mourningli', 'skil', 'enough', 'live', 'still', 'knowledg', 'could', 'set', 'mortal', 'sole', 'child', 'lord', 'bequeath', 'overlook']\n"
     ]
    }
   ],
   "source": [
    "# Função para Stem\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "# Aplica o Stem\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered, stemmer)\n",
    "print(\"Tokens sem stop words e com stemming: \", stemmed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo agora para todos os arquivos para criar o corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis de automatização\n",
    "path = 'data/shakes'\n",
    "token_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando funções\n",
    "# Remove punctuation\n",
    "def no_punctuation(text):\n",
    "    lowers = text.lower()\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    no_punctuation = lowers.translate(table)\n",
    "    no_punctuation = no_punctuation.replace('\\n', ' ')\n",
    "    return no_punctuation\n",
    "\n",
    "# Faz Tokenização e chama no_punctuation()\n",
    "def stem_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(no_punctuation(text))\n",
    "    tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de arquivos:  4\n"
     ]
    }
   ],
   "source": [
    "# Obtém todos os arquivos para criar o Corpus \n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        token_dict[file] = shakes.read()\n",
    "print(\"Quantidade de arquivos: \", len(token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['shakes2.txt', 'shakes1.txt', 'shakes4.txt', 'shakes3.txt'])\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo os índices do dicionário criado\n",
    "print(token_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I remember, Adam, it was upon this fashion\n",
      "bequeathed me by will but poor a thousand crowns,\n",
      "and, as thou sayest, charged my brother, on his\n",
      "blessing, to breed me well: and there begins my\n",
      "sadness.\n"
     ]
    }
   ],
   "source": [
    "# Imprime o texto do primeiro arquivo\n",
    "print(token_dict['shakes2.txt'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adam', 'age', 'almost', 'along', 'away', 'bear', 'begin', 'bequeath', 'bertram', 'besid', 'best', 'better', 'blood', 'bodi', 'bond', 'break', 'breed', 'bright', 'brother', 'carri', 'chang', 'charl', 'come', 'comfort', 'commend', 'compani', 'confess', 'court', 'crown', 'daughter', 'dearer', 'death', 'desir', 'devic', 'die', 'doth', 'duke', 'educ', 'end', 'enemi', 'entreat', 'enviou', 'even', 'everi', 'excel', 'eye', 'fair', 'fare', 'fashion', 'father', 'favour', 'feed', 'find', 'foil', 'forth', 'fortun', 'fourscor', 'free', 'friend', 'full', 'gainst', 'gentl', 'gentlewoman', 'give', 'go', 'gone', 'good', 'grace', 'great', 'ground', 'hath', 'he', 'head', 'heart', 'heaven', 'hereaft', 'hind', 'hire', 'home', 'honesti', 'hope', 'ill', 'inde', 'jaqu', 'keep', 'kill', 'kindli', 'king', 'know', 'knowledg', 'lack', 'ladi', 'late', 'leav', 'let', 'lie', 'life', 'light', 'like', 'live', 'look', 'lord', 'love', 'lusti', 'made', 'make', 'malic', 'man', 'manner', 'master', 'may', 'mean', 'mine', 'miss', 'much', 'must', 'nativ', 'natur', 'never', 'niec', 'none', 'noth', 'oft', 'old', 'one', 'pain', 'pale', 'part', 'peopl', 'piti', 'place', 'plagu', 'pleasur', 'poor', 'power', 'prais', 'quoth', 'remedi', 'remors', 'sad', 'sake', 'sanctifi', 'say', 'school', 'see', 'seek', 'servic', 'seventeen', 'shall', 'sir', 'sister', 'speak', 'stay', 'still', 'strong', 'suddenli', 'sweet', 'taen', 'take', 'tear', 'tell', 'ten', 'thee', 'therefor', 'thi', 'thing', 'think', 'thou', 'though', 'thousand', 'thu', 'ti', 'till', 'time', 'togeth', 'traitor', 'tree', 'upon', 'us', 'use', 'usurp', 'valu', 'villan', 'virgin', 'virtu', 'virtuou', 'weep', 'well', 'went', 'wherefor', 'wherein', 'winter', 'within', 'world', 'would', 'yea', 'year', 'yet', 'young', 'youth']\n",
      "------------\n",
      "thee                           Score: 0.41921715386147523\n",
      "brother                        Score: 0.31441286539610647\n",
      "thou                           Score: 0.31441286539610647\n",
      "noth                           Score: 0.20960857693073762\n",
      "speak                          Score: 0.20960857693073762\n",
      "yet                            Score: 0.16969620791438908\n",
      "keep                           Score: 0.15720643269805323\n",
      "know                           Score: 0.15720643269805323\n",
      "much                           Score: 0.15720643269805323\n",
      "thi                            Score: 0.15720643269805323\n",
      "begin                          Score: 0.13293095079115413\n",
      "best                           Score: 0.13293095079115413\n",
      "devic                          Score: 0.13293095079115413\n",
      "end                            Score: 0.13293095079115413\n",
      "school                         Score: 0.13293095079115413\n",
      "villan                         Score: 0.13293095079115413\n",
      "shall                          Score: 0.1272721559357918\n",
      "adam                           Score: 0.10480428846536881\n",
      "besid                          Score: 0.10480428846536881\n",
      "charl                          Score: 0.10480428846536881\n",
      "------------\n",
      "thi                            Score: 0.25323622614385766\n",
      "master                         Score: 0.20439875441159797\n",
      "love                           Score: 0.1863786817766061\n",
      "go                             Score: 0.16115032572790944\n",
      "thee                           Score: 0.16115032572790944\n",
      "daughter                       Score: 0.14599911029399856\n",
      "servic                         Score: 0.14599911029399856\n",
      "virgin                         Score: 0.14599911029399856\n",
      "like                           Score: 0.13812885062392238\n",
      "much                           Score: 0.13812885062392238\n",
      "thou                           Score: 0.13812885062392238\n",
      "come                           Score: 0.11679928823519885\n",
      "king                           Score: 0.11679928823519885\n",
      "lord                           Score: 0.11679928823519885\n",
      "make                           Score: 0.11679928823519885\n",
      "may                            Score: 0.11679928823519885\n",
      "none                           Score: 0.11679928823519885\n",
      "ti                             Score: 0.11679928823519885\n",
      "live                           Score: 0.11510737551993531\n",
      "old                            Score: 0.11510737551993531\n",
      "------------\n",
      "gainst                         Score: 0.3165111337999545\n",
      "bond                           Score: 0.15825556689997725\n",
      "dearer                         Score: 0.15825556689997725\n",
      "desir                          Score: 0.15825556689997725\n",
      "duke                           Score: 0.15825556689997725\n",
      "entreat                        Score: 0.15825556689997725\n",
      "fare                           Score: 0.15825556689997725\n",
      "forth                          Score: 0.15825556689997725\n",
      "ground                         Score: 0.15825556689997725\n",
      "hereaft                        Score: 0.15825556689997725\n",
      "knowledg                       Score: 0.15825556689997725\n",
      "ladi                           Score: 0.15825556689997725\n",
      "late                           Score: 0.15825556689997725\n",
      "malic                          Score: 0.15825556689997725\n",
      "niec                           Score: 0.15825556689997725\n",
      "piti                           Score: 0.15825556689997725\n",
      "pleasur                        Score: 0.15825556689997725\n",
      "prais                          Score: 0.15825556689997725\n",
      "remors                         Score: 0.15825556689997725\n",
      "sake                           Score: 0.15825556689997725\n",
      "------------\n",
      "sweet                          Score: 0.38008525550128647\n",
      "life                           Score: 0.25157453614922093\n",
      "adam                           Score: 0.19004262775064323\n",
      "bodi                           Score: 0.19004262775064323\n",
      "brother                        Score: 0.19004262775064323\n",
      "chang                          Score: 0.19004262775064323\n",
      "court                          Score: 0.19004262775064323\n",
      "enviou                         Score: 0.19004262775064323\n",
      "even                           Score: 0.19004262775064323\n",
      "find                           Score: 0.19004262775064323\n",
      "free                           Score: 0.19004262775064323\n",
      "head                           Score: 0.19004262775064323\n",
      "like                           Score: 0.19004262775064323\n",
      "made                           Score: 0.19004262775064323\n",
      "old                            Score: 0.19004262775064323\n",
      "say                            Score: 0.19004262775064323\n",
      "thing                          Score: 0.19004262775064323\n",
      "tree                           Score: 0.19004262775064323\n",
      "winter                         Score: 0.19004262775064323\n",
      "would                          Score: 0.19004262775064323\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Cria o objeto de vetorização para o TF-IDF Passando a função de tokenização criada para o pré-processamento\n",
    "vectorizer = TfidfVectorizer(tokenizer=stem_tokenize,max_features=200)\n",
    "\n",
    "# Gera os scores (TF-IDF) \n",
    "tfidf_result = vectorizer.fit_transform(token_dict.values())\n",
    "\n",
    "# Features do dicionário\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# Imprimindo os valores TF-IDF para cada token\n",
    "for doc in tfidf_result:\n",
    "    print('------------')\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(doc.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    for item in sorted_scores[:20]:\n",
    "        if item[1] > 0.0:\n",
    "            print (\"{0:30} Score: {1}\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação Exemplo - Sumarizador de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora veremos um exemplo de aplicação que utilizam os conceitos que foram vistos até o momento:\n",
    "\n",
    "[Exemplo - Sumarizador de texto](02.1-PLN_basico_exemplo_sumarizador.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TextBlob: Simplified Text Processing__\n",
    "   - TextBlob é uma bi biblioteca python para processamento de dados textuais. Fornece uma API para tarefas comuns de NLP.\n",
    "   (https://textblob.readthedocs.io/en/dev/)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
