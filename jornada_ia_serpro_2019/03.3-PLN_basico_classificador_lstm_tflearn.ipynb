{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN) - Básico\n",
    "\n",
    "SERPRO - SUPSD - DIVISÃO DE DESENVOLVIMENTO E SUSTENTAÇÃO DE PRODUTOS COGNITIVOS\n",
    "\n",
    "\n",
    "## Parte 3: Exemplo - Classificação de Sentenças com LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neurais Recorrentes (RNNs - Recurrent Neural Networks) são uma poderosa família de redes neurais que são amplamente utilizadas para tarefas de modelagem de sequência (por exemplo, previsão de preço de ações, modelagem de linguagem, séries temporais, etc...). A capacidade das RNNs de explorar dependências temporais de entidades em uma sequência as torna poderosas. Neste exercício, utilizaremos uma LSTM, que é uma RNN com células de memória para classificação de sentenças."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na classificação da sentenças, uma dada sentença deve ser classificada em uma classe (categoria). Usaremos um banco de dados de perguntas, onde cada pergunta é rotulada pelo assunto da pergunta. Por exemplo, a pergunta \"Quem foi Abraham Lincoln?\" será uma pergunta e seu rótulo será Pessoa. Para isso, usaremos um conjunto de dados de classificação de frases disponível em http://cogcomp.org/Data/QA/QC/; aqui você encontrará sentenças de treinamento e seus respectivos rótulos e 500 sentenças de teste.\n",
    "\n",
    "Categorias: http://cogcomp.org/Data/QA/QC/definition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos a seguinte estratégia:  \n",
    " - Extrair os dados\n",
    " - Pré-processamento dos dados\n",
    " - Construir a Rede Neural LSTM\n",
    " - Treinamento do modelo\n",
    " - Avaliar o modelo\n",
    " - Realizando Predições com dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando o Dataset\n",
    "\n",
    "Usaremos o __[dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/)__ que é composto de perguntas como entradas e seu respectivo tipo como saída. Por exemplo: quem era Abraham Lincon? E a saída ou rótulo seria Humano.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório\n",
    "dir_name = 'data/question-classif-data'\n",
    "\n",
    "# Dataset de treino\n",
    "#train_filename = 'train_1000.label'\n",
    "train_filename = 'train_5500.label'\n",
    "\n",
    "# Dataset de teste\n",
    "test_filename = 'TREC_10.label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos encontrados e verificados!\n"
     ]
    }
   ],
   "source": [
    "# Verifica se os arquivos existem\n",
    "filenames = [train_filename,test_filename]\n",
    "num_files = len(filenames)\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('Arquivos encontrados e verificados!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e visualizando os dados\n",
    "\n",
    "Abaixo nós carregamos o texto e visualizar alguns dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category Subcategory                                           Question\n",
      "0     DESC      manner  how did serfdom develop in and then leave russ...\n",
      "1     ENTY      cremat  what films featured the character popeye doyle...\n",
      "2     DESC      manner  how can i find a list of celebrities ' real na...\n",
      "3     ENTY      animal  what fowl grabs the spotlight after the chines...\n",
      "4     ABBR         exp                  what is the full form of .com ?\\n\n"
     ]
    }
   ],
   "source": [
    "# Função para leitura dos dados\n",
    "def read_data_into_pandas(filename):\n",
    "    col_names =  ['Category', 'Subcategory', 'Question']\n",
    "    data  = pd.DataFrame(columns = col_names)\n",
    "    with open(filename,'r',encoding='latin-1') as f: \n",
    "        count=1;\n",
    "        for row in f:\n",
    "            #Quebra a linha em duas no primeiro espaço (category:subcategory and question)\n",
    "            row_str = row.split(\" \",1)\n",
    "            #Separa a categoria da subcategoria\n",
    "            row_category = row_str[0].split(\":\")            \n",
    "            data.loc[len(data)] = [row_category[0],row_category[1],row_str[1].lower()]\n",
    "            count+=1\n",
    "    return data\n",
    "# Carrega os dados em DataFrames específicos para treinamento e teste\n",
    "data_train = read_data_into_pandas(os.path.join(dir_name,train_filename))\n",
    "data_test = read_data_into_pandas(os.path.join(dir_name,test_filename))\n",
    "\n",
    "# Concatena os dados de treinamento e teste. Será utilizado para construir um vocabulário único\n",
    "data = pd.concat([data_train,data_test])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento\n",
      " - Número questões na categoria HUMAN : 3669\n",
      " - Exemplos de questões na categoria HUMAN :\n",
      "5     what contemptible scoundrel stole the cork fro...\n",
      "6     what team did baseball 's st. louis browns bec...\n",
      "7                     what is the oldest profession ?\\n\n",
      "9     name the scar-faced bounty hunter of the old w...\n",
      "12                 who was the pride of the yankees ?\\n\n",
      "Name: Question, dtype: object\n",
      "\n",
      "Teste\n",
      " - Número questões na categoria HUMAN : 195\n",
      " - Exemplos de questões na categoria HUMAN :\n",
      "2                                   who was galileo ?\\n\n",
      "6     george bush purchased a small interest in whic...\n",
      "11                 what person 's head is on a dime ?\\n\n",
      "13    who was the first man to fly across the pacifi...\n",
      "17      who developed the vaccination against polio ?\\n\n",
      "Name: Question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Visualizando algumas informações sobre os dados\n",
    "# Exemplo Treinamento\n",
    "print('Treinamento')\n",
    "print(' - Número questões na categoria HUMAN : {}'.format(data_train[data_train['Category'] == 'HUM'].size))\n",
    "print(' - Exemplos de questões na categoria HUMAN :')\n",
    "print(data_train[data_train['Category'] == 'HUM']['Question'].head())\n",
    "print('')\n",
    "# Exemplo Teste\n",
    "print('Teste')\n",
    "print(' - Número questões na categoria HUMAN : {}'.format(data_test[ data_test['Category'] == 'HUM'].size))\n",
    "print(' - Exemplos de questões na categoria HUMAN :')\n",
    "print(data_test[data_test['Category'] == 'HUM']['Question'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento e Vetorização dos Dados para o Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos a classe Tokenizer do __[Keras](https://keras.io/)__ para montar nosso vocabulário. Ela permite vetorizar o texto do corpus, transformando o texto na sequência de inteiros (cada inteiro representa o índice do token em um dicionário) \n",
    "https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Vocabulário:  8761\n",
      "10 primeiras palavras do vocabulário: \n",
      "('the', 1)\n",
      "('what', 2)\n",
      "('is', 3)\n",
      "('of', 4)\n",
      "('in', 5)\n",
      "('a', 6)\n",
      "('how', 7)\n",
      "(\"'s\", 8)\n",
      "('was', 9)\n",
      "('who', 10)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Question'].values)\n",
    "print(\"Tamanho do Vocabulário: \", len(tokenizer.word_index))\n",
    "print(\"10 primeiras palavras do vocabulário: \")\n",
    "iterator = iter(tokenizer.word_index.items())\n",
    "for i in range(10):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de frase vetorizada: [7, 15, 3656, 2318, 5, 14, 440, 852, 1033]\n",
      "Exemplo de frase vetorizada: [2, 853, 639, 1, 134, 1319, 3657]\n",
      "Exemplo de frase vetorizada: [2, 3658, 3659, 1, 3660, 137, 1, 499, 46, 4, 1, 1683]\n",
      "Exemplo de frase vetorizada preenchida com zeros:  [   7   15 3656 2318    5   14  440  852 1033    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "Shape do vetor de treinamento: (5452, 33)\n",
      "Tamanho da maior frase: 33\n",
      "Categorias:  Index(['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'], dtype='object')\n",
      "Exemplo vetor de label: [0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Vetorização dos dados de treinamento\n",
    "train_X = tokenizer.texts_to_sequences(data_train['Question'].values)\n",
    "print(\"Exemplo de frase vetorizada:\", train_X[0])\n",
    "print(\"Exemplo de frase vetorizada:\", train_X[1])\n",
    "print(\"Exemplo de frase vetorizada:\", train_X[3])\n",
    "\n",
    "# Padding dos dados de treinamento: preencher com zeros para que todas as frases fiquem do mesmo tamanho\n",
    "# Necessário para alimentar a rede neural  \n",
    "train_X = pad_sequences(train_X,value=0.)\n",
    "\n",
    "print(\"Exemplo de frase vetorizada preenchida com zeros: \", train_X[0])\n",
    "print(\"Shape do vetor de treinamento:\",train_X.shape)\n",
    "print(\"Tamanho da maior frase:\",train_X.shape[1])\n",
    "\n",
    "# Vetorização dos labels para os dados de treinamento\n",
    "train_labels = pd.get_dummies(data_train['Category'])\n",
    "print(\"Categorias: \",train_labels.columns)\n",
    "train_Y = pd.get_dummies(data_train['Category']).values\n",
    "print(\"Exemplo vetor de label:\",train_Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do vetor de teste (500, 33)\n"
     ]
    }
   ],
   "source": [
    "# Vetorização dos dados de teste\n",
    "test_X = tokenizer.texts_to_sequences(data_test['Question'].values)\n",
    "\n",
    "# Temos que usar o mesmo tamanho de frases pros dados de testes, por isso usamos o tamanho dos dados de treinamento\n",
    "test_X = pad_sequences(test_X,maxlen=train_X.shape[1],value=0.)\n",
    "print(\"Shape do vetor de teste\",test_X.shape)\n",
    "test_labels = pd.get_dummies(data_test['Category']).columns\n",
    "test_Y = pd.get_dummies(data_test['Category']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção da Rede Neural (LSTM)\n",
    "http://tflearn.org/layers/recurrent/#lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma rede LSTM com a seguinte arquitetura:\n",
    "- O input será uma matriz com dimensão X (onde x é maior número de palavras de um documento)\n",
    "- Camada de entrada terá o número de neurônio igual ao número de features (words embeddings) \n",
    "- Uma camada LSTM com 128 unidades nas camadas ocultas nas células LSTM. Geralmente quanto maior melhor é desempenho. Os valores comuns são 128, 256, 512, etc.\n",
    "- Camada Dropout para evitar o overfitting\n",
    "- E uma camada totalmente conectada com 6 neurônios, referente as quantidade de categorias, função de ativação Softmax com as probabilidades para cada classe\n",
    "- Por fim uma camada de regressão com a função de perda e otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/layers/recurrent.py:69: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/layers/recurrent.py:681: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/04449579445/anaconda3/envs/pln_basico/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Reset do grafo tenforflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Construção da Rede\n",
    "network = tflearn.input_data([None, train_X.shape[1]])\n",
    "network = tflearn.embedding(network, input_dim=len(tokenizer.word_index)+1, output_dim=128)\n",
    "network = tflearn.lstm(network, 128, dropout=0.5)\n",
    "network = tflearn.fully_connected(network, 6, activation='softmax')\n",
    "network = tflearn.regression(network, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar o modelo vamos utilizar 90% dos dados para treinamento e 10% para validação. \n",
    "\n",
    "Lembrado que os hiperparâmetros do treinamento podem ser ajustados de acordo com a necessidade e com o intuito de melhorar a acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3079  | total loss: \u001b[1m\u001b[32m0.05078\u001b[0m\u001b[0m | time: 6.962s\n",
      "| Adam | epoch: 020 | loss: 0.05078 - acc: 0.9893 -- iter: 4896/4906\n",
      "Training Step: 3080  | total loss: \u001b[1m\u001b[32m0.04804\u001b[0m\u001b[0m | time: 8.005s\n",
      "| Adam | epoch: 020 | loss: 0.04804 - acc: 0.9903 | val_loss: 0.89458 - val_acc: 0.8095 -- iter: 4906/4906\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, n_epoch = 20,validation_set=0.1,batch_size=32,shuffle=True, show_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando o modelo com os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8420000014305115]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando Predições com dados de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar algumas predições dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase -> how far is it from denver to aspen \n",
      "    Label category-> NUM\n",
      "    Label predict-> NUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> what county is modesto california in \n",
      "    Label category-> LOC\n",
      "    Label predict-> LOC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> who was galileo \n",
      "    Label category-> HUM\n",
      "    Label predict-> HUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> what is an atom \n",
      "    Label category-> DESC\n",
      "    Label predict-> DESC\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Frase -> when did hawaii become a state \n",
      "    Label category-> NUM\n",
      "    Label predict-> NUM\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Predição dos dados de testes\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "# Criação do dicionário reverso (key -> Índice ; value -> Token)\n",
    "reverse_dictionary = dict(zip(tokenizer.word_index.values(), tokenizer.word_index.keys()))\n",
    "\n",
    "# Varre os dados de teste e imprime a predição\n",
    "for i in range(len(test_X[:5])):\n",
    "    frase='';\n",
    "    for j in test_X[i]:\n",
    "        if j != 0:\n",
    "            frase+=reverse_dictionary[j]+' '\n",
    "        else:\n",
    "            break\n",
    "    print('Frase ->',frase)\n",
    "    print('    Label category->',data_test['Category'][i])\n",
    "    print('    Label predict->', test_labels[np.argmax(predictions[i])])\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando predições com um dado qualquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.8468869e-03 6.1455142e-04 4.6957177e-03 3.3550777e-03 9.8799729e-01\n",
      "  4.9038429e-04]]\n",
      "Label predict-> LOC\n"
     ]
    }
   ],
   "source": [
    "test = tokenizer.texts_to_sequences([\"where is Brazil?\"])\n",
    "test = pad_sequences(test,maxlen=train_X.shape[1],value=0.)\n",
    "\n",
    "predictions = model.predict(test)\n",
    "print(predictions)\n",
    "print('Label predict->', test_labels[np.argmax(predictions[0])])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 1\n",
    "\n",
    "Modifique este jupyter para que o modelo obtenha um melhor desempenho.\n",
    "\n",
    "Primeiro crie uma função de pré-processamento dos dados para realizar as seguintes tarefas:\n",
    "\n",
    "- Remoção de Pontuação\n",
    "- Tokenização\n",
    "- Remoção de Stop words\n",
    "- Aplicação de Stemmer\n",
    "\n",
    "Dica: Essa função foi criada e utilizada no jupyter anterior.\n",
    "\n",
    "Reprocesse e veja os resultados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 2\n",
    "\n",
    "Faça modificando para tentar melhorar a acurácia do modelo:\n",
    "\n",
    "- Uso de Lemmatization ao invés de Stemming\n",
    "- Modificações dos hiperparâmetros da rede neural, tais como 'n_epoch' e 'batch_size'\n",
    "- Modifique a arquitetura da rede neural, tal como a quantidade de neurônios\n",
    "\n",
    "Qual configuração obteve melhores resultados?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "464px",
    "left": "56px",
    "top": "111.133px",
    "width": "247px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
