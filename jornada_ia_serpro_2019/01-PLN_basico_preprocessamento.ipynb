{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN) - Básico \n",
    "\n",
    "SERPRO - SUPSS - DIVISÃO DE DESENVOLVIMENTO E SUSTENTAÇÃO DE PRODUTOS COGNITIVOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 - Introdução e Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é a primeira parte do curso básico sobre PLN. O curso está divido em 3 partes:\n",
    " - Introdução e pré-processamento\n",
    " - Modelagem Estatística dos Dados\n",
    " - Estratégias de Processamento utilizando Aprendizagem Profunda\n",
    " \n",
    "Este jupyter notebook trás a parte introdutória sobre PLN. Serão abordados conceitos básicos e tarefas de baixo nível para pré processamento dos dados (textos). Essas tarefas são necessárias para a construção de um bom modelo de processamento de linguagem natural. Segue o conteúdo:\n",
    " \n",
    " - Introdução\n",
    "    - Conceito de Corpus\n",
    "    - Bibliotecas Python para PLN\n",
    " - Pré-processamento\n",
    "    - Tokenização\n",
    "    - Remoção de Stopwords\n",
    "    - Part-Of-Speech Tagging (POS Tagging)\n",
    "    - Stemming\n",
    "    - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento de linguagem natural (PLN) é uma subárea da inteligência artificial e da linguística que estuda os problemas da geração e compreensão automática de línguas humanas naturais.\n",
    "\n",
    "Atualmente, PLN é baseada em Machine Learning, que examina e utiliza padrões em dados para melhor compreender a linguagem natural. PLN é a aplicação de computadores em diferentes variações da linguagem para construir aplicações capazes de processar a linguagem natural humana e extrair informações relevantes como análise de sentimentos.\n",
    "\n",
    "PLN é uma das principais técnicas para sistemas inteligentes interagir com seres humanos, por exemplo, assistentes pessoais. Você utiliza PLN em aplicações como:\n",
    " - Corretores Ortográficos (Microsoft Word)\n",
    " - Engines de Reconhecimento de Voz (Siri, Google Voice)\n",
    " - Classificadores de Spam\n",
    " - Mecanismos de Busca (Google, Bing)\n",
    " - Sistemas de Inteligência Artificial como Assistentes Pessoais\n",
    " - Classificadores de documentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas Python para PLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem várias bibliotecas para PLN, por exemplo UIMA, OpenNLP, Genism , NLTK etc. \n",
    "Nessa primeira parte iremos trabalhar com a NLTK (Natural Language Toolkit), uma das ferramentas mais completas para se trabalhar com PLN em Python. Com a [NLTK](#section_nltk) conseguimos: \n",
    " - Separar as sentenças em um parágrafo\n",
    " - Separar as palavras dentro de cada sentença\n",
    " - Reconhecer padrões no texto e criar modelos de classificação\n",
    "\n",
    "Então, vamos instalar a biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "Collecting singledispatch (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "#Instalar a biblioteca NLTK\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, a NLTK vem com conjunto de dados: textos, gramáticas, modelos treinados, etc. A lista completa está publicada em   http://www.nltk.org/nltk_data/. \n",
    "\n",
    "A seguir iremos baixar todos esses dados, pois vamos usá-los em breve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/03662232677/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Instalando os arquivos de dados do NLTK\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceito de Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus é uma coleção de documentos de texto e Corpora é o plural de Corpus. Esse termo vem da palavra em Latim para corpo (nesse caso, o corpo de um texto). Um Corpus customizado é uma coleção de arquivos de texto organizados em um diretório.\n",
    "\n",
    "Se você for treinar seu próprio modelo como parte de um processo de classificação de texto (como análise de texto), você terá que criar seu próprio Corpus e treiná-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import WordListCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data',\n",
       " 'Data Science',\n",
       " 'Inteligência Artificial',\n",
       " 'Deep Learning',\n",
       " 'Processamento de Linguagem Natural']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um Corpus (arquivo palavras.txt no mesmo diretório do Jupyter Notebook)\n",
    "reader = WordListCorpusReader('.', ['data/palavras.txt'])\n",
    "\n",
    "#Ler as palavras do Corpus\n",
    "reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/palavras.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ler ids dos arquivos do Corpus\n",
    "reader.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Big Data\\nData Science\\nInteligência Artificial\\nDeep Learning\\nProcessamento de Linguagem Natural\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ler o texto de forma bruta\n",
    "reader.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data',\n",
       " 'Data Science',\n",
       " 'Inteligência Artificial',\n",
       " 'Deep Learning',\n",
       " 'Processamento de Linguagem Natural']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import line_tokenize\n",
    "#Separa o arquivo em linhas\n",
    "line_tokenize(reader.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "#Categorias do Corpus brown. Ver https://www.nltk.org/book/ch02.html\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "Friday\n",
      "an\n",
      "investigation\n",
      "of\n",
      "Atlanta's\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "''\n",
      "that\n",
      "any\n",
      "irregularities\n",
      "took\n",
      "place\n",
      ".\n",
      "The\n",
      "jury\n",
      "further\n",
      "said\n",
      "in\n",
      "term-end\n",
      "presentments\n",
      "that\n",
      "the\n",
      "City\n",
      "Executive\n",
      "Committee\n",
      ",\n",
      "which\n",
      "had\n",
      "over-all\n",
      "charge\n",
      "of\n",
      "the\n",
      "election\n",
      ",\n",
      "``\n",
      "deserves\n",
      "the\n",
      "praise\n",
      "and\n",
      "thanks\n",
      "of\n",
      "the\n",
      "City\n",
      "of\n",
      "Atlanta\n",
      "''\n",
      "for\n",
      "the\n",
      "manner\n",
      "in\n",
      "which\n",
      "the\n",
      "election\n",
      "was\n",
      "conducted\n",
      ".\n",
      "The\n",
      "September-October\n",
      "term\n",
      "jury\n",
      "had\n",
      "been\n",
      "charged\n",
      "by\n",
      "Fulton\n",
      "Superior\n",
      "Court\n",
      "Judge\n",
      "Durwood\n",
      "Pye\n",
      "to\n",
      "investigate\n",
      "reports\n",
      "of\n",
      "possible\n",
      "``\n",
      "irregularities\n",
      "''\n",
      "in\n",
      "the\n",
      "hard-fought\n",
      "primary\n",
      "which\n",
      "was\n",
      "won\n",
      "by\n",
      "Mayor-nominate\n",
      "Ivan\n",
      "Allen\n",
      "Jr.\n",
      ".\n",
      "``\n",
      "Only\n",
      "a\n",
      "relative\n",
      "handful\n",
      "of\n",
      "such\n",
      "reports\n",
      "was\n",
      "received\n",
      "''\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      ",\n",
      "``\n",
      "considering\n",
      "the\n",
      "widespread\n",
      "interest\n",
      "in\n",
      "the\n",
      "election\n",
      ",\n",
      "the\n",
      "number\n",
      "of\n",
      "voters\n",
      "and\n",
      "the\n",
      "size\n",
      "of\n",
      "this\n",
      "city\n",
      "''\n",
      ".\n",
      "The\n",
      "jury\n",
      "said\n",
      "it\n",
      "did\n",
      "find\n",
      "that\n",
      "many\n",
      "of\n",
      "Georgia's\n",
      "registration\n",
      "and\n",
      "election\n",
      "laws\n",
      "``\n",
      "are\n",
      "outmoded\n",
      "or\n",
      "inadequate\n",
      "and\n",
      "often\n",
      "ambiguous\n",
      "''\n",
      ".\n",
      "It\n",
      "recommended\n",
      "that\n",
      "Fulton\n",
      "legislators\n",
      "act\n",
      "``\n",
      "to\n",
      "have\n",
      "these\n",
      "laws\n",
      "studied\n",
      "and\n",
      "revised\n",
      "to\n",
      "the\n",
      "end\n",
      "of\n",
      "modernizing\n",
      "and\n",
      "improving\n",
      "them\n",
      "''\n",
      ".\n",
      "The\n",
      "grand\n",
      "jury\n",
      "commented\n",
      "on\n",
      "a\n",
      "number\n",
      "of\n",
      "other\n",
      "topics\n",
      ",\n",
      "among\n",
      "them\n",
      "the\n",
      "Atlanta\n",
      "and\n",
      "Fulton\n",
      "County\n",
      "purchasing\n",
      "departments\n",
      "which\n",
      "it\n",
      "said\n",
      "``\n",
      "are\n",
      "well\n",
      "operated\n",
      "and\n",
      "follow\n",
      "generally\n",
      "accepted\n",
      "practices\n",
      "which\n",
      "inure\n",
      "to\n",
      "the\n",
      "best\n",
      "interest\n",
      "of\n",
      "both\n",
      "governments\n",
      "''\n",
      ".\n",
      "Merger\n",
      "proposed\n",
      "However\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      "it\n",
      "believes\n",
      "``\n",
      "these\n",
      "two\n",
      "offices\n",
      "should\n",
      "be\n",
      "combined\n",
      "to\n",
      "achieve\n",
      "greater\n",
      "efficiency\n",
      "and\n",
      "reduce\n",
      "the\n",
      "cost\n",
      "of\n",
      "administration\n",
      "''\n",
      ".\n",
      "The\n",
      "City\n",
      "Purchasing\n",
      "Department\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      ",\n",
      "``\n",
      "is\n",
      "lacking\n",
      "in\n",
      "experienced\n",
      "clerical\n",
      "personnel\n",
      "as\n",
      "a\n",
      "result\n",
      "of\n",
      "city\n",
      "personnel\n",
      "policies\n",
      "''\n",
      ".\n",
      "It\n",
      "urged\n",
      "that\n",
      "the\n",
      "city\n",
      "``\n",
      "take\n",
      "steps\n",
      "to\n",
      "remedy\n",
      "''\n",
      "this\n",
      "problem\n",
      ".\n",
      "Implementation\n",
      "of\n",
      "Georgia's\n",
      "automobile\n",
      "title\n",
      "law\n",
      "was\n",
      "also\n",
      "recommended\n",
      "by\n",
      "the\n",
      "outgoing\n",
      "jury\n",
      ".\n",
      "It\n",
      "urged\n",
      "that\n",
      "the\n",
      "next\n",
      "Legislature\n",
      "``\n",
      "provide\n",
      "enabling\n",
      "funds\n",
      "and\n",
      "re-set\n",
      "the\n",
      "effective\n",
      "date\n",
      "so\n",
      "that\n",
      "an\n",
      "orderly\n",
      "implementation\n",
      "of\n",
      "the\n",
      "law\n",
      "may\n",
      "be\n",
      "effected\n",
      "''\n",
      ".\n",
      "The\n",
      "grand\n",
      "jury\n",
      "took\n",
      "a\n",
      "swipe\n",
      "at\n",
      "the\n",
      "State\n",
      "Welfare\n",
      "Department's\n",
      "handling\n",
      "of\n",
      "federal\n",
      "funds\n",
      "granted\n",
      "for\n",
      "child\n",
      "welfare\n",
      "services\n",
      "in\n",
      "foster\n",
      "homes\n",
      ".\n",
      "``\n",
      "This\n",
      "is\n",
      "one\n",
      "of\n",
      "the\n",
      "major\n",
      "items\n",
      "in\n",
      "the\n",
      "Fulton\n",
      "County\n",
      "general\n",
      "assistance\n",
      "program\n",
      "''\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      ",\n",
      "but\n",
      "the\n",
      "State\n",
      "Welfare\n",
      "Department\n",
      "``\n",
      "has\n",
      "seen\n",
      "fit\n",
      "to\n",
      "distribute\n",
      "these\n",
      "funds\n",
      "through\n",
      "the\n",
      "welfare\n",
      "departments\n",
      "of\n",
      "all\n",
      "the\n",
      "counties\n",
      "in\n",
      "the\n",
      "state\n",
      "with\n",
      "the\n",
      "exception\n",
      "of\n",
      "Fulton\n",
      "County\n",
      ",\n",
      "which\n",
      "receives\n",
      "none\n",
      "of\n",
      "this\n",
      "money\n",
      ".\n",
      "The\n",
      "jurors\n",
      "said\n",
      "they\n",
      "realize\n",
      "``\n",
      "a\n",
      "proportionate\n",
      "distribution\n",
      "of\n",
      "these\n",
      "funds\n",
      "might\n",
      "disable\n",
      "this\n",
      "program\n",
      "in\n",
      "our\n",
      "less\n",
      "populous\n",
      "counties\n",
      "''\n",
      ".\n",
      "Nevertheless\n",
      ",\n",
      "``\n",
      "we\n",
      "feel\n",
      "that\n",
      "in\n",
      "the\n",
      "future\n",
      "Fulton\n",
      "County\n",
      "should\n",
      "receive\n",
      "some\n",
      "portion\n",
      "of\n",
      "these\n",
      "available\n",
      "funds\n",
      "''\n",
      ",\n",
      "the\n",
      "jurors\n",
      "said\n",
      ".\n",
      "``\n",
      "Failure\n",
      "to\n",
      "do\n",
      "this\n",
      "will\n",
      "continue\n",
      "to\n",
      "place\n",
      "a\n",
      "disproportionate\n",
      "burden\n",
      "''\n",
      "on\n",
      "Fulton\n",
      "taxpayers\n",
      ".\n",
      "The\n",
      "jury\n",
      "also\n",
      "commented\n",
      "on\n",
      "the\n",
      "Fulton\n",
      "ordinary's\n",
      "court\n",
      "which\n",
      "has\n",
      "been\n",
      "under\n",
      "fire\n",
      "for\n",
      "its\n",
      "practices\n",
      "in\n",
      "the\n",
      "appointment\n",
      "of\n",
      "appraisers\n",
      ",\n",
      "guardians\n",
      "and\n",
      "administrators\n",
      "and\n",
      "the\n",
      "awarding\n",
      "of\n",
      "fees\n",
      "and\n",
      "compensation\n",
      ".\n",
      "Wards\n",
      "protected\n",
      "The\n",
      "jury\n",
      "said\n",
      "it\n",
      "found\n",
      "the\n",
      "court\n",
      "``\n",
      "has\n",
      "incorporated\n",
      "into\n",
      "its\n",
      "operating\n",
      "procedures\n",
      "the\n",
      "recommendations\n",
      "''\n",
      "of\n",
      "two\n",
      "previous\n",
      "grand\n",
      "juries\n",
      ",\n",
      "the\n",
      "Atlanta\n",
      "Bar\n",
      "Association\n",
      "and\n",
      "an\n",
      "interim\n",
      "citizens\n",
      "committee\n",
      ".\n",
      "``\n",
      "These\n",
      "actions\n",
      "should\n",
      "serve\n",
      "to\n",
      "protect\n",
      "in\n",
      "fact\n",
      "and\n",
      "in\n",
      "effect\n",
      "the\n",
      "court's\n",
      "wards\n",
      "from\n",
      "undue\n",
      "costs\n",
      "and\n",
      "its\n",
      "appointed\n",
      "and\n",
      "elected\n",
      "servants\n",
      "from\n",
      "unmeritorious\n",
      "criticisms\n",
      "''\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      ".\n",
      "Regarding\n",
      "Atlanta's\n",
      "new\n",
      "multi-million-dollar\n",
      "airport\n",
      ",\n",
      "the\n",
      "jury\n",
      "recommended\n",
      "``\n",
      "that\n",
      "when\n",
      "the\n",
      "new\n",
      "management\n",
      "takes\n",
      "charge\n",
      "Jan.\n",
      "1\n",
      "the\n",
      "airport\n",
      "be\n",
      "operated\n",
      "in\n",
      "a\n",
      "manner\n",
      "that\n",
      "will\n",
      "eliminate\n",
      "political\n",
      "influences\n",
      "''\n",
      ".\n",
      "The\n",
      "jury\n",
      "did\n",
      "not\n",
      "elaborate\n",
      ",\n",
      "but\n",
      "it\n",
      "added\n",
      "that\n",
      "``\n",
      "there\n",
      "should\n",
      "be\n",
      "periodic\n",
      "surveillance\n",
      "of\n",
      "the\n",
      "pricing\n",
      "practices\n",
      "of\n",
      "the\n",
      "concessionaires\n",
      "for\n",
      "the\n",
      "purpose\n",
      "of\n",
      "keeping\n",
      "the\n",
      "prices\n",
      "reasonable\n",
      "''\n",
      ".\n",
      "Ask\n",
      "jail\n",
      "deputies\n",
      "On\n",
      "other\n",
      "matters\n",
      ",\n",
      "the\n",
      "jury\n",
      "recommended\n",
      "that\n",
      ":\n",
      "(\n",
      "1\n",
      ")\n",
      "Four\n",
      "additional\n",
      "deputies\n",
      "be\n",
      "employed\n",
      "at\n",
      "the\n",
      "Fulton\n",
      "County\n",
      "Jail\n",
      "and\n",
      "``\n",
      "a\n",
      "doctor\n",
      ",\n",
      "medical\n",
      "intern\n",
      "or\n",
      "extern\n",
      "be\n",
      "employed\n",
      "for\n",
      "night\n",
      "and\n",
      "weekend\n",
      "duty\n",
      "at\n",
      "the\n",
      "jail\n",
      "''\n",
      ".\n",
      "(\n",
      "2\n",
      ")\n",
      "Fulton\n",
      "legislators\n",
      "``\n",
      "work\n",
      "with\n",
      "city\n",
      "officials\n",
      "to\n",
      "pass\n",
      "enabling\n",
      "legislation\n",
      "that\n",
      "will\n",
      "permit\n",
      "the\n",
      "establishment\n",
      "of\n",
      "a\n",
      "fair\n",
      "and\n",
      "equitable\n",
      "''\n",
      "pension\n",
      "plan\n",
      "for\n",
      "city\n",
      "employes\n",
      ".\n",
      "The\n",
      "jury\n",
      "praised\n",
      "the\n",
      "administration\n",
      "and\n",
      "operation\n",
      "of\n",
      "the\n",
      "Atlanta\n",
      "Police\n",
      "Department\n",
      ",\n",
      "the\n",
      "Fulton\n",
      "Tax\n",
      "Commissioner's\n",
      "Office\n",
      ",\n",
      "the\n",
      "Bellwood\n",
      "and\n",
      "Alpharetta\n",
      "prison\n",
      "farms\n",
      ",\n",
      "Grady\n",
      "Hospital\n",
      "and\n",
      "the\n",
      "Fulton\n",
      "Health\n",
      "Department\n",
      ".\n",
      "Mayor\n",
      "William\n",
      "B.\n",
      "Hartsfield\n",
      "filed\n",
      "suit\n",
      "for\n",
      "divorce\n",
      "from\n",
      "his\n",
      "wife\n",
      ",\n",
      "Pearl\n",
      "Williams\n",
      "Hartsfield\n",
      ",\n",
      "in\n",
      "Fulton\n",
      "Superior\n",
      "Court\n",
      "Friday\n",
      ".\n",
      "His\n",
      "petition\n",
      "charged\n",
      "mental\n",
      "cruelty\n",
      ".\n",
      "The\n",
      "couple\n",
      "was\n",
      "married\n",
      "Aug.\n",
      "2\n",
      ",\n",
      "1913\n",
      ".\n",
      "They\n",
      "have\n",
      "a\n",
      "son\n",
      ",\n",
      "William\n",
      "Berry\n",
      "Jr.\n",
      ",\n",
      "and\n",
      "a\n",
      "daughter\n",
      ",\n",
      "Mrs.\n",
      "J.\n",
      "M.\n",
      "Cheshire\n",
      "of\n",
      "Griffin\n",
      ".\n",
      "Attorneys\n",
      "for\n",
      "the\n",
      "mayor\n",
      "said\n",
      "that\n",
      "an\n",
      "amicable\n",
      "property\n",
      "settlement\n",
      "has\n",
      "been\n",
      "agreed\n",
      "upon\n",
      ".\n",
      "The\n",
      "petition\n",
      "listed\n",
      "the\n",
      "mayor's\n",
      "occupation\n",
      "as\n",
      "``\n",
      "attorney\n",
      "''\n",
      "and\n",
      "his\n",
      "age\n",
      "as\n",
      "71\n",
      ".\n",
      "It\n",
      "listed\n",
      "his\n",
      "wife's\n",
      "age\n",
      "as\n",
      "74\n",
      "and\n",
      "place\n",
      "of\n",
      "birth\n",
      "as\n",
      "Opelika\n",
      ",\n",
      "Ala.\n",
      ".\n",
      "The\n",
      "petition\n",
      "said\n",
      "that\n",
      "the\n",
      "couple\n",
      "has\n",
      "not\n",
      "lived\n",
      "together\n",
      "as\n",
      "man\n",
      "and\n",
      "wife\n",
      "for\n",
      "more\n",
      "than\n",
      "a\n",
      "year\n",
      ".\n",
      "The\n",
      "Hartsfield\n",
      "home\n",
      "is\n",
      "at\n",
      "637\n",
      "E.\n",
      "Pelham\n",
      "Rd.\n",
      "Aj\n",
      ".\n",
      "Henry\n",
      "L.\n",
      "Bowden\n",
      "was\n",
      "listed\n",
      "on\n",
      "the\n",
      "petition\n",
      "as\n",
      "the\n",
      "mayor's\n",
      "attorney\n",
      ".\n",
      "Hartsfield\n",
      "has\n",
      "been\n",
      "mayor\n",
      "of\n",
      "Atlanta\n",
      ",\n",
      "with\n",
      "exception\n",
      "of\n",
      "one\n",
      "brief\n",
      "interlude\n",
      ",\n",
      "since\n",
      "1937\n",
      ".\n",
      "His\n",
      "political\n",
      "career\n",
      "goes\n",
      "back\n",
      "to\n",
      "his\n",
      "election\n",
      "to\n",
      "city\n",
      "council\n",
      "in\n",
      "1923\n",
      ".\n",
      "The\n",
      "mayor's\n",
      "present\n",
      "term\n",
      "of\n",
      "office\n",
      "expires\n",
      "Jan.\n",
      "1\n",
      ".\n",
      "He\n",
      "will\n",
      "be\n",
      "succeeded\n",
      "by\n",
      "Ivan\n",
      "Allen\n",
      "Jr.\n",
      ",\n",
      "who\n",
      "became\n",
      "a\n",
      "candidate\n",
      "in\n",
      "the\n",
      "Sept.\n",
      "13\n",
      "primary\n",
      "after\n",
      "Mayor\n",
      "Hartsfield\n",
      "announced\n",
      "that\n",
      "he\n",
      "would\n",
      "not\n",
      "run\n",
      "for\n",
      "reelection\n",
      ".\n",
      "Georgia\n",
      "Republicans\n",
      "are\n",
      "getting\n",
      "strong\n",
      "encouragement\n",
      "to\n",
      "enter\n",
      "a\n",
      "candidate\n",
      "in\n",
      "the\n",
      "1962\n",
      "governor's\n",
      "race\n",
      ",\n",
      "a\n",
      "top\n",
      "official\n",
      "said\n",
      "Wednesday\n",
      ".\n",
      "Robert\n",
      "Snodgrass\n",
      ",\n",
      "state\n",
      "GOP\n",
      "chairman\n",
      ",\n",
      "said\n",
      "a\n",
      "meeting\n",
      "held\n",
      "Tuesday\n",
      "night\n",
      "in\n",
      "Blue\n",
      "Ridge\n",
      "brought\n",
      "enthusiastic\n",
      "responses\n",
      "from\n",
      "the\n",
      "audience\n",
      ".\n",
      "State\n",
      "Party\n",
      "Chairman\n",
      "James\n",
      "W.\n",
      "Dorsey\n",
      "added\n",
      "that\n",
      "enthusiasm\n",
      "was\n",
      "picking\n",
      "up\n",
      "for\n",
      "a\n",
      "state\n",
      "rally\n",
      "to\n",
      "be\n",
      "held\n",
      "Sept.\n",
      "8\n",
      "in\n",
      "Savannah\n",
      "at\n",
      "which\n",
      "newly\n",
      "elected\n",
      "Texas\n",
      "Sen.\n",
      "John\n",
      "Tower\n",
      "will\n",
      "be\n",
      "the\n",
      "featured\n",
      "speaker\n",
      ".\n",
      "In\n",
      "the\n",
      "Blue\n",
      "Ridge\n",
      "meeting\n",
      ",\n",
      "the\n",
      "audience\n",
      "was\n",
      "warned\n",
      "that\n",
      "entering\n",
      "a\n",
      "candidate\n",
      "for\n",
      "governor\n",
      "would\n",
      "force\n",
      "it\n",
      "to\n",
      "take\n",
      "petitions\n",
      "out\n",
      "into\n",
      "voting\n",
      "precincts\n",
      "to\n",
      "obtain\n",
      "the\n",
      "signatures\n",
      "of\n",
      "registered\n",
      "voters\n",
      ".\n",
      "Despite\n",
      "the\n",
      "warning\n",
      ",\n",
      "there\n",
      "was\n",
      "a\n",
      "unanimous\n",
      "vote\n",
      "to\n",
      "enter\n",
      "a\n",
      "candidate\n",
      ",\n",
      "according\n",
      "to\n",
      "Republicans\n",
      "who\n",
      "attended\n",
      ".\n",
      "When\n",
      "the\n",
      "crowd\n",
      "was\n",
      "asked\n",
      "whether\n",
      "it\n",
      "wanted\n",
      "to\n",
      "wait\n",
      "one\n",
      "more\n",
      "term\n",
      "to\n",
      "make\n",
      "the\n",
      "race\n",
      ",\n",
      "it\n",
      "voted\n",
      "no\n",
      "--\n",
      "and\n",
      "there\n",
      "were\n",
      "no\n",
      "dissents\n",
      ".\n",
      "The\n",
      "largest\n",
      "hurdle\n",
      "the\n",
      "Republicans\n",
      "would\n",
      "have\n",
      "to\n",
      "face\n",
      "is\n",
      "a\n",
      "state\n",
      "law\n",
      "which\n",
      "says\n",
      "that\n",
      "before\n",
      "making\n",
      "a\n",
      "first\n",
      "race\n",
      ",\n",
      "one\n",
      "of\n",
      "two\n",
      "alternative\n",
      "courses\n",
      "must\n",
      "be\n",
      "taken\n",
      ":\n",
      "1\n",
      "Five\n",
      "per\n",
      "cent\n",
      "of\n",
      "the\n",
      "voters\n",
      "in\n",
      "each\n",
      "county\n",
      "must\n",
      "sign\n",
      "petitions\n",
      "requesting\n",
      "that\n",
      "the\n",
      "Republicans\n",
      "be\n",
      "allowed\n",
      "to\n",
      "place\n",
      "names\n",
      "of\n",
      "candidates\n",
      "on\n",
      "the\n",
      "general\n",
      "election\n",
      "ballot\n",
      ",\n",
      "or\n",
      "2\n",
      "The\n",
      "Republicans\n",
      "must\n",
      "hold\n",
      "a\n",
      "primary\n",
      "under\n",
      "the\n",
      "county\n",
      "unit\n",
      "system\n",
      "--\n",
      "a\n",
      "system\n",
      "which\n",
      "the\n",
      "party\n",
      "opposes\n",
      "in\n",
      "its\n",
      "platform\n",
      ".\n",
      "Sam\n",
      "Caldwell\n",
      ",\n",
      "State\n",
      "Highway\n",
      "Department\n",
      "public\n",
      "relations\n",
      "director\n",
      ",\n",
      "resigned\n",
      "Tuesday\n",
      "to\n",
      "work\n",
      "for\n",
      "Lt.\n",
      "Gov.\n",
      "Garland\n",
      "Byrd's\n",
      "campaign\n",
      ".\n",
      "Caldwell's\n",
      "resignation\n",
      "had\n",
      "been\n",
      "expected\n",
      "for\n",
      "some\n",
      "time\n",
      ".\n",
      "He\n",
      "will\n",
      "be\n",
      "succeeded\n",
      "by\n",
      "Rob\n",
      "Ledford\n",
      "of\n",
      "Gainesville\n",
      ",\n",
      "who\n",
      "has\n",
      "been\n",
      "an\n",
      "assistant\n",
      "more\n",
      "than\n",
      "three\n",
      "years\n",
      ".\n",
      "When\n",
      "the\n",
      "gubernatorial\n",
      "campaign\n",
      "starts\n",
      ",\n",
      "Caldwell\n",
      "is\n",
      "expected\n",
      "to\n",
      "become\n",
      "a\n",
      "campaign\n",
      "coordinator\n",
      "for\n",
      "Byrd\n",
      ".\n",
      "The\n",
      "Georgia\n",
      "Legislature\n",
      "will\n",
      "wind\n",
      "up\n",
      "its\n",
      "1961\n",
      "session\n",
      "Monday\n",
      "and\n",
      "head\n",
      "for\n",
      "home\n",
      "--\n",
      "where\n",
      "some\n",
      "of\n",
      "the\n",
      "highway\n",
      "bond\n",
      "money\n",
      "it\n",
      "approved\n",
      "will\n",
      "follow\n",
      "shortly\n",
      ".\n",
      "Before\n",
      "adjournment\n",
      "Monday\n",
      "afternoon\n",
      ",\n",
      "the\n",
      "Senate\n",
      "is\n",
      "expected\n",
      "to\n",
      "approve\n",
      "a\n",
      "study\n",
      "of\n",
      "the\n",
      "number\n",
      "of\n",
      "legislators\n",
      "allotted\n",
      "to\n",
      "rural\n",
      "and\n",
      "urban\n",
      "areas\n",
      "to\n",
      "determine\n",
      "what\n",
      "adjustments\n",
      "should\n",
      "be\n",
      "made\n",
      ".\n",
      "Gov.\n",
      "Vandiver\n",
      "is\n",
      "expected\n",
      "to\n",
      "make\n",
      "the\n",
      "traditional\n",
      "visit\n",
      "to\n",
      "both\n",
      "chambers\n",
      "as\n",
      "they\n",
      "work\n",
      "toward\n",
      "adjournment\n",
      ".\n",
      "Vandiver\n",
      "likely\n",
      "will\n",
      "mention\n",
      "the\n",
      "$100\n",
      "million\n",
      "highway\n",
      "bond\n",
      "issue\n",
      "approved\n",
      "earlier\n",
      "in\n",
      "the\n",
      "session\n",
      "as\n",
      "his\n",
      "first\n",
      "priority\n",
      "item\n",
      ".\n",
      "Construction\n",
      "bonds\n",
      "Meanwhile\n",
      ",\n",
      "it\n",
      "was\n",
      "learned\n",
      "the\n",
      "State\n",
      "Highway\n",
      "Department\n",
      "is\n",
      "very\n",
      "near\n",
      "being\n",
      "ready\n",
      "to\n",
      "issue\n",
      "the\n",
      "first\n",
      "$30\n",
      "million\n",
      "worth\n",
      "of\n",
      "highway\n",
      "reconstruction\n",
      "bonds\n",
      ".\n",
      "The\n",
      "bond\n",
      "issue\n",
      "will\n",
      "go\n",
      "to\n",
      "the\n",
      "state\n",
      "courts\n",
      "for\n",
      "a\n",
      "friendly\n",
      "test\n",
      "suit\n",
      "to\n",
      "test\n",
      "the\n",
      "validity\n",
      "of\n",
      "the\n",
      "act\n",
      ",\n",
      "and\n",
      "then\n",
      "the\n",
      "sales\n",
      "will\n",
      "begin\n",
      "and\n",
      "contracts\n",
      "let\n",
      "for\n",
      "repair\n",
      "work\n",
      "on\n",
      "some\n",
      "of\n",
      "Georgia's\n",
      "most\n",
      "heavily\n",
      "traveled\n",
      "highways\n",
      ".\n",
      "A\n",
      "Highway\n",
      "Department\n",
      "source\n",
      "said\n",
      "there\n",
      "also\n",
      "is\n",
      "a\n",
      "plan\n",
      "there\n",
      "to\n",
      "issue\n",
      "some\n",
      "$3\n",
      "million\n",
      "to\n",
      "$4\n",
      "million\n",
      "worth\n",
      "of\n",
      "Rural\n",
      "Roads\n",
      "Authority\n",
      "bonds\n",
      "for\n",
      "rural\n",
      "road\n",
      "construction\n",
      "work\n",
      ".\n",
      "A\n",
      "revolving\n",
      "fund\n",
      "The\n",
      "department\n",
      "apparently\n",
      "intends\n",
      "to\n",
      "make\n",
      "the\n",
      "Rural\n",
      "Roads\n",
      "Authority\n",
      "a\n",
      "revolving\n",
      "fund\n",
      "under\n",
      "which\n",
      "new\n",
      "bonds\n",
      "would\n",
      "be\n",
      "issued\n",
      "every\n",
      "time\n",
      "a\n",
      "portion\n",
      "of\n",
      "the\n",
      "old\n",
      "ones\n",
      "are\n",
      "paid\n",
      "off\n",
      "by\n",
      "tax\n",
      "authorities\n",
      ".\n",
      "Vandiver\n",
      "opened\n",
      "his\n",
      "race\n",
      "for\n",
      "governor\n",
      "in\n",
      "1958\n",
      "with\n",
      "a\n",
      "battle\n",
      "in\n",
      "the\n",
      "Legislature\n",
      "against\n",
      "the\n",
      "issuance\n",
      "of\n",
      "$50\n",
      "million\n",
      "worth\n",
      "of\n",
      "additional\n",
      "rural\n",
      "roads\n",
      "bonds\n",
      "proposed\n",
      "by\n",
      "then\n",
      "Gov.\n",
      "Marvin\n",
      "Griffin\n",
      ".\n",
      "The\n",
      "Highway\n",
      "Department\n",
      "source\n",
      "told\n",
      "The\n",
      "Constitution\n",
      ",\n",
      "however\n",
      ",\n",
      "that\n",
      "Vandiver\n",
      "has\n",
      "not\n",
      "been\n",
      "consulted\n",
      "yet\n",
      "about\n",
      "the\n",
      "plans\n",
      "to\n",
      "issue\n",
      "the\n",
      "new\n",
      "rural\n",
      "roads\n",
      "bonds\n",
      ".\n",
      "Schley\n",
      "County\n",
      "Rep.\n",
      "B.\n",
      "D.\n",
      "Pelham\n",
      "will\n",
      "offer\n",
      "a\n",
      "resolution\n",
      "Monday\n",
      "in\n",
      "the\n",
      "House\n",
      "to\n",
      "rescind\n",
      "the\n",
      "body's\n",
      "action\n",
      "of\n",
      "Friday\n",
      "in\n",
      "voting\n",
      "itself\n",
      "a\n",
      "$10\n",
      "per\n",
      "day\n",
      "increase\n",
      "in\n",
      "expense\n",
      "allowances\n",
      ".\n",
      "Pelham\n",
      "said\n",
      "Sunday\n",
      "night\n",
      "there\n",
      "was\n",
      "research\n",
      "being\n",
      "done\n",
      "on\n",
      "whether\n",
      "the\n",
      "``\n",
      "quickie\n",
      "''\n",
      "vote\n",
      "on\n",
      "the\n",
      "increase\n",
      "can\n",
      "be\n",
      "repealed\n",
      "outright\n",
      "or\n",
      "whether\n",
      "notice\n",
      "would\n",
      "have\n",
      "to\n",
      "first\n",
      "be\n",
      "given\n",
      "that\n",
      "reconsideration\n",
      "of\n",
      "the\n",
      "action\n",
      "would\n",
      "be\n",
      "sought\n",
      ".\n",
      "While\n",
      "emphasizing\n",
      "that\n",
      "technical\n",
      "details\n",
      "were\n",
      "not\n",
      "fully\n",
      "worked\n",
      "out\n",
      ",\n",
      "Pelham\n",
      "said\n",
      "his\n",
      "resolution\n",
      "would\n",
      "seek\n",
      "to\n",
      "set\n",
      "aside\n",
      "the\n",
      "privilege\n",
      "resolution\n",
      "which\n",
      "the\n",
      "House\n",
      "voted\n",
      "through\n",
      "87-31\n",
      ".\n",
      "A\n",
      "similar\n",
      "resolution\n",
      "passed\n",
      "in\n",
      "the\n",
      "Senate\n",
      "by\n",
      "a\n",
      "vote\n",
      "of\n",
      "29-5\n",
      ".\n",
      "As\n",
      "of\n",
      "Sunday\n",
      "night\n",
      ",\n",
      "there\n",
      "was\n",
      "no\n",
      "word\n",
      "of\n",
      "a\n",
      "resolution\n",
      "being\n",
      "offered\n",
      "there\n",
      "to\n",
      "rescind\n",
      "the\n",
      "action\n",
      ".\n",
      "Pelham\n",
      "pointed\n",
      "out\n",
      "that\n",
      "Georgia\n",
      "voters\n",
      "last\n",
      "November\n",
      "rejected\n",
      "a\n",
      "constitutional\n",
      "amendment\n",
      "to\n",
      "allow\n",
      "legislators\n",
      "to\n",
      "vote\n",
      "on\n",
      "pay\n",
      "raises\n",
      "for\n",
      "future\n",
      "Legislature\n",
      "sessions\n",
      ".\n",
      "A\n",
      "veteran\n",
      "Jackson\n",
      "County\n",
      "legislator\n",
      "will\n",
      "ask\n",
      "the\n",
      "Georgia\n",
      "House\n",
      "Monday\n",
      "to\n",
      "back\n",
      "federal\n",
      "aid\n",
      "to\n",
      "education\n",
      ",\n",
      "something\n",
      "it\n",
      "has\n",
      "consistently\n",
      "opposed\n",
      "in\n",
      "the\n",
      "past\n",
      ".\n",
      "Rep.\n",
      "Mac\n",
      "Barber\n",
      "of\n",
      "Commerce\n",
      "is\n",
      "asking\n",
      "the\n",
      "House\n",
      "in\n",
      "a\n",
      "privilege\n",
      "resolution\n",
      "to\n",
      "``\n",
      "endorse\n",
      "increased\n",
      "federal\n",
      "support\n",
      "for\n",
      "public\n",
      "education\n",
      ",\n",
      "provided\n",
      "that\n",
      "such\n",
      "funds\n",
      "be\n",
      "received\n",
      "and\n",
      "expended\n",
      "''\n",
      "as\n",
      "state\n",
      "funds\n",
      ".\n",
      "Barber\n",
      ",\n",
      "who\n",
      "is\n",
      "in\n",
      "his\n",
      "13th\n",
      "year\n",
      "as\n",
      "a\n",
      "legislator\n",
      ",\n",
      "said\n",
      "there\n",
      "``\n",
      "are\n",
      "some\n",
      "members\n",
      "of\n",
      "our\n",
      "congressional\n",
      "delegation\n",
      "in\n",
      "Washington\n",
      "who\n",
      "would\n",
      "like\n",
      "to\n",
      "see\n",
      "it\n",
      "(\n",
      "the\n",
      "resolution\n",
      ")\n",
      "passed\n",
      "''\n",
      ".\n",
      "But\n",
      "he\n",
      "added\n",
      "that\n",
      "none\n",
      "of\n",
      "Georgia's\n",
      "congressmen\n",
      "specifically\n",
      "asked\n",
      "him\n",
      "to\n",
      "offer\n",
      "the\n",
      "resolution\n",
      ".\n",
      "The\n",
      "resolution\n",
      ",\n",
      "which\n",
      "Barber\n",
      "tossed\n",
      "into\n",
      "the\n",
      "House\n",
      "hopper\n",
      "Friday\n",
      ",\n",
      "will\n",
      "be\n",
      "formally\n",
      "read\n",
      "Monday\n",
      ".\n",
      "It\n",
      "says\n",
      "that\n",
      "``\n",
      "in\n",
      "the\n",
      "event\n",
      "Congress\n",
      "does\n",
      "provide\n",
      "this\n",
      "increase\n",
      "in\n",
      "federal\n",
      "funds\n",
      "''\n",
      ",\n",
      "the\n",
      "State\n",
      "Board\n",
      "of\n",
      "Education\n",
      "should\n",
      "be\n",
      "directed\n",
      "to\n",
      "``\n",
      "give\n",
      "priority\n",
      "''\n",
      "to\n",
      "teacher\n",
      "pay\n",
      "raises\n",
      ".\n",
      "Colquitt\n",
      "--\n",
      "After\n",
      "a\n",
      "long\n",
      ",\n",
      "hot\n",
      "controversy\n",
      ",\n",
      "Miller\n",
      "County\n",
      "has\n",
      "a\n",
      "new\n",
      "school\n",
      "superintendent\n",
      ",\n",
      "elected\n",
      ",\n",
      "as\n",
      "a\n",
      "policeman\n",
      "put\n",
      "it\n",
      ",\n",
      "in\n",
      "the\n",
      "``\n",
      "coolest\n",
      "election\n",
      "I\n",
      "ever\n",
      "saw\n",
      "in\n",
      "this\n",
      "county\n",
      "''\n",
      ".\n",
      "The\n",
      "new\n",
      "school\n",
      "superintendent\n",
      "is\n",
      "Harry\n",
      "Davis\n",
      ",\n",
      "a\n",
      "veteran\n",
      "agriculture\n",
      "teacher\n",
      ",\n",
      "who\n",
      "defeated\n",
      "Felix\n",
      "Bush\n",
      ",\n",
      "a\n",
      "school\n",
      "principal\n",
      "and\n",
      "chairman\n",
      "of\n",
      "the\n",
      "Miller\n",
      "County\n",
      "Democratic\n",
      "Executive\n"
     ]
    }
   ],
   "source": [
    "palavras = brown.words(categories='news')\n",
    "for palavra in palavras[:2000]:\n",
    "    print(palavra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Processamento de Linguagem Natural possuem tarefas básicas que tem por objetivos pegar o dataset/corpus aplicar algum tipo de análise e realizar transformações. Por exemplo, limpeza, normalização, tokenização, stemming, Lemmatization etc. Essas tarefas são necessárias para preparar os dados e gerar um modelo com a melhor acurácia possível. \n",
    "\n",
    "A seguir vamos mostrar as seguintes tarefas: tokenização, remoção de stopwords, Part-Of-Speech Tagging (POS Tagging) e Stemming Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização (_tokenization_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processo de dividir uma string em listas de pedaços ou _tokens_. Um _token_ é uma parte inteira. Por exemplos: uma palavra é um token em uma sentença. Uma sentença é um token em um parágrafo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dividindo um parágrafo em frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragrafo = \"Você está começando o treinamento sobre PLN - Processamento de Texto em Linguagem Natural. Seja bem vindo! Obrigado por participar.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você está começando o treinamento sobre PLN - Processamento de Texto em Linguagem Natural.\n",
      "Seja bem vindo!\n",
      "Obrigado por participar.\n"
     ]
    }
   ],
   "source": [
    "# Dividindo o parágrafo em frases\n",
    "sentencas = sent_tokenize(paragrafo)\n",
    "for sentenca in sentencas:\n",
    "    print(sentenca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Você está começando o treinamento sobre PLN - Processamento de Texto em Linguagem Natural.',\n",
       " 'Seja bem vindo!',\n",
       " 'Obrigado por participar.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando dados do pacote NLTK podemos selecionar um dicionário específico para cada linguagem\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/portuguese.pickle')\n",
    "tokenizer.tokenize(paragrafo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-508c7dd25366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dados em espanhol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspanish_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/PY3/spanish.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspanish_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hola amigo. Estoy bien.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Dados em espanhol\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dividindo uma frase em palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após dividir o texto em sentenças, agora vamos dividí-las em palavras. Separar as sentenças em palavras é o principal objetivo para a partir daí aplicar técnicas de PLN. Existem diversas funções prontas na NLTK que podem ser utilizadas de acordo com a necessidade. A seguir mostraremos algumas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Curso',\n",
       " 'Básico',\n",
       " 'de',\n",
       " 'Processamento',\n",
       " \"d'água\",\n",
       " 'de',\n",
       " 'Linguagem',\n",
       " 'Natural',\n",
       " '?',\n",
       " 'Voltamos',\n",
       " 'agora.',\n",
       " 'Curso',\n",
       " 'no',\n",
       " 'Serpro.',\n",
       " 'teste']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Funções para separar uma frase em uma lista de palavras\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = 'Curso Básico de Processamento d\\'água de Linguagem Natural? Voltamos agora.\\nCurso no Serpro.\\r\\nteste'\n",
    "word_tokenize(text, language='portuguese', preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Processamento',\n",
       " 'de',\n",
       " 'Linguagem',\n",
       " 'Natural.',\n",
       " 'Curso',\n",
       " 'de',\n",
       " 'PLN',\n",
       " 'no',\n",
       " 'Serpro',\n",
       " '!']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('Processamento de Linguagem Natural. Curso de PLN no Serpro!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can', \"'\", 't', 'is', 'a', 'contraction', '.', 'd', \"'\", 'água']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização por pontuação\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Can't is a contraction. d'água\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction', \"d'água\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização por expressões regulares\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction. d'água\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps = True)\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treinando um Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, podemos realizar um treinamento do Tokenizer passando para ele um conjunto de dados com um padrão específico e depois que estiver treinado apresentar um novo conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iremos usar a PunktSentenceTokenizer para treinar um modelo\n",
    "#O PunktSentenceTokenizer utiliza um algoritmo não supervisionado para identificar padrões no texto\n",
    "#Help para o PunktSentenceTokenizer\n",
    "?PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando um Corpus que vem no pacote de dados NLTK.\n",
    "texto = webtext.raw('overheard.txt')\n",
    "#Passando o texto para o PunktSentenceTokenizer. O algoritmo será treinado com base nos padrões do texto. \n",
    "sent_tokenizer = PunktSentenceTokenizer(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White guy: So, do you have any plans for this evening?',\n",
       " 'Asian girl: Yeah, being angry!',\n",
       " 'White guy: Oh, that sounds good.',\n",
       " 'Guy #1: So this Jack guy is basically the luckiest man in the world.',\n",
       " \"Guy #2: Why, because he's survived like 5 attempts on his life and it's not even noon?\",\n",
       " 'Guy #1: No; he could totally nail those two chicks.',\n",
       " 'Dad: Could you tell me where the auditorium is?',\n",
       " \"Security guy: It's on the second floor.\",\n",
       " \"Dad: Wait, you mean it's actually in the building?\",\n",
       " \"Girl: But, I mean, it's not like I ever plan on giving birth.\",\n",
       " \"Guy: Well, if your mother gave birth, it's like your chances are good that you'll give birth too.\",\n",
       " 'Girl: ...Uh, dude, mother gave birth.',\n",
       " 'Guy: Absolutely.',\n",
       " \"Guy #1: I don't mind getting old; I love getting old.\",\n",
       " \"Guy #2: Yeah, just as long as you don't get pregnant.\",\n",
       " 'Hobo: Can you spare any change?',\n",
       " 'Man: Sorry, no.',\n",
       " 'Hobo: Who the hell you saying no to?',\n",
       " \"I wasn't asking you anyway, asshole!\",\n",
       " 'Hobo: Excuse me, this is a picture of my daughter Sofiya, she was in a fire recently and now she is brain damage can you spare some change so that we can give her a proper funeral?',\n",
       " 'Anything will help, even a penny.',\n",
       " 'Man: Wait a minute!',\n",
       " 'Is this the same daughter that was in a fire last summer?',\n",
       " \"You mean to tell me you haven't buried her yet?\",\n",
       " 'Guy: How old are you?',\n",
       " 'Hipster girl: You know, I never answer that question.',\n",
       " \"Because to me, it's about how mature you are, you know?\",\n",
       " 'I mean, a fourteen year old could be more mature than a twenty-five year old, right?',\n",
       " \"I'm sorry, I just never answer that question.\",\n",
       " \"Guy: But, uh, you're older than eighteen, right?\",\n",
       " 'Hipster girl: Oh, yeah.',\n",
       " 'Queer #1: What should I get?',\n",
       " \"I'll have a falafel.\",\n",
       " 'Ha, ha!',\n",
       " \"Queer #2: I'll have one of these lamb slices.\",\n",
       " 'Pizza guy: Eggplant.',\n",
       " 'Queer #2: No, one of these.',\n",
       " \"Pizza guy: That's eggplant.\",\n",
       " 'Queer #2: Oh...Well, it looked like lamb.',\n",
       " \"Queer #1: I'm gonna throw caution to the wind and get a cheese slice.\",\n",
       " \"Queer #3: That's not lamb.\",\n",
       " 'Maybe you should ask for a duck confit slice.',\n",
       " 'Man: Is this kosher?',\n",
       " \"Vendor guy: Um...there's kosher salt in the bacon.\",\n",
       " 'Man #1: Yo!',\n",
       " 'You just picked your nose!',\n",
       " \"You're not gonna wash your hands first?\",\n",
       " 'Man #2: Why?',\n",
       " \"It's not like I picked your nose.\",\n",
       " \"Guy #1: You're the first person I've seen that washed their hands before going to the toilet.\",\n",
       " 'Guy #2: My godfather was a doctor and he got it into my head at a young enough age that I had to always do it.',\n",
       " 'Guy #1: Really?',\n",
       " 'He must have been a bit of a strange doctor.',\n",
       " 'Guy #2: Yeah...He specialized in infectious diseases.',\n",
       " \"Girl: Why do I have to die, why can't you die?\",\n",
       " 'Guy: .',\n",
       " \"Girl: Well...that's not fair.\",\n",
       " 'Old man: You are the most beautiful woman in the world.',\n",
       " 'Girl: Aw, thanks!',\n",
       " 'Guy: She has crabs !',\n",
       " 'Guy #1: What can I say?',\n",
       " \"I'm a sucker for orphan stories.\",\n",
       " 'Guy #2: Or something.',\n",
       " \"Guy #1: Think about it: I loved Lemony Snicket, Party of Five , Diff'rent Strokes , Star Wars .\",\n",
       " 'Guy #2: Yeah.',\n",
       " 'Wait.',\n",
       " \"Luke wasn't an orphan.\",\n",
       " 'Guy #1: Well, he sort of was, spiritually.',\n",
       " 'Guy: I could be fired for 8 counts of sexual harassment last night.',\n",
       " \"Girl: Yeah, and that's just on me alone.\",\n",
       " 'Guy: Oh man, really?',\n",
       " \"Girl: Yeah, but you bought me a drink, so it's okay.\",\n",
       " 'Asian girl: I really like sleep sex.',\n",
       " \"White girl #1: What's that?\",\n",
       " \"Asian girl: You know, when you're sleeping and you wake up and you're having sex.\",\n",
       " 'Sleep sex.',\n",
       " 'White girl #2: You mean getting raped?',\n",
       " 'Little boy: Dad, will you remember me tomorrow?',\n",
       " 'Dad: Of course.',\n",
       " 'Little boy: Will you remember me next week?',\n",
       " 'Dad: Yes.',\n",
       " 'Little boy: Next month?',\n",
       " 'Dad: Yup.',\n",
       " 'Little boy: Next year?',\n",
       " 'Two years?',\n",
       " 'Three years?',\n",
       " 'Dad.',\n",
       " 'Yes, yes and yes.',\n",
       " 'Little boy: ...Knock, knock.',\n",
       " \"Dad: Who's there?\",\n",
       " 'Little boy: Aw, man!',\n",
       " 'You forgot me already!',\n",
       " 'Guy #1: Bitches are all emotional, guys use their head.',\n",
       " 'That is why I call emotional guys \"bitches.\"',\n",
       " 'Guy #2: Word.',\n",
       " 'Guy #1: But bitches are crazy, they will call the cops on you now.',\n",
       " 'They will slap themselves in the face and when the cops show up they will point at you.',\n",
       " \"Guy #2: Fo' sho'.\",\n",
       " \"Guy #1: That is why I ain't got no kids.\",\n",
       " \"I don't want a bunch of my seed running around and people calling me a scumbag because I don't take care of my kids.\",\n",
       " 'Bitch will turn on you for that child support.',\n",
       " \"Guys #2: I know my girl ain't gonna be doing that because she know I'm only making minimum wage.\",\n",
       " 'Woman: I remember kindergarten.',\n",
       " 'I got lots of candy and everyone wanted to play with me and I used to wet the bed a lot.',\n",
       " 'Man: You used to what?',\n",
       " 'Woman: I used to wet the bed.',\n",
       " 'So they decided to move my bed farther away from the bathroom.',\n",
       " \"Tween girl #1: I'm gonna call that number 1-800-DIVORCE.\",\n",
       " 'I want to divorce my parents.',\n",
       " \"Tween boy: You can't divorce your parents, stupid.\",\n",
       " 'Can you marry your parents?',\n",
       " 'No!',\n",
       " \"Tween girl #1: Technically, technically you can but that's just sick.\",\n",
       " \"Tween girl #2: You're not really divorcing your parents.\",\n",
       " \"It's more like they giving up they rights.\",\n",
       " \"Tween girl #1: Look, I call it divorcing your parents because that's what they called it on The Simpsons so that's why I say it.\",\n",
       " 'Queer #1: That used to be a fun place.',\n",
       " 'Remember that backroom?',\n",
       " 'That was a fun backroom!',\n",
       " 'Queer #2: Are you nuts?',\n",
       " 'It was a nasty firetrap full of gropey old trolls, people came on me without my permission , and I had my wallet stolen!',\n",
       " 'Queer #1: True.',\n",
       " 'Woman: Hi!...Oh, I thought you were someone else.',\n",
       " 'Man: I am.',\n",
       " 'Indian mom: Eat your chicken.',\n",
       " 'Drunk Irishman: Ach, what nice bebbies.',\n",
       " 'I have a child too.',\n",
       " \"Indian mom: That's nice.\",\n",
       " 'Drunk Irishman: Just the one, though.',\n",
       " \"The doctor told me wife, that's it.\",\n",
       " 'No more chidren for you.',\n",
       " 'Indian mom: I see.',\n",
       " 'Little girl: Grandma, who is that man?',\n",
       " \"Indian mom: I'm your mother, not your grandmother.\",\n",
       " 'Drunk Irishman: Her boss made her lift a 500 pound piano all by her self.',\n",
       " 'A tiny woman!',\n",
       " 'A 500 pound piano!',\n",
       " 'Indian mom: I see.',\n",
       " 'Drunk Irishman: So no more children.',\n",
       " 'And me one of nine, you know.',\n",
       " 'Including the one deadborn one.',\n",
       " \"Guy #1: Why did you cut your knish like you're an Asian person?\",\n",
       " \"Girl: Because I'm a !\",\n",
       " 'Guy #2: Anna, you are one crazy cookie.',\n",
       " 'Man: I have never seen so many chinks in one Starbucks in all my life.',\n",
       " \"Guy: This is, like, the third time they've made me feel stupid in public.\",\n",
       " 'God, I hate Chinese people!',\n",
       " 'Girl: I think its a complete failure as an expression of ideology, but it is aesthetically pleasing.',\n",
       " 'Guy: What, circumcision?',\n",
       " 'Girl: No, Futurism.',\n",
       " 'Guy #1: So I had ex sex last night.',\n",
       " 'Guy #2: How was it?',\n",
       " \"Guy #1: Amazing as always, but now I'm fucked cause she's gonna start calling me again.\",\n",
       " 'Guy #2: Imagine that, you sleep with someone and then they call you...Crazy.',\n",
       " \"Tween girl: Why isn't she smiling?\",\n",
       " 'Mom: Honey, the French gave her to us.',\n",
       " 'Smiling is an American thing.',\n",
       " 'Dad: Listen to your mother, she knows a lot.',\n",
       " 'Girl: Whatever, tell your brother to go back to prison, learn how to read, and then he can talk to me.',\n",
       " 'Guy: But you egged his car!',\n",
       " 'You egged his fucking car!',\n",
       " \"What kind of bitch eggs someone's car?\",\n",
       " \"Drunk guy: You're the best lookin' thing in here.\",\n",
       " \"Waitress: Number 1, I'm not a thing.\",\n",
       " 'Number 2, thanks!',\n",
       " \"Pizza guy: You can't get a slice, man.\",\n",
       " 'Your money is fake\\nDude: My money is real.',\n",
       " 'I get it at the bank.',\n",
       " 'Straight from the white man.',\n",
       " 'HS girl #1: I saw on TV last night they were saying how you can bring people back from the dead.',\n",
       " 'HS girl #2: Uh, how dead?',\n",
       " \"HS girl #1: Like Hitler...\\nHS girl #3: That's crazy.\",\n",
       " 'I read the Bible.',\n",
       " 'You can bring people back from the dead.',\n",
       " 'HS girl #2: , they brought Jesus back from the dead.',\n",
       " \"Chick #1: Girl, my feets is killin' me.\",\n",
       " \"I's goin' home, gettin' in bed, put on the TV.\",\n",
       " \"I'm done.\",\n",
       " \"Chick #2: Change your name to Saran, 'cause it's a wrap.\",\n",
       " 'Girl #1: Your room always smells so good, like coconuts and coffee.',\n",
       " 'It reminds me of the Caribbean.',\n",
       " 'What kind of candles do you have?',\n",
       " \"Girl #2: That's not from a candle.\",\n",
       " \"The coconut is from the foot cream I use so my feet don't stink and the coffee smell is there because I spilled some on my carpet and never cleaned it up.\",\n",
       " 'Girl #1: Oh...where can I get the foot cream, then?',\n",
       " 'Bag lady: Could someone spare some change?',\n",
       " 'My welfare was denied.',\n",
       " 'Crazy lady: Yeah, yeah, they denied mine the first time too.',\n",
       " 'Get over it.',\n",
       " \"Man: Does anyone know how to get to--\\nCrazy lady: They aren't listening, they aren't going to talk to you.\",\n",
       " \"They can't talk.\",\n",
       " 'They are all mute.',\n",
       " \"Asian chick: So that's it, then?\",\n",
       " 'Asian guy: Yep.',\n",
       " \"Asian chick: We're breaking up, then?\",\n",
       " 'Asian guy: Yep.',\n",
       " 'A few minutes pass.',\n",
       " \"Asian chick: Hey, you'd look good with that girl up there in the pink checked coat.\",\n",
       " 'Tourist lady #1: Is this it?',\n",
       " 'Tourist lady #2: I think this is it.',\n",
       " \"Tourist lady #1: You'd think they'd have signs or something.\",\n",
       " 'Tourist lady #2: Yeah, this must be it, though.',\n",
       " 'Fratboy: This one is awesome.',\n",
       " \"Chicks love it....or, um, dudes, if you're into that kind of thing.\",\n",
       " 'Suit: I am, thank you.',\n",
       " 'Fratboy: Awesome.',\n",
       " 'Good save, huh?',\n",
       " 'Suit: Yeah.',\n",
       " 'Thanks for the recommendation.',\n",
       " 'Fratboy: Anytime.',\n",
       " 'Hobo #1: What flavors you got?',\n",
       " 'Clerk guy: We have regular, orange, raspberry, and vanilla.',\n",
       " \"Hobo #1: We'll take vanilla.\",\n",
       " 'Hobo #2: Vanilla?',\n",
       " 'What are you, gay?',\n",
       " \"Store girl: Here's your receipt and have a happy holiday!\",\n",
       " 'Store guy: The holidays are over.',\n",
       " \"Store girl: Valentine's Day is coming up.\",\n",
       " \"Thug guy: Yo, happy New Year's, man.\",\n",
       " \"Janitor guy: New Year's is over, yo.\",\n",
       " \"Thug guy: Happy Mother's Day!\",\n",
       " 'God Squad man: Jesus saves!',\n",
       " 'Books $1 only.',\n",
       " 'Guy: Fuck Jesus.',\n",
       " 'God Squad man: Fuck your mother...and your father.',\n",
       " 'Jesus saves people.',\n",
       " 'Books, $1.',\n",
       " \"Guy #1: We goin' uptown or downtown?\",\n",
       " \"Guy #2: Nigga, we goin' .\",\n",
       " 'Girl: I went to Boston this weekend.',\n",
       " 'Mostly just to avoid the L train.',\n",
       " \"Guy: I can't believe I was cockblocked by the L train.\",\n",
       " 'Chick: Are you hungry?',\n",
       " 'I have some leftover vietnamese food you can have.',\n",
       " 'Hobo: Well, what is it?',\n",
       " \"I'm religious.\",\n",
       " \"I don't eat pork.\",\n",
       " \"Chick: There's no pork.\",\n",
       " \"It's just vegetarian noodles.\",\n",
       " 'Hobo: Noodles?',\n",
       " \"Nah, I'm trying to cut back on carbs.\",\n",
       " 'Hipster guy: I need a woman to love me so I can alienate her.',\n",
       " \"The love part, that's where it gets difficult.\",\n",
       " 'Girl #1: Oh, come on.',\n",
       " \"It's so easy to find a needy bitch.\",\n",
       " 'Girl #2: Have you tried AA?',\n",
       " \"Girl #1: I haven't seen our homeless guy lately.\",\n",
       " 'Girl #2: We have a homeless guy?',\n",
       " 'Girl #1: Yeah, the guy who lives on that mattress under our building.',\n",
       " \"Girl #2: Oh yeah...I hope he's okay, I haven't seen him all week.\",\n",
       " \"Girl #1: You know you're a New Yorker when you worry about where your homeless guy is.\",\n",
       " \"Drunk guy: If you come in and dance with me, I'll buy you a drink.\",\n",
       " 'Sober girl: No, thanks.',\n",
       " \"There's a five dollar cover.\",\n",
       " \"Drunk guy: If you come in and dance with me, I'll give you five dollars.\",\n",
       " \"Sober girl: I don't dance.\",\n",
       " \"Drunk guy: I think you're hot.\",\n",
       " \"Sober girl: I'm sorry...Watch out.\",\n",
       " \"You're setting yourself on fire.\",\n",
       " \"Drunk guy: I'm on fire for you, baby!\",\n",
       " 'Tourist guy: Why do the buildings in New York have water tanks on the\\nroofs?',\n",
       " \"Cop guy: I don't know...maybe they knock it over if the building\\ngoes up in flames.\",\n",
       " 'Guy #1: Man, I really need to listen to more rap.',\n",
       " \"Guy #2: Dude, you don't need to listen; you need to live it.\",\n",
       " \"HS girl: I didn't want to listen to my dad explain sex to my mom.\",\n",
       " 'HS boy: Why would your dad explain sex to your mom?',\n",
       " 'HS girl: Because my brother asked what the song, \"Come my lady, come, come my lady\" meant.',\n",
       " \"And mom didn't know.\",\n",
       " 'HS boy: What does that song have to do with sex?',\n",
       " \"HS girl: You don't ?\",\n",
       " \"You're the only person in the world who doesn't know.\",\n",
       " \"HS boy: Your brother didn't know.\",\n",
       " 'HS girl: My brother is 7.',\n",
       " \"Guy: Yeah, that's the first thing I learned when I moved here: don't eat street meat, it's probably pigeon or something.\",\n",
       " 'Girl #1: Yeah, I wish I could get my husband to stop eating it.',\n",
       " \"Girl #2: I don't care what kind of meat it is as long as it's in my mouth.\",\n",
       " \"Girl #1: That's my sister; she's looking for a hook-up.\",\n",
       " \"Girl #2: No, I'm not!\",\n",
       " \"Guy: I'm married.\",\n",
       " 'A little tries to stuff his baseball cap in his pants.',\n",
       " 'Mother: That cap belongs on your head!',\n",
       " 'Little boy: It on my head.',\n",
       " \"Teen girl #1: Yeah, I ran away once, 'cause like, my parents were making me study for a science test.\",\n",
       " \"Teen girl #2: Oh my god, you don't have any problems.\",\n",
       " 'My parents are making me get a job!',\n",
       " 'Teen boy: Not having money to buy food is a .',\n",
       " 'Not having an is a problem.',\n",
       " \"Drunk guy: Don't you with !\",\n",
       " 'Sober woman: ?',\n",
       " 'Drunk guy: You sleep with a different guy every night!',\n",
       " 'Sober woman: I do not.',\n",
       " 'What are you talking about?',\n",
       " 'Drunk guy: You .',\n",
       " 'You want to .',\n",
       " 'Sober woman: This is ridiculous.',\n",
       " \"I've had enough.\",\n",
       " \"I don't have to take this anymore.\",\n",
       " 'Goodbye!',\n",
       " 'She leaves.',\n",
       " 'He turns to the next table.',\n",
       " 'Drunk guy: Yeah, did you see that girl who just left?',\n",
       " 'I just dumped her.',\n",
       " 'Can I buy you two a drink?',\n",
       " 'Dude: Do you guys sell bling?',\n",
       " 'Store guy: All the way in the back, under the skeleton pimp.',\n",
       " \"Guy: I'm like your gay boyfriend.\",\n",
       " 'Girl: Kind of.',\n",
       " 'Guy: ...only without the gay sex part, of course.',\n",
       " 'Girl: Yes, and without the sense of style.',\n",
       " \"Guy: I don't know if I'd say that...\",\n",
       " \"Girl: See, you're just very emotional.\",\n",
       " \"But you should really work on the style, 'cause it's the best thing about the gay.\",\n",
       " 'Woman: Move, motherfuckers, move !',\n",
       " 'Yuppie guy: Wait for the next one, this is too packed.',\n",
       " 'Woman: Bitch, I have to be on this train!',\n",
       " \"Yuppie guy: This isn't the train to heaven, you know.\",\n",
       " \"It's, like, going to Queens.\",\n",
       " 'Girl: One pack of Parliament Lights.',\n",
       " 'Vendor guy: You 18?',\n",
       " \"Girl: Well, I'm actually 22.\",\n",
       " \"Crazy guy: Girly, you look like you are 10...but it's okay.\",\n",
       " 'I like that.',\n",
       " 'American girl: Yeah, the subway runs express out of Astoria and local into Astoria.',\n",
       " \"It wouldn't make sense any other way.\",\n",
       " 'See all the people on the train?',\n",
       " 'German guy: Why would it only run express one way?',\n",
       " \"American girl: You're not from here, I don't expect you to understand.\",\n",
       " 'Girl #1: You wanna hear something, like, totally outrageous?',\n",
       " 'Girl #2: Always!',\n",
       " 'Girl #1: ...One side of my hair grows faster than the other.',\n",
       " 'Girl #2: No way!',\n",
       " \"That's weird.\",\n",
       " 'Girl #1: Yeah!',\n",
       " 'Like, the right side grows faster than the left side, and I have to show up at a salon and have them cut off the right side but not the left.',\n",
       " \"Girl #2: ...You're weird.\",\n",
       " 'Guy: I keep getting screwed over on my haircuts!',\n",
       " \"Last time they left it way too shaggy in the back, and this time it's much too short.\",\n",
       " 'I need to find a stylist I can stick with.>\\nChick: Hmm, so your hair is like shlong or something.',\n",
       " 'Guy: Huh?',\n",
       " 'Chick: Shlong.',\n",
       " 'Like short-long.',\n",
       " 'Shlong.',\n",
       " 'Guy: Um, well, \"shlong\" means \"penis\" in Yiddish.',\n",
       " 'So, uh, ha, ha, no.',\n",
       " \"Chick: Oh, I didn't know that!\",\n",
       " \"Guy: I sure hope I don't have a penis growing out the back of my head.\",\n",
       " \"Girl #1: So my brother's bar mitzvah is this Saturday and he asked me to cut off my hawk for it so I cut it off, but on Sunday I am going to dye it black with orange tips.\",\n",
       " \"Girl #2: Very Halloween, a bit late\\nGirl #1: Yeah, but it'll look good.\",\n",
       " 'Hipster guy #1: No way!',\n",
       " 'I thought I passed you the other day, but thought, \"Nah, it couldn\\'t be him, he wouldn\\'t grow his hair that long.\"',\n",
       " 'Hipster guy #2: Yeah.',\n",
       " 'No.',\n",
       " 'I grew it out, man.',\n",
       " 'Hipster guy #1: You look like Axl Rose!',\n",
       " \"Hipster guy #2: It's more of a Southern rock thing really.\",\n",
       " 'Like a My Morning Jacket look.',\n",
       " \"Teen girl #1: Let's go in this store.\",\n",
       " \"Teen girl #2: I don't know...it looks kind of sketch.\",\n",
       " \"And there's a weird guy staring at us.\",\n",
       " 'Teen girl #1: Come on!',\n",
       " 'What have we got to lose?',\n",
       " 'Teen girl #3: Um, our virginity?',\n",
       " 'Woman: Is that Perhaps?',\n",
       " 'Man: What?',\n",
       " 'Woman: Is that Perhaps?',\n",
       " 'Man: \" \"?',\n",
       " \"Woman: Yeah, there's this dog called Perhaps that hangs around here.\",\n",
       " 'Yours looks just like it.',\n",
       " 'Teen boy: Good afternoon ladies and gentlemen, my name is Dwayne and I am in a program that keeps me and other kids like me off the street.',\n",
       " 'They have us sell candy for $1.',\n",
       " 'All profits go directly to the youth program that keeps us off the street.',\n",
       " 'If you would like to purchase Snickers, Twix or Starburst, they are only $1.',\n",
       " 'Hobo: Hey, lady!',\n",
       " 'Hey, fat lady!',\n",
       " 'Buy some candy.',\n",
       " \"You like candy, don't you?\",\n",
       " 'Fat lady!',\n",
       " 'Get some candy!',\n",
       " 'Get some!',\n",
       " 'Lady: Asshole.',\n",
       " 'Girl: Do you smell that?',\n",
       " 'Smells like straight up pussy in this bitch.',\n",
       " \"Guy: I wouldn't know.\",\n",
       " \"Girl: What do you mean you wouldn't know?\",\n",
       " \"It's pussy.\",\n",
       " \"Guy: I wouldn't know.\",\n",
       " \"I'm gay.\",\n",
       " 'Girl: Damn, son.',\n",
       " 'So what does dick smell like?',\n",
       " \"Guy: Wouldn't you know?\",\n",
       " 'I mean when you get on your knees?',\n",
       " \"Queer on cell: So I saw this store that was going out of business...Yeah, so I got a faith and three hopes...Or was it two faiths and three hopes?...Ha, yeah, there wasn't any love or anything.\",\n",
       " 'I bet I could sell a faith to Madonna for a hundred million dollars.',\n",
       " 'Like, \"Here you go, this is the most religious thing ever.',\n",
       " 'More than you....bitch.',\n",
       " '\"...Ha, ha, yeah.',\n",
       " '\"It has holy waters from all over the world.',\n",
       " 'The Pope came on it.\"',\n",
       " \"Old man: Don't go see no shows, peoples!\",\n",
       " \"Shows is the fruits of the devil's wombs!\",\n",
       " 'Crazy guy: ...And Jesus coming.',\n",
       " 'I know you have heard this before, but this is real.',\n",
       " 'Jesus is coming right now!',\n",
       " 'Tourist girl: Wait, New York University is a Catholic school?',\n",
       " 'Businessguy: Those hi-tech bloggers are !',\n",
       " 'Teen girl: I always thought Gandhi was like back in the time of Jesus.',\n",
       " \"Guy: You can't spell good without God.\",\n",
       " 'God is good.',\n",
       " 'You need God for good.',\n",
       " \"It's good that people down in Louisiana don't have clothes.\",\n",
       " \"Hobo: If the Jehovah's Witnesses won't give you no pussy, then fuck 'em!\",\n",
       " \"Crazy guy: You're gonna burn!\",\n",
       " \"You're all gonna burn!\",\n",
       " 'The agents of Satan will burn you all!',\n",
       " \"Guy: They had to cut off my favorite jeans, and my Mike Tyson's Punch Out!!\",\n",
       " 'hoodie.',\n",
       " 'I was pissed, man.',\n",
       " 'I made that thing myself.',\n",
       " 'I put the pixelated blood on it and everything.',\n",
       " 'I would have said\\nsomething, but you know, I was kind of unconscious.',\n",
       " 'Cashier chick: You have to take care of yourself now?',\n",
       " \"That's a lot of money, honey; you better start designing clothes or something.\",\n",
       " 'Black woman: Now my life.',\n",
       " 'Fishnet stockings, woo!',\n",
       " 'Guy on cell: Damn nigga, !',\n",
       " \"I seriously never even bought a CD, 'cept for blank ones to burn from a spindle.\",\n",
       " \"Yeah...Wal-Mart...I'm a mufuckin' pirate!\",\n",
       " \"I need a peg leg and an eyepatch an' shit.\",\n",
       " 'Store lady: Look at these people.',\n",
       " 'They try on shoes and then leave them all over the floor.',\n",
       " 'Like this is they house.',\n",
       " \"Lady: Do you know your shirt's on inside-out?\",\n",
       " 'Teen girl: My face is zippered!',\n",
       " 'I zippered my face!',\n",
       " 'Ahhh!',\n",
       " 'Driver woman: I can see your underpants!',\n",
       " \"Guy: Didn't some retard dress up like the Statue of Liberty or something?\",\n",
       " \"Hipster chick: I'll pay ya when we get back to the office.\",\n",
       " \"These pants are suede; I can't keep any money in the pockets.\",\n",
       " 'I put money in, and it slides right out.',\n",
       " \"Guy: I don't know what they put in their food, but I took one dump, and then I had to take another!\",\n",
       " 'Guy on cell: But I take a shit!',\n",
       " 'Man: Have you tried this?',\n",
       " \"It doesn't taste like chocolate.\",\n",
       " 'It tastes like shit.',\n",
       " \"Construction guy: See what I do when I leave here, I get me some milk, and I drink the milk, and all that dust we be breathin' in, I shit it out.\",\n",
       " \"Nurse lady: There's nothing like a little jar of pee sitting on your desk.\",\n",
       " 'Woman on cell: So did they pee pee in the potty?',\n",
       " \"Chick: The only person's poo I want to be smelling is my own.\",\n",
       " 'Dad: The third rail will make you go buzz...My friend knew a guy who got drunk, pissed on the track, electrocuted himself and died.',\n",
       " \"MTA woman: ...and you know what else you can't eat when you're going to be working the front of the train?\",\n",
       " \"Welch's grape juice.\",\n",
       " \"You'll get the worst shits ever!\",\n",
       " 'Hobo: Hold the train!',\n",
       " \"I'm just going to go get some sugar for my coffee!\",\n",
       " 'Hobo: Care to make a donation to the Broke Ass Foundation?',\n",
       " 'Hobo: I got one thing to say to you: \"Thank you.\"',\n",
       " 'And...I got two things to say to you: \"Thank you\" and \"Flame on!\"',\n",
       " \"Hobo: Hey...I'm gonna rob you...then I'm gonna lick your twat!\",\n",
       " 'Hobo: , spare some change for a homeless pirate!',\n",
       " \"Hobo: Spare change, please?...Have a nice day...I'm sorry you can't read...Have a nice day.\",\n",
       " 'Hobo: All right everybody!',\n",
       " \"Let's all donate to the 99 Niggers\\nAre Homeless Fund!\",\n",
       " 'Hobo: I accept anything in bills, no change please.',\n",
       " 'Your one dollar can buy me 6 chicken nuggets from McDonalds.',\n",
       " 'Five dollars can buy me a whole meal.',\n",
       " \"Ten dollars can get me some nice booze so for a couple of hours I don't have to think about doing this again tomorrow.\",\n",
       " 'Hobo: Man, I told you I wanted a latte.',\n",
       " \"Ain't nobody listens these days.\",\n",
       " 'Hobo: Why you got a skirt on?',\n",
       " \"Why you showin' your legs?\",\n",
       " \"You know guys like legs, I bet you don't even have a pussy under there!\",\n",
       " \"Hobo: If you don't give me money you'll turn out like me.\",\n",
       " 'Hobo: Good afternoon, ladies and gentlemen.',\n",
       " \"I'm not feeling so good today so I'm going to make this quick.\",\n",
       " 'Relax, miss, this is not a robbery.',\n",
       " 'Hobo: Can you spare a dime?',\n",
       " 'Do I look like Malcolm X to you?',\n",
       " 'Do I look like Brother Malcolm?',\n",
       " 'If Brother Malcolm was here, would you give him a dime?',\n",
       " 'Hell no !',\n",
       " \"Hobo: Excuse me ma'am, where is Iraq?\",\n",
       " 'You got a map?',\n",
       " 'Tell me, where is Iraq?',\n",
       " \"How you gonna send American troops there if ya don't even know where the damn place is ?\",\n",
       " 'Where is Iraq?',\n",
       " \"You don't mind that I'm drinking, do ya, ma'am?\",\n",
       " 'Where is Iraq ?',\n",
       " 'Someone tell me!',\n",
       " 'UHO guy: Spare some money for the homeless!',\n",
       " 'Every penny counts!',\n",
       " 'Spare some money...Yeah, that little piece of paper is going to help.',\n",
       " \"Hobo: Hey, do you have any change to spare?...Hey, that's a nice coat, can I have it?\",\n",
       " 'Hobo: Fuck you!',\n",
       " \"I ain't waitin' on line for no fuckin' soup!\",\n",
       " \"If I'm gonna kiss anyone's ass, it's gonna be my own...Bobblehead!\",\n",
       " 'Woman: I mean, what kind of person marches their daughter into their sixth grade class and announces that their daughter just ate a whole chicken?',\n",
       " 'Who does that?',\n",
       " 'I never forgave her for that.',\n",
       " \"Guy: If I ever become a cannibal, now I'll know what cuts of meat to ask for.\",\n",
       " 'Guy: I had a turkey injected with pomegranate juice once.',\n",
       " 'It was very delicious.',\n",
       " \"Chick on cell: Yeah, if I'm really hungry it doesn't matter about morals anymore, I'll just dive right into bacon, anything.\",\n",
       " 'forget about the vegan thing.',\n",
       " \"So for god's sake don't ever leave me alone with bacon.\",\n",
       " 'Or human.',\n",
       " 'Girl: So I actually tried garlic knots one day when I wasn\\'t high and I was like, \"Wow, these good...and there\\'s really garlic on them, too!\"',\n",
       " \"Teen girl: I'm hungry.\",\n",
       " 'Not hungry like I want to eat, but hungry.',\n",
       " 'Italian lady: When I was young, my mother used to make so much carbonated food.',\n",
       " 'Man: Hey, can I have a pizza with no cheese?',\n",
       " \"Chick on cell: This day is going by fucking slow; it's only 1:30PM...My eye is going to fall out!\",\n",
       " 'So what do you want for dinner again?',\n",
       " 'Walkie talkie: Attention all units, attention all units...Does anyone want Chinese food?',\n",
       " 'Girl: Ew!',\n",
       " 'Ew!',\n",
       " 'I work at McDonalds; they spit in all your food, I swear to God.',\n",
       " \"Chick: Don't get too close to Paul because if he busts ass it's going to smell like Y2K!\",\n",
       " 'Announcement: Please do not disturb the canine dogs.',\n",
       " 'Little girl: We used to have a bunny just like that one!',\n",
       " 'Then we had to take it to the liquor store.',\n",
       " 'Man: My dog tail is too long.',\n",
       " 'I wanna cut it off.',\n",
       " 'Girl: I just got a Friendster request from a dog.',\n",
       " \"Woman on cell: Everyone has a fucking pit bull...it's like everyone on MySpace suddenly got a pit bull.\",\n",
       " \"Hipster guy: You know, now that I don't have a girlfriend, I should get a dog.\",\n",
       " \"Isn't that how it works?\",\n",
       " 'Woman: She was happier than a pig having 50 orgasms!',\n",
       " 'Guy: So we were talking about how warm it was out.',\n",
       " 'So he says, \"That stupid fucking groundhog is always wrong!',\n",
       " '\", and I says, \"You stupid fucking bastard, that ain\\'t \\'til February!\"',\n",
       " \"Girl: You're like a hamster in bed!\",\n",
       " 'Lady: Watch it, mister !',\n",
       " \"I've got two dogs here.\",\n",
       " \"Animals can't see you know!\",\n",
       " 'Blind man: Come on, follow me.',\n",
       " 'Little girl: Follow me!',\n",
       " \"I'm riding into the future!\",\n",
       " 'Intercom: ...You are requested to be at your gate for your non-stop flight to JFK, New York.',\n",
       " 'Woman: Red fist.',\n",
       " \"Can't go.\",\n",
       " 'Mom: Quit fucking around on the sidewalk!',\n",
       " 'Store chick: Next guest with 10 items or less, step down.',\n",
       " 'Not ten and a half, ten.',\n",
       " 'Guy: Gangstas coming out?',\n",
       " 'What the fuck is that?',\n",
       " \"Guy on cell: Let's face it.\",\n",
       " \"I'm pretty fabulous; I don't need you to come down.\",\n",
       " 'Girl on cell: I know!',\n",
       " \"$100 for pills that aren't even for something that important....it's not like they're AIDS pills!\",\n",
       " \"Chick on cell: He says it's better, but I just say it's cheaper.\",\n",
       " \"Man on cell: It's insane.\",\n",
       " 'Eighty percent of twenty-five.',\n",
       " 'Take thirty off of that.',\n",
       " \"Dude, I'm looking at houses.\",\n",
       " 'I mean, fuck it.',\n",
       " \"Man: You tell her that I'll lower it down to $50 for her, and you can assure her, she'll be satisfied at least 3 times.\",\n",
       " \"Guy: You just spent $200 on dinner and you can't spend $2 on a MetroCard?\",\n",
       " 'Girl: Yeah, so I was talking to the guys about getting a pull-out couch so I could stay over, but then Max said I would have to give sexual favors for money if I wanted to stay there.',\n",
       " 'Girl: Excuse me, are you selling Freud by any chance?',\n",
       " \"Girl: The fuckin' R train is a motherfuckin' myth.\",\n",
       " \"I swear to god, it's the fuckin' unicorn: only fools and virgins can see it.\",\n",
       " 'Dude: I wish I had a shyster lawyer!',\n",
       " 'Man: Eh.',\n",
       " \"You know, it's my fantasy in life just to be left alone.\",\n",
       " 'Teen boy: I hope the new Xbox has a vagina.',\n",
       " 'Bouncer guy: I really cried, yo.',\n",
       " 'I thought wrestling was real.',\n",
       " 'Guy: \"Close my eyes and think of England?\"',\n",
       " 'In all my born days, I never thought someone would ever say that to me...Then, I met you.',\n",
       " \"Drunk girl: I wanna like move to a faraway island with a mute sex slave and lots of indigenous pot...That's all I really need because I can talk to them and they can hear...But they can't respond...And they'll just express themselves through sex...Like when they're mad it will get rough...and when they arent mad it will be gentle like the motherfucking ocean.\",\n",
       " \"Magician guy: So you're taping 4 to 5 people in a row?\",\n",
       " \"That's great.\",\n",
       " 'I\\nwish my girlfriend could do that.',\n",
       " 'Just kidding.',\n",
       " \"Guy: If Hitler were still alive and he were gay you would have thought he'd decorated that apartment.\",\n",
       " 'It was a soulless aesthetic abomination.',\n",
       " 'Guy: Yeah, me neither; if you are ugly you at least have to be nice.',\n",
       " 'Chick on cell: I saw a woman with half a head.',\n",
       " 'Literally half a head.',\n",
       " 'She had this indentation in her left hemisphere where they had taken out her skull to remove her brain.',\n",
       " 'I almost barfed.',\n",
       " \"But I didn't.\",\n",
       " \"Artist guy: C'mon honey, I'll draw your picture, make you look like Chewbacca.\",\n",
       " \"Black guy on cell: Shit, man....Nah, I can't do it...Shit man, nigga look like Mumm-Ra from the Thundercats.\",\n",
       " 'Guy: Carson Daly looks like a colostomy bag...with cocoa butter on it.',\n",
       " 'Girl: And every time she\\'d yell at me for something I just wanted to be like, \"Shut up, you\\'re ugly.\"',\n",
       " \"Guy: You know when you look at someone and you can just tell they're a douchebag, like they have a douchebag face?\",\n",
       " 'Yeah, I hate people like that.',\n",
       " 'Southern girl: I got guys asking me to send them pictures of my cooter.',\n",
       " \"It's like guys know when you're taken; they flock to you like bees to moldy bread.\",\n",
       " \"Chick: You know who's got it tough?\",\n",
       " \"Those girls in Africa getting there clits cut off...I mean sometimes I can't afford a cup of coffee but at least I still have my clit.\",\n",
       " 'Construction guy: I that woman.',\n",
       " 'Man, I worship the ground between her legs.',\n",
       " \"Chick: Well, I'm not going to eat just anyone's pussy, but I'm going to with strangers.\",\n",
       " 'Chick on cell: I just described my pussy as \"vagically delicious,\" and I wanted to leave you a message because I thought you would appreciate that.',\n",
       " 'Girl: Ew, that felt like vaginal secretion!',\n",
       " 'Hipster girl: Just tell him you have genital sores.',\n",
       " \"Chick on cell: I'm PMSing, so like, don't take it personally.\",\n",
       " 'Guy: Omigod dude, the main detective guy from Law & Order: SVU guest stars as a pediatrician on !',\n",
       " \"I could never imagine him doing the things he's doing right now.\",\n",
       " 'Girl: No dude, omigod, you know he was on and he was a gay prisoner and he liked getting it in the ass and giving it too.',\n",
       " \"That's extreme, man.\",\n",
       " 'Queer: But wait, is English a race?',\n",
       " \"Guy passerby: Holy shit, that's going on Overheard tomorrow.\",\n",
       " 'Bike guy: Hey girl, I really like your red hair\\nChick: Yeah, me too.',\n",
       " \"That's why I dye it.\",\n",
       " \"But I don't like it nearly as much as I like not being interrupted when I am tryng to talk to someone.\",\n",
       " 'Cabbie: Are you going this way?',\n",
       " \"I'm not turning around!\",\n",
       " 'Chick: What the..?',\n",
       " \"I'm not hitchhiking, I'm fucking paying you, and if I tell you to turn around you damn well better turn around!\",\n",
       " 'He drives away.',\n",
       " 'Chick: Yeah, fuck you too, cunty Mr.',\n",
       " 'Crack Whore.',\n",
       " 'Hipster guy: Lady, you need therapy.',\n",
       " 'Chick: Man, you need to stop sucking dick.',\n",
       " 'And a haircut.',\n",
       " \"Teen girl: Wow, that's pretty big.\",\n",
       " \"Teen guy: And it won't stop growing.\",\n",
       " 'Teen girl: I think you need a doctor.',\n",
       " 'Teen guy: Oh yeah?',\n",
       " 'What am I supposed to say?',\n",
       " '\"Hey doc, my penis just won\\'t stop growing\"?',\n",
       " 'Yeah, right.',\n",
       " \"Teen girl: Uh...maybe you shouldn't say that out loud.\",\n",
       " \"Girl #1: There's no way I could get that guy.\",\n",
       " 'He is absolutely gorgeous!',\n",
       " 'Girl #2: What do you think you are?',\n",
       " 'An omelette?',\n",
       " \"Sarah Jessica Parker: No honey, that's the litterbox.\",\n",
       " \"That's where the\\nkitty goes pee-pee and poo-poo.\",\n",
       " 'Girl #1: What language are they singing in?',\n",
       " 'Is that German?',\n",
       " \"Girl #2: No, it's European.\",\n",
       " 'HS girl #1: Well, I do want people cloning me.',\n",
       " 'Unless God came to me in a dream and said, \"Pilar, you need to clone yourself so that you can live again and save the world\", then I will.',\n",
       " 'But otherwise, I do want people cloning me.',\n",
       " 'Because if you get cloned you know you have to relive all your same problems and stuff.',\n",
       " \"Isn't that how cloning works?\",\n",
       " 'Anyways, cloning is stupid.',\n",
       " 'HS girl #2: Yeah, cloning is stupid.',\n",
       " \"Why haven't they been working on a cure for AIDS or breast cancer?\",\n",
       " 'They just want to make everyone die so they can clone them.',\n",
       " 'Girl #1: Ew, that horse is peeing.',\n",
       " 'Girl #2: Dude...that is a of pee.',\n",
       " 'Old woman: What, you want to push me out the window?',\n",
       " \"Old man: I would, but unfortunately you won't fit.\",\n",
       " 'Old woman: Bastard.',\n",
       " 'Clerk guy: Has anyone in this room been convicted of a felony?',\n",
       " 'Come up to the front desk.',\n",
       " \"Husband: Okay, I'm going to go up there.\",\n",
       " 'He returns 5 minutes later.',\n",
       " \"Husband: Hey, I'm all done.\",\n",
       " \"I told you you should've murdered someone, you'd be out too!\",\n",
       " \"Girl: We can't have sex until we get married.\",\n",
       " 'Guy: Sex is a form of marriage.',\n",
       " \"Girl: But we're not ready to get married.\",\n",
       " 'Guy: Your mom.',\n",
       " 'Chick #1: Fucking shit, man, this bitch is kicking our asses!',\n",
       " 'Chick #2: You just used four different curse words in one sentence .',\n",
       " 'Girl #1: So this is the man you want to marry?',\n",
       " 'Girl #2: Yeah.',\n",
       " 'Girl #1: And you said there were many maggots on the turkey?',\n",
       " 'Guy: My dog is so racist.',\n",
       " 'She is scared of black people.',\n",
       " 'But she also hates the black people of dogs.',\n",
       " 'Girl: What does that even mean?',\n",
       " 'Guy: Pugs.',\n",
       " 'Hobo: Hey, can you spare 20 cents?',\n",
       " \"Girl: Sorry\\nHobo: Okay, 30 cents...40 cents...50 cents, but that's my final offer.\",\n",
       " 'Hobo: Can you spare a hundred bucks?',\n",
       " 'Guy: A hundred bucks?',\n",
       " 'Hobo: What the hell am I going to do with a quarter?',\n",
       " \"Hobo: Y'know what I'd do if I was rich?\",\n",
       " 'Girl: What?',\n",
       " 'Hobo: Buy a Big Mac.',\n",
       " 'Girl: But you already have a Big Mac...',\n",
       " 'Hobo: Oh, this is all theatrical.',\n",
       " 'I only have a dollar...Can you spare some change?',\n",
       " 'Hobo: Got any spare change?',\n",
       " 'Lady: No.',\n",
       " 'Hobo: Well, I take dollars too.',\n",
       " 'Give me your phone, we can discuss it later.',\n",
       " 'Hobo: Can you spare some money?',\n",
       " \"Girl: No, sorry, I don't have any change.\",\n",
       " \"Hobo: That's okay, I take dollar bills, too.\",\n",
       " 'No credit cards or checks.',\n",
       " 'Mom: ...so I was making a roast, but the thing was that I only had chicken stock gravy.',\n",
       " 'Chicken stock gravy!',\n",
       " 'So I it!',\n",
       " 'On the beef!',\n",
       " 'Chick: You live on the edge, Mom.',\n",
       " 'Woman: Our biggest problem with sex was that he came too fast, because he was so into me.',\n",
       " 'So now he uses desensitizing condoms, and that works a lot better, especially because it takes me a really long time to have an orgasm with him.',\n",
       " 'Woman: I was really anxious, so I went to my GP and she prescribed Klonopin.',\n",
       " \"That completely took my anxiety away, but then my doctor said that she didn't feel that that was a good long-term drug.\",\n",
       " 'I guess I agree with that.',\n",
       " 'I did take one Klonopin on the plane yesterday, but that was okay because it was just a recreational Klonopin.',\n",
       " \"Woman: Now that I'm a wife I thought I should be more proper, but it turns out he likes me slutty.\",\n",
       " 'Woman: I think the most passionate sex I will ever have will be during some really passionate adulterous affair.',\n",
       " 'I would have to make a really conscious decision not to have an affair; it would be like fourth-order cognition.',\n",
       " \"Man: Wait, so you're on Law & Order ?\",\n",
       " 'Epatha Merkerson: Mm-hmm.',\n",
       " \"Man: Wow, I don't watch the show, but my son and daughter do.\",\n",
       " \"What's your name?\",\n",
       " \"I'll have to tell them I saw you.\",\n",
       " 'Epatha Merkerson: Epatha.',\n",
       " 'Man: Epala?',\n",
       " 'Epatha Merkerson: Epatha.',\n",
       " 'Man: Epasa?',\n",
       " 'Epatha Merkerson: E-path-a\\nMan: Ensala?',\n",
       " \"Maybe I should write this down, I'm sure I'll forget.\",\n",
       " 'Emana, you said?',\n",
       " 'Guy #1: If I had a dollar for every time I saw her blowing a guy...',\n",
       " \"Guy #2: You'd have a lot of dollars?\",\n",
       " 'Guy #1: One.',\n",
       " \"Lady: Oh, there's sales tax when you register a car?\",\n",
       " \"DMV woman: Yeah, ther'e sales tax.\",\n",
       " \"You can't buy nuthin' without payin' no sales tax.\",\n",
       " \"Lady: Well, this is the first car I've ever bought.\",\n",
       " \"Didn't I already\\npay sales tax when I bought the car?\",\n",
       " \"I don't understand.\",\n",
       " \"What if I don't have the money?\",\n",
       " \"I don't have that kind of money on me.\",\n",
       " 'I waited an hour on this line for nothing.',\n",
       " 'What do I do now?',\n",
       " \"DMV woman: Honey, we ain't got no installment plan.\",\n",
       " 'Hobo: I need some money to buy food.',\n",
       " 'Please help a brother out with any change you have.',\n",
       " 'Little girl: Hey mister, you were just in here.',\n",
       " \"Hobo: No, I wasn't.\",\n",
       " \"All homeless people don't look the same, you know!\",\n",
       " 'Little girl: But you all dress the same.',\n",
       " 'Teen girl #1: You know, the Special Olympics?',\n",
       " 'Teen girl #2: Special Olympics?',\n",
       " 'Teen girl #1: Yeah, Olympics for the retarded people.',\n",
       " 'Teen girl #2: You mean the wheelchair people that fight with each other?',\n",
       " \"Hipster girl #1: So you're still writing songs and performing?\",\n",
       " 'Hipster girl #2: Uh huh.',\n",
       " \"Hipster girl #1: And you're also acting, right?\",\n",
       " 'And modeling too?',\n",
       " 'Hipster girl #2: Yeah.',\n",
       " \"Hipster girl #1: Which would you say you're most passionate about?\",\n",
       " \"Hipster girl #2: I guess I'd have to say the modeling.\",\n",
       " \"Chick: You've had the greatest sex with me.\",\n",
       " 'Right?',\n",
       " 'Guy: Yeah.',\n",
       " 'I guess...',\n",
       " \"Guy: I'm tellin' ya, if a girl's bathroom is dirty, that means that\\nher pussy ain't too clean, too.\",\n",
       " 'Girl #1: Well, I have a clean bathroom.',\n",
       " \"Girl #2: Well, I'd be surprised if you said you have a dirty bathroom\\nafter this conversation.\",\n",
       " 'Man #1: You dropped your glove, sir.',\n",
       " \"Man #2: That's how they caught O.J. Simpson, man!\",\n",
       " 'A guy stands up and vomits in the middle of the restaurant.',\n",
       " 'Guy #2: Seriously...you might wanna rethink this All-You-Can-Drink Sunday buffet.',\n",
       " 'Little girl: Mommy, my ears hurt!',\n",
       " \"Mom: That's your third strike!\",\n",
       " 'I said stop!',\n",
       " 'She hits her daughter.',\n",
       " \"Little girl: That didn't hurt.\",\n",
       " \"Mom: I will kill you right now, don't tell me that didn't hurt.\",\n",
       " 'God Squad guy: Jesus is the answer!',\n",
       " \"Come to Jesus and he'll hold you in his arms!\",\n",
       " 'Come home to Jesus!',\n",
       " \"Guy #2: By the way, just so you know, the rest of us all think you're fucking nuts !\",\n",
       " \"overheard by: tourist girl\\n\\nGuy: You know what's really gross?\",\n",
       " 'Seeing the rats that get run over by the subway cars.',\n",
       " \"They're all split open and stuff.\",\n",
       " 'Girl: This one time I saw a rat get washed up on the shore.',\n",
       " 'He was missing all his skin.',\n",
       " 'Guy: Did he look happy?',\n",
       " 'Girl: No.',\n",
       " 'Bartender guy: Yo dude, block the door with your foot for a minute.',\n",
       " 'He does.',\n",
       " 'Bartender guy then proceeds to cut a line on the top of the urinal, snort it, and return to work.',\n",
       " 'Station lady: Go down those stairs over there, and the track is on your left.',\n",
       " 'Old woman: Where?',\n",
       " 'Station lady: Down those stairs, on your left.',\n",
       " 'Old woman: Thank you!',\n",
       " 'I wish I had your job.',\n",
       " \"Station lady: You couldn't handle my job.\",\n",
       " 'Hobo #1: You got more teeth than me.',\n",
       " 'Hobo #2: Yeah, I got six, but three of them are broken.',\n",
       " 'Hobo #1: You got six and a half.',\n",
       " 'Guy: Hi, I need to go to Nutley, New Jersey.',\n",
       " \"I know that the 192 bus goes, but--\\nTicket woman: Don't make yourself too comfortable, just ask.\",\n",
       " 'Girl: Do you all have a financial planner?',\n",
       " \"I think it's very important.\",\n",
       " \"Guy: I didn't go to Harvard Business School just to let some guy from Cornell manage my money.\",\n",
       " 'Chick #1: I hate taking subways.',\n",
       " \"They're so gross!\",\n",
       " 'Chick #2: I know!',\n",
       " 'So many dirty, smelly people.',\n",
       " \"And it's so expensive.\",\n",
       " 'Chick #1: Yeah, $2 is a lot of money.',\n",
       " 'Guy: Yeah, well, if you can find someone else to cart your ass around this city for $2, be my guest.',\n",
       " 'Drunk chick #1: I have the best blind date story ever.',\n",
       " 'Drunk chick #2: Oh yeah?',\n",
       " \"Drunk chick #1: My sister's friend flew from Australia to LA for a blind date, and she ended up flying to Aruba with the guy and marrying him like a week later.\",\n",
       " 'Drunk chick #2: Wow.',\n",
       " \"That's awesome!\",\n",
       " 'Drunk chick #1: But I think she was just, like, 35 and desperate to get married.',\n",
       " 'Two hobos are passing a bottle.',\n",
       " \"Woman: You can't do that!\",\n",
       " 'This is a passenger train...The blood of Jesus Christ!',\n",
       " \"You can't do that; this is a passenger train!\",\n",
       " \"You need to find Jesus!...That is the devil's drink.\",\n",
       " 'By the blood of Jesus you need to repent!',\n",
       " 'Hobo #1: Lady, I am the devil.',\n",
       " \"Woman: You can't do that on a passenger train!\",\n",
       " 'If I see a police I will have you arrested!',\n",
       " 'Hobo #2: You wanna borrow my cell phone?',\n",
       " 'Guy: I just geeked out my profile by a million percent.',\n",
       " 'What do you think?',\n",
       " 'Girl: Hold up, let me check...',\n",
       " 'Guy: So what do you think?',\n",
       " 'Girl: Yeah, that Evangelion child shit is weird.',\n",
       " 'Guy: Like how weird?',\n",
       " 'Teen boy: Do you have any matches?',\n",
       " 'Counter lady: Can I see ID?',\n",
       " 'Teen boy: You need ID for matches?',\n",
       " 'For just matches?',\n",
       " \"Counter lady: I can't give you matches without ID.\",\n",
       " 'Teen boy: ID for matches...what the fuck is this world coming to?',\n",
       " 'Cashier chick: \"You\\'ve got cigarettes, but you don\\'t have matches?',\n",
       " 'That don\\'t make sense!\"',\n",
       " 'We sell lighters, stupidass.',\n",
       " 'Buy one.',\n",
       " \"Mom: Shit, I ain't paying for peak hours.\",\n",
       " 'Tween girl: I can hide in the bathroom.',\n",
       " 'Mom: Or you can flash him.',\n",
       " 'Woman #1: So my mom is all depressed because of the Hurricane Katrina stuff, and she says she has no time to take care of herself.',\n",
       " 'And I say, \"It\\'s just a call to duty, Mom.\"',\n",
       " \"I mean, if she'd go to the beauty parlor...\",\n",
       " 'Woman #2: The beauty parlor probably got destroyed.',\n",
       " 'Woman #1: Yes, and they had to build a new one.',\n",
       " \"And I say, if she just goes in there and has them...fix her hair, or something...she'll feel so much better!\",\n",
       " 'Girl: Hey honey, slow down.',\n",
       " \"My feet hurt and I'm cold.\",\n",
       " \"Guy: Why don't you shut the fuck up and walk?\",\n",
       " 'I want to go the fuck home, bitch.',\n",
       " \"Tourist lady #1: Sweeney Todd ...I heard that's a spoof on a cooking show.\",\n",
       " 'Tourist lady #2: Oh, is it about Julia Child?',\n",
       " 'Tourist lady #1: I think so.',\n",
       " 'Girl #1: ...so, my professor started talking about The Diary of Anne Frank .',\n",
       " 'Girl #2: Oh, Anne Frank!',\n",
       " 'I used to love her!',\n",
       " 'I had the diary, the notebooks and the pencils and everything.',\n",
       " 'Queer: ?',\n",
       " 'Girl #1: I think she means Lisa Frank.',\n",
       " 'Dad: Did you bring your book?',\n",
       " 'Teen boy: Yeah.',\n",
       " \"Dad: Oh good; that way we don't have to talk.\",\n",
       " \"Woman #1: He's crazy.\",\n",
       " 'Woman #2: No, no, no.',\n",
       " 'See, when you say \"crazy\" I\\'m thinkin\\' , like smashing-car-windows crazy.',\n",
       " \"Chick #1: I know he's crazy.\",\n",
       " 'Chick #2: Right, so you should be able to be like, \"He\\'s crazy\", and\\nleave him.',\n",
       " \"Chick #1: But I'm used to his level of craziness.\",\n",
       " 'Teen Asian boy: So, the spelling bee--\\nTeen Indian girl: Was one of the kids Indian?',\n",
       " 'Teen Asian boy: Yeah, there was an Indian kid and a white kid.',\n",
       " 'Teen Indian girl: So typical.',\n",
       " 'My parents entered me in a spelling bee and I was fucking horrible.',\n",
       " 'Teen Asian boy: Ha, ha, ha!',\n",
       " 'Anyway, there were those two kids and I just wanted to throw PlayStations at them and yell, \"I\\'m setting you free!',\n",
       " 'I\\'m setting you free!\"',\n",
       " 'Yarmulke man: Excuse me, where does this train go to?',\n",
       " 'Do-rag guy: Florida.',\n",
       " 'Yarmulke man: Florida?',\n",
       " 'Texas?',\n",
       " 'California?',\n",
       " 'Do-rag guy: Yep.',\n",
       " 'Yarmulke man: Okay!',\n",
       " 'Good.',\n",
       " 'Woman #1: You ever just have one of those days ?',\n",
       " 'Woman #2: Yeah.',\n",
       " \"Woman #1: I'm having a whole week.\",\n",
       " 'I swear to god.',\n",
       " 'And I just walked here from...Oh, forget it.',\n",
       " 'Woman #2: Oh.',\n",
       " \"Woman #1: And now I can't even find my makeup!\",\n",
       " \"I swear to god, if they don't have it, I'm gonna fucking...I don't know!...I'll fucking kill a tourist!\",\n",
       " \"Woman #2: Oh, I hope it's not me!...Ha, ha, ha!\",\n",
       " '5 minutes later.',\n",
       " \"Woman #2: That's her!\",\n",
       " \"That's her!\",\n",
       " \"That's the New Yorker who cursed at me and threatened me!\",\n",
       " \"Woman #3: It's like seeing one in their natural habitat!\",\n",
       " \"I can't wait to tell everyone a New Yorker threatened you!\",\n",
       " 'Woman #2: I know!',\n",
       " \"It's awesome!\",\n",
       " \"Girl #1: It's so cool that we get to ride the train all day for free.\",\n",
       " 'Girl #2: Yeah, I guess so.',\n",
       " 'Girl #1: We should just ride it all day to like, take advantage.',\n",
       " 'Girl #2: Ha, ha!',\n",
       " \"Oh my god, that's so Jewish.\",\n",
       " \"Chick #1: You know why guys don't like mushrooms?\",\n",
       " \"Chick #2: Who said guys don't like mushrooms?\",\n",
       " 'Chick #1: Because they taste like cum!',\n",
       " 'Guy #1: I bought my dad a Clint Eastwood biography for Christmas.',\n",
       " \"I feel like that's a pretty solid bet for any dad.\",\n",
       " 'Clint, Frank Sinatra, maybe Brando.',\n",
       " 'Guy #2: What about James Dean?',\n",
       " 'Guy #1: Yeah, I guess.',\n",
       " 'What about that Vin Diesel?',\n",
       " 'Guy #3: You are seriously obsessed, dude.',\n",
       " \"Guy #1: Don't hate on the Diesel.\",\n",
       " 'Ooh, you know who everyone loves?',\n",
       " 'That Anne Frank.',\n",
       " 'Woman: Anne Frank was a lesbo.',\n",
       " \"Guy #1: I couldn't not buy it.\",\n",
       " \"Guy #2: Yeah, I'm thinking about it too.\",\n",
       " \"It's totally worth it.\",\n",
       " 'Guy #1: I mean, there are two real porn stars in it.',\n",
       " 'If it was just one, I could have passed it up.',\n",
       " 'Guy #2: Yeah, man.',\n",
       " 'But for that price, you almost have to do it!',\n",
       " \"Girl: Say, for instance, if somebody killed your mother and you killed theirs to get back at him--\\nGuy: Don't even such a thing!\",\n",
       " \"You're talking to fucking Oedipus here.\",\n",
       " \"Woman: The color of the car is not burgundy; it's purple.\",\n",
       " \"Man: No, I think it's burgundy.\",\n",
       " \"Woman: No, it's purple.\",\n",
       " 'I should know what purple is, I used to have purple hair.',\n",
       " 'Man: ...You had purple hair?...When did you have purple hair?',\n",
       " 'Chick: Oh my god, my hair is so dark!',\n",
       " 'Stylist guy: Does it look fake?',\n",
       " \"Chick: No, I just didn't know it would be this dark.\",\n",
       " 'Stylist guy: Well, it will look lighter when your hair dries.',\n",
       " 'Chick: ?',\n",
       " 'Girl #1: Marilyn Monroe is, like, one of my idols.',\n",
       " \"Girl #2: Wait, isn't he that guy with the glass eye?\",\n",
       " 'Girl #1: Um...no.',\n",
       " \"Man #1: Honey, we don't have to see Memoirs of a Geisha .\",\n",
       " \"You lived it, didn't you?\",\n",
       " 'Woman: You just know the right things to say!',\n",
       " 'Man #2: Some people make me wish that snow outside was really acid.',\n",
       " 'Woman: Yo, my cousin is going to be on American Idol .',\n",
       " 'Guy: Wow, she any good?',\n",
       " \"Woman: No, she's terrible, she sounds like a dying seal.\",\n",
       " 'Girl #1: Where did you hear that?',\n",
       " 'What news have been watching?',\n",
       " 'Girl #2: Canadian.',\n",
       " \"Black guy: You would like him 'cause he looks like a gorilla, and they are from the Amazon like you.\",\n",
       " 'White girl: Dummy, gorillas are from Africa; you of all people\\nshould know that.',\n",
       " 'Chick: I wish it would snow so I could make a Kate Moss joke.',\n",
       " 'Dad: If you start to get blown away, just drop the umbrella.',\n",
       " 'Guy: The sun is nice today.',\n",
       " 'Guy: I really liked the wind on that block.',\n",
       " 'It was bearable.',\n",
       " \"Guy: Ha, ha, Mother Nature's a bitch, and she just gave you a blowjob!\",\n",
       " 'Woman: My husband has this hierarchy of terrible things that can happen to a person, and you wanna know what tops off his list?',\n",
       " 'According to him, the number one most horrible thing that can ever happen to a person is getting snow on your wrists.',\n",
       " 'Old lady: She came to me and said, \"We the people of the 15th floor have decided that you are not friendly.\"',\n",
       " 'And I said, \"That\\'s not in the lease.\"',\n",
       " \"Guy: Don't ever give up your dreams.\",\n",
       " 'This is New York.',\n",
       " \"It's not even about the numbers.\",\n",
       " \"I've come too far to give up my dreams.\",\n",
       " \"Don't give up your dreams...So Canal Street is this way?\",\n",
       " 'Tourist woman: Now this is the New York!',\n",
       " 'This is the New York you see on TV!',\n",
       " 'Teen girl: New York is the best country in the world.',\n",
       " \"Vendor guy: You go to Chelsea and it's like glory hole city!\",\n",
       " 'Girl: Stop turning it!',\n",
       " 'It could fall over on you!',\n",
       " \"And it's got points!\",\n",
       " 'Jewish mom: You guys live in a very silly place.',\n",
       " 'Brooklyn.',\n",
       " \"That's silly.\",\n",
       " 'Everyone left with the Dodgers.',\n",
       " 'College guy on cell: Hi, Dad!',\n",
       " \"Yeah, it's good.\",\n",
       " 'Seen a couple shows, went to some museums, gonna get something to eat...',\n",
       " 'Teen girl: This is, like, intellectual popcorn.',\n",
       " 'Drunk guy: Hey!',\n",
       " 'Hey!',\n",
       " 'Girl in the red shirt!...Prettiest girl on the whole train!...A:d look at this- she can read, too!',\n",
       " 'Guy: This is just bullshit.',\n",
       " 'Or whatever the Hebrew for \"bullshit\" is.',\n",
       " 'Teen boy: Dinosaurs are so stupid!',\n",
       " 'Woman: Have you ever tried to talk about thesis statements to people who have their fingers up their noses?',\n",
       " 'Girl: So, it works out.',\n",
       " 'You like girls who are as smart as you are.',\n",
       " 'He likes girls who are smarter than he is, and likes girls who are, well, dumber than he is.',\n",
       " 'Guy: I definitely liked Picasso more when he was freaking out.',\n",
       " \"Guy: I be readin' that Shakespeare shit, yo!\",\n",
       " 'He talkin\\' \\'bout some \"Epoxy dat wench\", and \"Wherefore to thou.\"',\n",
       " \"Thief guy: You can't touch me.\",\n",
       " 'I know my rights.',\n",
       " \"I'm an educated criminal; I'm your worstest nightmare.\",\n",
       " \"Guy on cell: I'll have to call you back from a landline, can you give me the number?\",\n",
       " 'Uh huh...uh huh...uh huh...you know what?',\n",
       " \"I don't have a pen to write this down, does it spell anything?\",\n",
       " 'Girl on Nextel: Oh baby, you shoulda woke me up...We coulda done the do again.',\n",
       " \"Girl on cell: But I really want Chase...I guess I'll have to call him back and tell him to put it in my butt.\",\n",
       " 'Guy: I should call her.',\n",
       " \"She's probably taking off her pants right now.\",\n",
       " 'Guy: You know, for a vegetarian you sure have a lot of man meat.',\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents1 = sent_tokenizer.tokenize(texto)\n",
    "sents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Função padrão do NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(texto)\n",
    "\n",
    "# Neste caso não houve diferença entre as funções\n",
    "sents2[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentença 678 do texto usando o PunktSentenceTokenizer\n",
    "sents1[678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all theatrical.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentença 678 do texto usando o sent_tokenize\n",
    "#Aqui houve uma pequena diferença, onde tokenizer treinado apresentou um resultado mais preciso\n",
    "sents2[678] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenização geralmente é uma das primeiras etapas da limpeza do texto. Isso ajuda na próxima etapa do processo de limpeza que é a remoção das “stop words (palavras que não afetam o significado do texto)”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords são palavras comuns que normalmente não contribuem para o significado de uma frase, pelo menos com relação ao propósito da informação e do processamento da linguagem natural. São palavras como \"The\" e \"a\" ((em inglês) ou \"O/A\" e \"Um/Uma\" (em português). Muitos mecanismos de busca filtram essas palavras (stopwords), como forma de economizar espaço em seus  índices de pesquisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me', 'from', 'just', 're', 'it', 'these', 'once', 'is', 'she', 'other', 'couldn', 'or', 'this', 'in', \"hasn't\", 'then', 'above', 'themselves', 'i', \"should've\", 'ain', 'against', 'an', 'you', 'yourself', 'my', 'only', \"wasn't\", 'the', 'if', 'that', \"haven't\", 'on', 'more', 'over', \"you're\", \"hadn't\", \"that'll\", 'ma', 'down', 'at', 'should', 'as', \"you'd\", 'both', 't', 'of', 'yourselves', 'had', 'mightn', 'himself', 'd', 'than', 'where', 'are', 'same', 'by', 'for', 'further', 'been', 'out', 'off', \"you'll\", 'myself', 'most', \"didn't\", 'have', 'who', 'am', 'doing', 'while', 'does', 'his', 'can', 'before', \"isn't\", 'won', 'to', \"doesn't\", \"you've\", 'hasn', \"it's\", 'no', 'those', 've', 'few', 'y', 'your', 'what', 'because', 'don', 'and', 'how', 'too', 'them', 'up', \"don't\", 'wasn', 'each', 'aren', 'shan', 'now', 'after', 'o', \"won't\", \"wouldn't\", 'a', 'haven', 'not', 'here', 'wouldn', 'weren', 'needn', \"shan't\", 'there', 'we', \"mustn't\", 'during', 'below', 'about', \"needn't\", 'very', 'shouldn', 'didn', 's', \"couldn't\", 'ours', 'isn', 'll', 'theirs', 'such', 'mustn', 'why', 'being', 'until', 'hers', 'm', 'ourselves', 'some', 'having', 'under', 'has', 'into', 'nor', 'all', 'again', 'yours', 'itself', 'its', 'were', \"she's\", 'when', \"shouldn't\", 'own', 'him', \"mightn't\", 'doesn', 'their', 'be', 'he', 'herself', 'they', 'was', 'any', 'hadn', 'will', 'but', 'which', 'whom', 'so', 'do', \"aren't\", 'between', 'our', 'did', 'with', 'through', \"weren't\", 'her'}\n",
      "[\"Can't\", 'contraction']\n"
     ]
    }
   ],
   "source": [
    "# Stop words em inglês\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "print(english_stops)\n",
    "\n",
    "# Lista de palavras\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "\n",
    "# List comprehension para aplicar as english_stop words a lista de palavras\n",
    "words_without_stop_words = [word for word in words if word not in english_stops]\n",
    "print(words_without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta Exercício 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estou', 'estudando', 'um', 'tema', 'interesante', 'em', 'pln']\n",
      "['estudando', 'tema', 'interesante', 'pln']\n"
     ]
    }
   ],
   "source": [
    "#Stop words em português\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Lista de palavras\n",
    "palavras = [\"Estou\", 'estudando', 'um', 'tema', 'interesante', 'em', 'PLN']\n",
    "\n",
    "# List comprehension para transofmrar em minúsculo\n",
    "palavras_minusculo = [word.lower() for word in palavras]\n",
    "print('Palavras em minúsculo',palavras_minusculo)\n",
    "\n",
    "# List comprehension para aplicar as portuguese_stop words a lista de palavras\n",
    "palavras_sem_stopwords = [palavra for palavra in palavras_minusculo if palavra not in portuguese_stops]\n",
    "print(palavras_sem_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDs das Stop Words\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'não',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'à',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'já',\n",
       " 'eu',\n",
       " 'também',\n",
       " 'só',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'até',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'você',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'às',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'nós',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estávamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivéramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivéssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'há',\n",
       " 'havemos',\n",
       " 'hão',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvéramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvéssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverá',\n",
       " 'houveremos',\n",
       " 'houverão',\n",
       " 'houveria',\n",
       " 'houveríamos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'são',\n",
       " 'era',\n",
       " 'éramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fôramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fôssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'serão',\n",
       " 'seria',\n",
       " 'seríamos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tém',\n",
       " 'tinha',\n",
       " 'tínhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivéramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivéssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terá',\n",
       " 'teremos',\n",
       " 'terão',\n",
       " 'teria',\n",
       " 'teríamos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de stop words\n",
    "stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-Speech Tagging (_POS Tagging_)\n",
    "\n",
    "As operações anteriores (tokenização  e remoção de stopwords) são normalmente as atividades iniciais em um processo de limpeza de um conjunto de dados. Entretanto, essas operações não levam em consideração o contexto. Para poder trabalhar com PLN precisamos levar em consideração o contexto e para isso existe o conceito de Part-of-Speech Tagging (_POS Tagging_).\n",
    "\n",
    "O POS Tagging é o processo de rotulação de elementos textuais - tipicamente palavras e pontuação - com o fim de evidenciar a estrutura gramatical de um determinado trecho de texto. Ou seja, definir dentro de um conjunto de dados os verbos, pronomes, nomes, substantivos, conjunções etc. Essa rotulação depende do contexto.\n",
    "\n",
    "Em reconhecimento e síntese de fala, seu uso é útil para extração de termos, desambiguação, composição de novas frases e pesquisa lexicográfica.\n",
    "\n",
    "OBS: É preciso tokenizar o texto antes de aplicar o POS Tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time to start with natural language processing.',\n",
       " 'Python will make our life easier!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Texto\n",
    "frase = \"Time to start with natural language processing. Python will make our life easier!\"\n",
    "\n",
    "# Tokenization em sentenças\n",
    "sent_tokens = sent_tokenize(frase)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'to',\n",
       " 'start',\n",
       " 'with',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'Python',\n",
       " 'will',\n",
       " 'make',\n",
       " 'our',\n",
       " 'life',\n",
       " 'easier',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization em palavras\n",
    "word_tokens = word_tokenize(frase)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('start', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Python', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('make', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('life', 'NN'),\n",
       " ('easier', 'JJR'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando pos_tag aos tokens\n",
    "tags = pos_tag(word_tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "# Visualizando o significado de cada código do POS Tag.\n",
    "# Nesse caso, visualizando VB\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "None\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      "None\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "None\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "None\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "None\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Definição para cada definição de código\n",
    "list_of_tags = []\n",
    "for pair in tags:\n",
    "    list_of_tags.append(pair[1])\n",
    "list_of_tags = list(set(list_of_tags))\n",
    "for pos in list_of_tags:\n",
    "    print(nltk.help.upenn_tagset(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__POS Tagging em português com o NLTK__\n",
    "\n",
    "Vamos utilizar um corpus da NLTK em português já 'taggeado' para treinar um POS Tagging default ([Exemplos de Processamento em português](#section_pt_tag))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Um', '>N+art'), ('revivalismo', 'H+n'), ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import floresta\n",
    "floresta.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplos de tags: [('um', 'art'), ('revivalismo', 'n'), ('refrescante', 'adj'), ('o', 'art'), ('7_e_meio', 'prop'), ('é', 'v-fin'), ('um', 'art'), ('ex-libris', 'n'), ('de', 'prp'), ('a', 'art')]\n"
     ]
    }
   ],
   "source": [
    "#As tags consistem de algumas informações sintáticas, seguidas por um sinal de mais,seguidas pela POS tag. \n",
    "#Apenas nos interessa a informação da tag. Este método serve para simplificar as tags do corpus\n",
    "def simplify_tag(t):\n",
    "     if \"+\" in t:\n",
    "        return t[t.index(\"+\")+1:]\n",
    "     else:\n",
    "        return t\n",
    "\n",
    "twords = floresta.tagged_words()\n",
    "twords = [(w.lower(), simplify_tag(t)) for (w,t) in twords]    \n",
    "print('Exemplos de tags:',twords[:10])\n",
    "\n",
    "tsents = floresta.tagged_sents()\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as tags obtidas vamos treinar o tagger em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Tagger [('é', 'n'), ('um', 'n'), ('árvore', 'n'), ('cortar', 'n'), ('ao', 'n'), ('a', 'n')]\n",
      "UnigramTagger treinado [('é', 'v-fin'), ('um', 'art'), ('árvore', 'n'), ('cortar', 'v-inf'), ('cortado', 'v-pcp'), ('ao', 'prp'), ('a', 'art')]\n"
     ]
    }
   ],
   "source": [
    "#Divisão dos dados de treinamento e testes\n",
    "train = tsents[100:]\n",
    "test = tsents[:100]\n",
    "\n",
    "#Cria um DefaultTagger: tudo será \"n\"-> \"noum\"\n",
    "tagger0 = nltk.DefaultTagger('n')\n",
    "print('Default Tagger',tagger0.tag(['é','um','árvore','cortar','ao','a']))\n",
    "\n",
    "#Vamos treinar um UnigramTagger a partir dos dados de treinamento\n",
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "print('UnigramTagger treinado',tagger1.tag(['é','um','árvore','cortar','cortado','ao','a']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming é a técnica de __remover sufixos e prefixos__ de uma palavra, chamada stem. Por exemplo, o stem da palavra cooking é cook. Um bom algoritmo sabe que \"ing\" é um sufixo e pode ser removido. \n",
    "\n",
    "Stemming é muito usado em mecanismos de buscas para indexação de palavras. Ao invés de armazenar todas as formas de uma palavras, um mecanismo de busca armazena apenas o stem da palavra, reduzindo o tamanho do índice e aumentando a performance do processo de busca.\n",
    "\n",
    "A seguir iremos apresentar alguns Stemmer e seus comportamentos para algumas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PorterStemmer:\n",
      "cooking -> cook\n",
      "cookery -> cookeri\n"
     ]
    }
   ],
   "source": [
    "#Cria o Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nPorterStemmer:\")\n",
    "print(\"cooking ->\", stemmer.stem('cooking'))\n",
    "print(\"cookery ->\", stemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LancasterStemmer:\n",
      "cooking -> cook\n",
      "cookery -> cookery\n"
     ]
    }
   ],
   "source": [
    "#Cria o Stemmer\n",
    "stemmer2 = LancasterStemmer()\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nLancasterStemmer:\")\n",
    "print(\"cooking ->\",stemmer2.stem('cooking'))\n",
    "print(\"cookery ->\",stemmer2.stem('cookery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RegexpStemmer:\n",
      "cooking -> cook\n",
      "cookery -> cookery\n"
     ]
    }
   ],
   "source": [
    "#Cria o Stemmer\n",
    "stemmer3 = RegexpStemmer('ing')\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nRegexpStemmer:\")\n",
    "print(\"cooking ->\",stemmer3.stem('cooking'))\n",
    "print(\"cookery ->\",stemmer3.stem('cookery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SnowballStemmer:\n",
      "cooking -> cook\n",
      "cookery -> cookeri\n"
     ]
    }
   ],
   "source": [
    "#Cria o Stemmer\n",
    "stemmer3 = SnowballStemmer('english')\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nSnowballStemmer:\")\n",
    "print(\"cooking ->\",stemmer3.stem('cooking'))\n",
    "print(\"cookery ->\",stemmer3.stem('cookery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta Exercício 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowballStemmer Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "\n",
      "SnowballStemmer:\n",
      "Conzinhando ->  conzinh\n",
      "Culinária ->  culinár\n",
      "Computação ->  comput\n",
      "Computando ->  comput\n",
      "Computar ->  comput\n",
      "pedreir\n",
      "pedr\n",
      "pedregulh\n"
     ]
    }
   ],
   "source": [
    "#Linguagens suportadas pelo SnowballStemmer\n",
    "print(\"SnowballStemmer Languages:\",SnowballStemmer.languages)\n",
    "\n",
    "#Cria o Stemmer para a linguagem em português\n",
    "portuguese_stemmer = SnowballStemmer('portuguese')\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nSnowballStemmer:\")\n",
    "print(\"Conzinhando -> \",portuguese_stemmer.stem('Conzinhando'))\n",
    "print(\"Culinária -> \",portuguese_stemmer.stem('Culinária'))\n",
    "print(\"Computação -> \",portuguese_stemmer.stem('Computação'))\n",
    "print(\"Computando -> \",portuguese_stemmer.stem('Computando'))\n",
    "print(\"Computar -> \",portuguese_stemmer.stem('Computar'))\n",
    "print(portuguese_stemmer.stem('Pedreiro'))\n",
    "print(portuguese_stemmer.stem('Pedra'))\n",
    "print(portuguese_stemmer.stem('Pedregulho'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta Exercício 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SnowballStemmer:\n",
      "Conzinhando ->  conzinh\n",
      "Culinária ->  culinár\n",
      "Computação ->  comput\n",
      "Computando ->  comput\n",
      "Computar ->  comput\n",
      "pedr\n",
      "pedr\n",
      "pedregulh\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "#Cria o Stemmer para a linguagem em português\n",
    "portuguese_stemmer = RSLPStemmer()\n",
    "\n",
    "#Aplica o Stemmer\n",
    "print(\"\\nSnowballStemmer:\")\n",
    "print(\"Conzinhando -> \",portuguese_stemmer.stem('Conzinhando'))\n",
    "print(\"Culinária -> \",portuguese_stemmer.stem('Culinária'))\n",
    "print(\"Computação -> \",portuguese_stemmer.stem('Computação'))\n",
    "print(\"Computando -> \",portuguese_stemmer.stem('Computando'))\n",
    "print(\"Computar -> \",portuguese_stemmer.stem('Computar'))\n",
    "print(portuguese_stemmer.stem('Pedreira'))\n",
    "print(portuguese_stemmer.stem('Pedra'))\n",
    "print(portuguese_stemmer.stem('Pedregulho'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatização na linguística, é o processo de agrupar as diferentes formas flexionadas de uma palavra para que possam ser analisadas como um único item. Na linguística computacional, a Lemmatização é o processo algorítmico de determinação do lema para uma determinada palavra. Uma vez que o processo pode envolver tarefas complexas, como entender o contexto e determinar a parte da fala de uma palavra em uma frase (requerendo, por exemplo, conhecimento da gramática de uma linguagem), pode ser uma tarefa difícil implementar um lematizador para uma nova língua.\n",
    "\n",
    "Em muitas línguas, as palavras aparecem em várias formas inflexíveis. Por exemplo, em inglês, o verbo 'to walk' pode aparecer como 'walk', 'walks', 'walking'. A forma base, 'walk', que se poderia procurar em um dicionário, é chamado de lema para a palavra. A combinação da forma base com a parte da fala geralmente é chamada de lexema da palavra.\n",
    "\n",
    "A Lemmatização está intimamente relacionada com o Stemming. A diferença é que um stemmer opera em uma única palavra sem conhecimento do contexto e, portanto, não pode discriminar entre palavras que têm diferentes significados, dependendo da parte da fala. No entanto, os stemmers são geralmente mais fáceis de implementar e executar mais rapidamente, e a precisão reduzida pode não ser importante para algumas aplicações. Assim o Stemming é mais rápido enquanto que o Lemmatization é mais preciso.\n",
    "\n",
    "De forma geral, Stemming e Lemmatization são operações parecidas. A principal diferença entre eles é que o __Stemmning pode gerar palavras inexistentes, enquanto os lemas são palavras reais__. A escolha então será com base no problema o qual pretende-se resolver.\n",
    "\n",
    "Assim, sua root stem pode não ser algo que você pode procurar em um dicionário, mas você pode procurar um lema. Algumas vezes você terminará com uma palavra muito semelhante, mas as vezes, você terminará com uma palavra completamente diferente. Vamos ver alguns exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking -> cooking\n",
      "dogs -> dog\n",
      "churches -> church\n",
      "are -> are\n",
      "is -> is\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Criar um Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Com argumentos default\n",
    "print(\"cooking ->\",wordnet_lemmatizer.lemmatize('cooking'))\n",
    "print(\"dogs ->\",wordnet_lemmatizer.lemmatize('dogs'))\n",
    "print(\"churches ->\",wordnet_lemmatizer.lemmatize('churches'))\n",
    "print(\"are ->\",wordnet_lemmatizer.lemmatize('are'))\n",
    "print(\"is ->\",wordnet_lemmatizer.lemmatize('is'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatization e POS tagging__\n",
    "\n",
    "No exemplo acima vimos que o lema para 'are', 'is' e cooking não foi achado corretamente. Para corrigir  precisamos iremos passar o POS Tagging. \n",
    "\n",
    "Além disso,  a mesma palavra pode ter múltiplos lemas com base no contexto. Para isso o também utilizaremos o POS Tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is -> be\n",
      "are -> be\n",
      "cooking -> cook\n"
     ]
    }
   ],
   "source": [
    "#Passando a classe gramatical da palavra através do parâmetro pos\n",
    "#pos = v => Verbo\n",
    "print(\"is ->\",wordnet_lemmatizer.lemmatize('is', pos='v'))\n",
    "print(\"are ->\",wordnet_lemmatizer.lemmatize('are', pos='v'))\n",
    "print(\"cooking ->\",wordnet_lemmatizer.lemmatize('cooking', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3496c304d334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# [('feet', 'NNS')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# [('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Lemmatization e POS Tagging em português com o [spaCy](#section_spacy)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalando a biblioteca e baixando o pacote com o modelo treinado em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/6e/a89da6b5c83f8811e46e3a9270c1aed90e9b9ee6c60faf52b7239e5d3d69/spacy-2.0.18-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting regex==2018.01.10 (from spacy)\n",
      "Collecting ujson>=1.35 (from spacy)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl\n",
      "Collecting thinc<6.13.0,>=6.12.1 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/db/a7/46640a46fd707aeb204aa4257a70974b6a22a0204ba703164d803215776f/thinc-6.12.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting dill<0.3,>=0.2 (from spacy)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy) (1.16.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/61/9b0520c28eb199a4b1ca667d96dd625bba003c14c75230195f9691975f85/cymem-2.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting msgpack<0.6.0,>=0.5.6 (from thinc<6.13.0,>=6.12.1->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.13.0,>=6.12.1->spacy)\n",
      "Collecting msgpack-numpy<0.4.4 (from thinc<6.13.0,>=6.12.1->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.12.0)\n",
      "Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.13.0,>=6.12.1->spacy)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.13.0,>=6.12.1->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl\n",
      "Collecting toolz>=0.8.0 (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy)\n",
      "Installing collected packages: regex, ujson, murmurhash, plac, cymem, preshed, chardet, urllib3, idna, requests, msgpack, toolz, cytoolz, msgpack-numpy, wrapt, dill, tqdm, thinc, spacy\n",
      "Successfully installed chardet-3.0.4 cymem-2.0.2 cytoolz-0.9.0.1 dill-0.2.9 idna-2.8 msgpack-0.5.6 msgpack-numpy-0.4.3.2 murmurhash-1.0.2 plac-0.9.6 preshed-2.0.1 regex-2018.1.10 requests-2.21.0 spacy-2.0.18 thinc-6.12.1 toolz-0.9.0 tqdm-4.31.1 ujson-1.35 urllib3-1.24.1 wrapt-1.10.11\n",
      "Collecting spacy-nightly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/8cdb12143d1961907b69c410c79bdb5d5c6e19c5320271d23f8657b2fe11/spacy-nightly-2.1.0a9.dev0.tar.gz (27.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 27.6MB 1.3MB/s ta 0:00:011    91% |█████████████████████████████▎  | 25.3MB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (1.16.1)\n",
      "Collecting wasabi<1.1.0,>=0.0.12 (from spacy-nightly)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (2.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (1.0.2)\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy-nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/f0/a8/9f771a1497b999d0c2d69782facf211e15478d58dd9fe0dfe22646e9affd/blis-0.2.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting srsly<1.1.0,>=0.0.5 (from spacy-nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (2.21.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from spacy-nightly) (0.9.6)\n",
      "Collecting thinc<7.1.0,>=7.0.1 (from spacy-nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/1c/0cc64cab7cc8b9889b57c98e938dd15c646f0ab718e5589baa43b8aafb60/thinc-7.0.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (3.0.4)\n",
      "Collecting thinc-gpu-ops<0.1.0,>=0.0.1 (from thinc<7.1.0,>=7.0.1->spacy-nightly)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.1->spacy-nightly) (4.31.1)\n",
      "Building wheels for collected packages: spacy-nightly\n",
      "  Building wheel for spacy-nightly (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/03662232677/.cache/pip/wheels/0a/3d/80/18b098c25f8f97c5691fb3f665b6df19623938f2fd870bf1aa\n",
      "Successfully built spacy-nightly\n",
      "\u001b[31mspacy 2.0.18 has requirement thinc<6.13.0,>=6.12.1, but you'll have thinc 7.0.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wasabi, blis, srsly, thinc-gpu-ops, thinc, spacy-nightly\n",
      "  Found existing installation: thinc 6.12.1\n",
      "    Uninstalling thinc-6.12.1:\n",
      "      Successfully uninstalled thinc-6.12.1\n",
      "Successfully installed blis-0.2.2 spacy-nightly-2.1.0a9.dev0 srsly-0.0.5 thinc-7.0.1 thinc-gpu-ops-0.0.4 wasabi-0.0.15\n",
      "Collecting pt_core_news_sm==2.1.0a7 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0a7/pt_core_news_sm-2.1.0a7.tar.gz#egg=pt_core_news_sm==2.1.0a7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0a7/pt_core_news_sm-2.1.0a7.tar.gz (12.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.8MB 791kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
      "  Running setup.py install for pt-core-news-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed pt-core-news-sm-2.1.0a7\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages/pt_core_news_sm\n",
      "-->\n",
      "/home/03662232677/anaconda3/envs/pln_basico2/lib/python3.6/site-packages/spacy/data/pt_core_news_sm\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install spacy-nightly\n",
    "!spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import da biblioteca e carregar o modelo em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos pegar uma frase de exemplo, separar em tokens e imprimir seus respectivos lemmas e tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Processamento\ttextual\té\tfácil\t?\tO\tpedreiro\tfoi\tna\tpedreira\tpegar\tpedras\t.\t\n",
      "POS Tagging: NOUN\tADJ\tVERB\tADJ\tPUNCT\tDET\tNOUN\tVERB\tADP\tPROPN\tVERB\tNOUN\tPUNCT\t\n",
      "Lemmatization: Processamento\ttextual\tser\tfácil\t?\tO\tpedreiro\tser\to\tpedreiro\tpegar\tpedrar\t.\t\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "pos = \"\"\n",
    "lemma = \"\"\n",
    "for token in nlp(\"Processamento textual é fácil? O pedreiro foi na pedreira pegar pedras.\"):\n",
    "    text += token.text + \"\\t\"\n",
    "    pos += token.pos_ + \"\\t\"\n",
    "    lemma += token.lemma_ + \"\\t\"\n",
    "\n",
    "print(\"Texto:\", text)\n",
    "print(\"POS Tagging:\", pos)\n",
    "print(\"Lemmatization:\", lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta Exercício 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: A\tpedreira\tfoi\tna\tpedreira\ttrabalhar\tcom\tpedra\t.\t\n",
      "POS Tagging: DET\tNOUN\tVERB\tADV\tPROPN\tVERB\tADP\tNOUN\tPUNCT\t\n",
      "Lemmatization: A\tpedreiro\tser\to\tpedreiro\ttrabalhar\tcom\tpedrar\t.\t\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "pos = ''\n",
    "lemma = ''\n",
    "for token in nlp1(\"A pedreira foi na pedreira trabalhar com pedra.\"):\n",
    "    text += token.text + \"\\t\"\n",
    "    pos += token.pos_ + \"\\t\"\n",
    "    lemma += token.lemma_ + \"\\t\"\n",
    "\n",
    "print(\"Texto:\", text)\n",
    "print(\"POS Tagging:\", pos)\n",
    "print(\"Lemmatization:\", lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_pt_tag\">__How To: Examples for Portuguese Processing__ </a>\n",
    "   - Contém vários exemplos de processamento de texto relacionado à língua portuguesa (http://www.nltk.org/howto/portuguese_en.html)\n",
    "   \n",
    "<a id=\"section_spacy\">__spaCy__ </a>\n",
    "   - spaCy  é uma biblioteca open-source para NLP em Python.  (https://spacy.io/)\n",
    "\n",
    "<a id=\"section_nltk\">__NLTK__ </a>\n",
    "   - NLTK (Natural Language Toolkit), ferramenta para PLN em Python. \n",
    "       - http://www.nltk.org/index.html \n",
    "       - http://www.nltk.org/book/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
